{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "\n",
        "\n",
        "folder = \\\n",
        "        f\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs\"\n",
        "full_embedded_word_vec_dictionary = f'{folder}/common_info_embedd_files/full_embedded_word_vec_dictionary.txt'\n",
        "embedded_words = f\"{folder}/word_embedded_files/\"\n",
        "clear_file([full_embedded_word_vec_dictionary])\n",
        "onlyfiles = [join(embedded_words, f) for f in listdir(embedded_words) if isfile(join(embedded_words, f))]\n",
        "global_words_set = set()\n",
        "\n",
        "print(onlyfiles)\n",
        "n = 0\n",
        "for file in onlyfiles:\n",
        "    # n+=1\n",
        "    # if n > 2: break\n",
        "    print(file[-8:])\n",
        "    with open(file,'r') as valid_file:\n",
        "        for embedded_word_line_org in valid_file: \n",
        "            embedded_word_line = embedded_word_line_org.split(' ')\n",
        "            word = embedded_word_line[0]\n",
        "            embedded_word_vector = embedded_word_line[1:]\n",
        "            # print(word, len(embedded_word_vector))\n",
        "            # print(embedded_word_line_org[-1], len(embedded_word_line_org[-1]))            \n",
        "            \"\"\"\n",
        "                skip the word_vec if word with vector existed\n",
        "            \"\"\"                     \n",
        "            if word not in global_words_set:\n",
        "                global_words_set.add(word)\n",
        "                write_append_data_to_txt_file(full_embedded_word_vec_dictionary, embedded_word_line_org[:-1])\n",
        "                              "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Replace origin data by tokenized_words (with _)\n",
        "    ## 1. create the dictionary of token \n",
        "        {'code-123': {'token word': 'token_word' }}  \n",
        "    ## 2. replace base on <dictionary of token >\n",
        "        {'code-123': {'token word': 'token_word' }}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. create the dictionary of token  \n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs\"\n",
        "# STILL error on BERT UNK handling after\n",
        "split_sentence_with_token_from_paragraph = f'{folder}/split_sentence_with_token_from_paragraph.txt'\n",
        "valid_embedded_files_path = f'{folder}/common_info_embedd_files/valid_embedded_files_list.txt'\n",
        "full_dict_for_replace_token_path = f'{folder}/common_info_embedd_files/full_dict_for_replace_token.json'\n",
        "\n",
        "\n",
        "\n",
        "valid_file_ls = []\n",
        "with open(valid_embedded_files_path,'r') as valid_file:\n",
        "    for vf in valid_file:\n",
        "        valid_file_ls.append(vf[:-1])\n",
        "print(\"* \"*50)\n",
        "print(valid_file_ls)\n",
        "\n",
        "n = 0\n",
        "full_dict = {}\n",
        "with open(split_sentence_with_token_from_paragraph,'r') as tokened_sentence_file:\n",
        "    for token_line in tokened_sentence_file:\n",
        "        # n+=1\n",
        "        # if n > 7: continue\n",
        "        splited_ls = token_line.split(\"$$##$$$$##$$\")\n",
        "        code = splited_ls[0]     \n",
        "        print(code)   \n",
        "        if code in valid_file_ls:\n",
        "            full_dict[code] = {}\n",
        "            # full_dict[code][\"list_embedded_words\"] = []\n",
        "            for a_sent in splited_ls[1:]:\n",
        "                print(\">>> \", a_sent)\n",
        "                for wk in a_sent.split(\" \"):    \n",
        "                    if \"_\" in wk:\n",
        "                        org_word = wk.replace(\"_\", \" \")\n",
        "                        full_dict[code][org_word] = wk\n",
        "                        # full_dict[code][\"list_embedded_words\"].append(org_word)\n",
        "\n",
        "print(\"* \"*50)\n",
        "print(full_dict)\n",
        "            \n",
        "                \n",
        "\n",
        "import json\n",
        " \n",
        "# Serializing json\n",
        "json_object = json.dumps(full_dict, indent=4)\n",
        "\n",
        "\n",
        "clear_file([full_dict_for_replace_token_path])\n",
        "with open(full_dict_for_replace_token_path, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. create the dictionary of token  \n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "import json\n",
        "\n",
        "folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs\"\n",
        "full_dict_for_replace_token_path = f'{folder}/common_info_embedd_files/full_dict_for_replace_token.json'\n",
        "\n",
        "f = open(full_dict_for_replace_token_path)\n",
        "data = json.load(f)\n",
        "# print(data)\n",
        "\n",
        "d = data['23352535']\n",
        "\n",
        "print(data['23352535'])\n",
        "print(d['Công an'])\n",
        "\n",
        "            \n",
        "# relation_types_ls = [\n",
        "#     \"AFFILIATION\",\n",
        "#     \"LOCATED\",\n",
        "#     \"PART - WHOLE\",\n",
        "#     \"PERSONAL - SOCIAL\",\n",
        "# ]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replace the embedding word in origin "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 1. create the dictionary of token  \n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "import json\n",
        "from underthesea import sent_tokenize, word_tokenize\n",
        "\n",
        "\n",
        "folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs\"\n",
        "# STILL error on BERT UNK handling after\n",
        "split_sentence_with_token_from_paragraph = f'{folder}/split_sentence_with_token_from_paragraph.txt'\n",
        "valid_embedded_files_path = f'{folder}/common_info_embedd_files/valid_embedded_files_list.txt'\n",
        "full_dict_for_replace_token_path = f'{folder}/common_info_embedd_files/full_dict_for_replace_token.json'\n",
        "\n",
        "replace_embedded_file = f'{folder}/common_info_embedd_files/CDR_DevelopmentSet.PubTator.txt'\n",
        "clear_file([replace_embedded_file])\n",
        "\n",
        "f = open(full_dict_for_replace_token_path)\n",
        "full_dict = json.load(f)\n",
        "\n",
        "valid_file_ls = []\n",
        "with open(valid_embedded_files_path,'r') as valid_file:\n",
        "    for vf in valid_file:\n",
        "        valid_file_ls.append(vf[:-1])\n",
        "print(\"* \"*50)\n",
        "print(valid_file_ls)\n",
        "org_docs_path = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/CDR_DevelopmentSet.PubTator.txt\"\n",
        "\n",
        "n = 0\n",
        "with open(org_docs_path,'r') as org_docs_file:\n",
        "    for org_docs_line in org_docs_file:\n",
        "        # if n > 25: break\n",
        "        # n+=1\n",
        "        new_line = org_docs_line\n",
        "        code = org_docs_line[0:8]\n",
        "        # print(code)\n",
        "        if not(code in valid_file_ls):\n",
        "            continue\n",
        "        word_after_code =  org_docs_line[8:11]\n",
        "        # print(org_docs_line)\n",
        "        if word_after_code == '|t|':\n",
        "            new_line='\\n'+org_docs_line\n",
        "        elif word_after_code == '|a|':\n",
        "            print(\"a \"*40)\n",
        "            new_line_tk = sent_tokenize(org_docs_line[11:])\n",
        "            \n",
        "            new_line_str = \"\"\n",
        "            for nl in new_line_tk:\n",
        "                nl = nl.replace(\"\\xa0\", \" \")\n",
        "                if nl[-1] == \".\":\n",
        "                    nl = nl[:-1]\n",
        "                print(nl)\n",
        "                new_line_str+=\" .|\"+nl\n",
        "            print(new_line_str)\n",
        "            last_char = \" .\\n\"\n",
        "            if new_line[-1] == \" \":\n",
        "                last_char = \".\\n\"\n",
        "                \n",
        "            new_line = new_line[:12]+new_line_str[4:]+ last_char\n",
        "                \n",
        "            for w_org, w_emb in full_dict[code].items():\n",
        "                new_line = new_line.replace(w_org, w_emb)\n",
        "        else:\n",
        "            line_ls_tab_split = org_docs_line.split('\\t')\n",
        "            # print(line_ls_tab_split[1])\n",
        "            if len(line_ls_tab_split) > 4 and  line_ls_tab_split[4] in \\\n",
        "                    [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "                ent_word = line_ls_tab_split[3]\n",
        "                # print(ent_word, \">>> \"+line_ls_tab_split[4], full_dict[code].get(ent_word))\n",
        "                for w_org, w_emb in full_dict[code].items():\n",
        "                    if w_org in ent_word:\n",
        "                        new_line = new_line.replace(w_org, w_emb)\n",
        "                        # break\n",
        "                    \n",
        "        # print(len(new_line), len(org_docs_line))\n",
        "        # print(new_line)\n",
        "        # print(org_docs_line)\n",
        "        write_append_data_to_txt_file(replace_embedded_file, new_line[:-1])\n",
        "                    \n",
        "                    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt_split_ls = [\n",
        "    # \"Etoposide - related myocardial infarction .|The occurrence of a myocardial infarction is reported after chemotherapy containing etoposide , in a man with no risk factors for coronary heart disease .|Possible causal mechanisms are discussed .\",\n",
        "    # \"Fatal haemorrhagic myocarditis secondary to cyclophosphamide therapy .|Haemorrhagic myocarditis is a rare but important complication of cyclophosphamide therapy .|Echocardiographic identification of the disorder can be made .|We believe that the ultrasound features of this disorder have not been previously reported .\",\n",
        "    # \"Why may epsilon - aminocaproic acid ( EACA ) induce myopathy in man ? Report of a case and literature review .|A case of necrotizing myopathy due to a short epsilon - aminocaproic acid ( EACA ) treatment in a 72 year - old patient with subarachnoid haemorrhage ( SAH ) is described .|Pathogenetic hypotheses are discussed .\"\n",
        "    # \"Obsolete but dangerous antacid preparations .|One case of acute hypercalcaemia and two of recurrent nephrolithiasis are reported in patients who had regularly consumed large amounts of calcium carbon - ate - sodium bicarbonate powders for more than 20 years .|The powders had been obtained from pharmacists unknown to the patients ' medical practitioners .|It is suggested that these preparations were responsible for the patient 's problems , and that such powders should no longer be freely obtainable .\",\n",
        "    \"Hepatic veno - occlusive disease caused by 6 - thioguanine .|Clinically reversible veno - occlusive disease of the liver developed in a 23 - year - old man with acute lymphocytic leukemia after 10 months of maintenance therapy with 6 - thioguanine .|Serial liver biopsies showed the development and resolution of intense sinusoidal engorgement .|Although this disease was clinically reversible , some subintimal fibrosis about the terminal hepatic veins persisted .|This case presented a unique opportunity to observe the histologic features of clinically reversible hepatic veno - occlusive disease over time , and may be the first case of veno - occlusive related solely to 6 - thioguanine .\",\n",
        "]\n",
        "word_slicing_ls = [\n",
        "    # [\"0:17\t1:18\",  \"27\t30\"],\n",
        "    # [\"5:17\t6:18\", \"1:8\t3:10\" ],\n",
        "    # [\"2:7:31:36\t6:8:35:37\", \"10:25\t11:27\", \"25\t27\", \"47:50\t49:51\"],\n",
        "    # [\"27\t31\", \"10\t11\", \"32\t34\", \"15\t16\"],\n",
        "    [\"7:40:109\t10:43:112\", \"0:13:88\t5:20:93\", \"30\t33\", \"66\t67\"]\n",
        "]\n",
        "\n",
        "for item in  zip(txt_split_ls, word_slicing_ls):\n",
        "    print(\"=\"*70)\n",
        "    txt_split, word_slicing = item[0], item[1]\n",
        "    txt_split = \" \".join(txt_split.split(\".|\")).split(\" \")\n",
        "    print(txt_split)\n",
        "\n",
        "    for wi in word_slicing:\n",
        "        wi = wi.split('\\t')\n",
        "        wi_start = wi[0].split(\":\")\n",
        "        wi_end = wi[1].split(\":\")\n",
        "        for w in zip(wi_start, wi_end):\n",
        "            print(w[0], w[1])\n",
        "            print(txt_split[int(w[0]):int(w[1])])\n",
        "            # print(w[0], w[1])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PROCESS TO DATA PROCESSED DATA\n",
        "    fx: 11:44:52 12:45:55"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "INPUT: \n",
        "    - all docs, 1 paragraph - 1 line (filter (no-relation) raw data without any processing like: remove dot, quote ...)\n",
        "    - All_sentence.PubTator.txt\n",
        "    \n",
        "OUTPUT: \n",
        "    - all docs, 1 paragraph - with splited sentences with struct: <code><$$##$$$$##$$><sent><$$##$$$$##$$><sent>\n",
        "    - saved to <split_sentence_from_paragraph.txt>\n",
        "    \n",
        "# HOW \n",
        "split to sentence smartly by underthesea (recognize 20.10, TP.HCM ...)\n",
        "replace \\xa0 (bug by text after processed by underthesea)\n",
        "\n",
        "\"\"\"\n",
        "file_name = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/CDR_DevelopmentSet.PubTator.txt\"\n",
        "\n",
        "from underthesea import sent_tokenize, word_tokenize\n",
        "\n",
        "def split_sentence_from_paragraph(filename):\n",
        "\n",
        "    stop_line = 9999\n",
        "    current_line = 0\n",
        "    # clear_file(out_put)\n",
        "    # clear_file(out_put_with_token)\n",
        "\n",
        "    with open(filename, \"r\") as file:\n",
        "        n = 1\n",
        "        for line in file:\n",
        "            current_line+=1\n",
        "            if current_line > stop_line:\n",
        "                break\n",
        "            \n",
        "            # print(line)\n",
        "            code_ = line[0:8]\n",
        "            # print(code_)\n",
        "            # print(\"code_ \", code_)\n",
        "            if line[8:11] == '|a|':\n",
        "                if n > 1: break\n",
        "                n+=1\n",
        "                print(line[11:] )\n",
        "                line = line[11:]\n",
        "                sentence_list = sent_tokenize(line)\n",
        "                print(sentence_list)\n",
        "                for s in sentence_list:\n",
        "                    print(s)\n",
        "\n",
        "split_sentence_from_paragraph(file_name)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### START TO CREATE PROCESSED FILE\n",
        "# sample:\n",
        "625456\tObsolete ... preparations .|One case ... years .|\n",
        "\n",
        "1:CID:2\tR2L\tNON-CROSS\t27-31\t10-11\t\n",
        "    D002119\tcalcium carbon - ate\tChemical\t27\t31\t1\t\n",
        "    D006934\thypercalcaemia\tDisease\t10\t11\t1\t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "\n",
        "special_split_sent_not_final_path = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/CDR_DevelopmentSet.PubTator.txt\"\n",
        "\n",
        "n = 0\n",
        "with open(special_split_sent_not_final_path,'r') as _org_docs_file:\n",
        "    current_code = None\n",
        "    prev_code = None\n",
        "    passage = None\n",
        "    rel_dict = {}\n",
        "    ent_dict = {}\n",
        "    passage_sents_ls = []\n",
        "    range_sent = []\n",
        "    for org_docs_line in _org_docs_file:\n",
        "        # if n > 10000: break\n",
        "        # n+=1\n",
        "        if len(org_docs_line) == 1:\n",
        "            # passage_splt = passage.split(\" \")\n",
        "            # pprint(passage_sents_ls)\n",
        "            \n",
        "            break_main_loop = False\n",
        "            for k,v in ent_dict.items():\n",
        "                if len(v[\"data\"]) != v[\"real_count\"]:\n",
        "                    print(passage_sents_ls)\n",
        "                    print(len(v[\"data\"]), v[\"real_count\"])\n",
        "                    for i in v[\"data\"]:\n",
        "                        print(passage_sents_ls[i[0]:i[-1]])\n",
        "                        break_main_loop = True           \n",
        "                        break\n",
        "            if break_main_loop:\n",
        "                for k,v in ent_dict.items():\n",
        "                    if len(v[\"data\"]) != v[\"real_count\"]:\n",
        "                        for i in v[\"data\"]:\n",
        "                            print(passage_sents_ls[i[0]:i[-1]])\n",
        "                break\n",
        "\n",
        "            current_code = None\n",
        "            passage = None\n",
        "            rel_dict = {}\n",
        "            ent_dict = {}\n",
        "            passage_sents_ls = []\n",
        "            range_sent = []\n",
        "            continue\n",
        "\n",
        "        code = org_docs_line[0:8]\n",
        "        if current_code != None:\n",
        "            if current_code != code:\n",
        "                print(\"* \"*100)\n",
        "                print(\"ERROR \"*10)\n",
        "                print(\"code \"*10)\n",
        "                print(\"* \"*100)        \n",
        "                break\n",
        "        else:\n",
        "            current_code = code  \n",
        "        # print(n, org_docs_line[:-1])\n",
        "        # print(passage[:-1])    \n",
        "        word_after_code =  org_docs_line[8:11]\n",
        "        if word_after_code == '|t|':\n",
        "            pass\n",
        "        elif word_after_code == '|a|':\n",
        "            passage = org_docs_line[11:]\n",
        "            passage_sents_ls = passage.split(\".|\")    \n",
        "            passage_sents_ls = (\" \".join(passage_sents_ls)).split(\" \")\n",
        "                \n",
        "            # len(\".|\") is 2\n",
        "            range_sent = [[0, len(sents_ls[0])+2]]\n",
        "\n",
        "            i = 0\n",
        "            prev_len = 0\n",
        "            current_len = 0\n",
        "            for s in sents_ls:\n",
        "                # ignore 1st\n",
        "                if i == 0:\n",
        "                    i+=1\n",
        "                    continue\n",
        "                current_len = range_sent[i-1][1]\n",
        "                prev_len = current_len + len(s) + 2\n",
        "\n",
        "                range_sent.append((current_len, prev_len))\n",
        "                # print(i, current_len, prev_len)\n",
        "                # print(len(s)+current_len, \" /// \", s)\n",
        "                i+=1\n",
        "            # m = 0\n",
        "            # range_sent_extend = []\n",
        "            # for i in  range_sent:\n",
        "            #     # print(sents_ls[m])\n",
        "            #     # print(passage[i[0]:i[1]])\n",
        "            #     # print(len(sents_ls[m]), len(passage[i[0]:i[1]]) )\n",
        "            #     # if m == 0:\n",
        "            #     #     m+=1\n",
        "            #     #     range_sent_extend = [[i]]\n",
        "            #     #     continue\n",
        "            #     range_sent_extend.append( (m,i[0]-2*m, i[1]) )\n",
        "            #     m+=1            \n",
        "            # print(\"* \"*50)\n",
        "            # print(range_sent)\n",
        "            # print(range_sent_extend)            \n",
        "        else:\n",
        "            line_ls_tab_split = org_docs_line.split('\\t')\n",
        "            if len(line_ls_tab_split) > 4 and  line_ls_tab_split[4] in \\\n",
        "                    [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "                ent_code = line_ls_tab_split[5][:-1]\n",
        "                if ent_dict.get(ent_code, None) == None:      \n",
        "                    ent_dict[ent_code] = {}\n",
        "                    ent_dict[ent_code][\"real_count\"] = 1                   \n",
        "                    ent_dict[ent_code][\"data\"] = []\n",
        "                    ent_word = line_ls_tab_split[3]\n",
        "                    uni_idx = 0\n",
        "                    has_ent_in_sent = ent_word.split(\" \")\n",
        "\n",
        "                    for idx_w, word in enumerate(passage_sents_ls):\n",
        "                        if (has_ent_in_sent[0] == word):\n",
        "                            end_expect_idx = idx_w+len(has_ent_in_sent)-1                        \n",
        "                            if has_ent_in_sent[-1] == passage_sents_ls[end_expect_idx]:\n",
        "                                ent_dict[ent_code][\"data\"].append((idx_w, end_expect_idx+1))\n",
        "                \n",
        "                else:\n",
        "                    ent_dict[ent_code][\"real_count\"] +=1\n",
        "                    # process positon of ent ONCE only \n",
        "                    continue\n",
        "                # pass\n",
        "                \n",
        "            else:\n",
        "                err = False\n",
        "                if len(line_ls_tab_split) != 4:\n",
        "                    err = True\n",
        "                # print(line_ls_tab_split[1])\n",
        "\n",
        "                if line_ls_tab_split[1] not in \\\n",
        "                    [\"AFFILIATION\", \"LOCATED\", \"PART - WHOLE\", \"PERSONAL - SOCIAL\"]:\n",
        "                    err = True\n",
        "                # print(line_ls_tab_split[1])\n",
        "                if err:\n",
        "                    print(\"* \"*100)\n",
        "                    print(\"ERROR \"*10)\n",
        "                    print(\"code \"*10)\n",
        "                    print(\"* \"*100)        \n",
        "                    break\n",
        "                # print(line_ls_tab_split[1])\n",
        "                # print(rel_dict.get(line_ls_tab_split[1], None))\n",
        "                if rel_dict.get(line_ls_tab_split[1], None) == None:             \n",
        "                    rel_dict[line_ls_tab_split[1]] = [(line_ls_tab_split[2], line_ls_tab_split[3][:-1])]\n",
        "                else:\n",
        "                    rel_dict[line_ls_tab_split[1]].append( (line_ls_tab_split[2], line_ls_tab_split[3][:-1]) )\n",
        "            \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# split prepare pubTator big file replaced token to single file (easy to processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "import re\n",
        "\n",
        "special_split_sent_not_final_IN_path = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/CDR_DevelopmentSet.PubTator.txt\"\n",
        "# special_split_sent_not_final_IN_path = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/train_processed/CDR_TrainingSet.PubTator.txt\"\n",
        "for_processed_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "abnormal_chars_file_path = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/abnormal_chars_file\"\n",
        "clear_file([abnormal_chars_file_path])\n",
        "s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
        "s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
        "def remove_accents(input_str):\n",
        "\ts = ''\n",
        "\tfor c in input_str:\n",
        "\t\tif c in s1:\n",
        "\t\t\ts += s0[s1.index(c)]\n",
        "\t\telse:\n",
        "\t\t\ts += c\n",
        "\treturn s\n",
        "\n",
        "with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "    n = 0\n",
        "    code_ls = set()\n",
        "    for line in _org_docs_file:\n",
        "        # if n > 23: break\n",
        "        # n+=1\n",
        "        if len(line) < 8:\n",
        "            continue\n",
        "        line = line.replace(\"   \",\" \").replace(\"  \",\" \")\n",
        "        code = line[:8] # \n",
        "        file_name = for_processed_OUT_PATH+\"/\"+code\n",
        "        if line[8:11] == \"|t|\": # start=filename\n",
        "            clear_file([file_name])\n",
        "        write_append_data_to_txt_file(file_name, line[:-1])\n",
        "        code_ls.add(code)\n",
        "        \"\"\" Use for fine abnormal chars\"\"\"\n",
        "        # line_ls_tab_split_org = line.split('\\t')\n",
        "        # print(\"line_ls_tab_split >>> \", line_ls_tab_split)\n",
        "        # if len(line_ls_tab_split_org) > 4 and  line_ls_tab_split_org[4] in \\\n",
        "        #         [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "        #     line_ls_tab_split = line_ls_tab_split_org[3].replace(\"_\",\" \")\n",
        "        #     line_ls_tab_split = line_ls_tab_split.replace(\" \",\"\")\n",
        "        #     line_ls_tab_split = remove_accents(line_ls_tab_split)\n",
        "        #     if not re.match(\"^[A-Za-z0-9_-]*$\", line_ls_tab_split):\n",
        "        #         write_append_data_to_txt_file(\n",
        "        #             abnormal_chars_file_path,\n",
        "        #             line_ls_tab_split_org[3]\n",
        "        #         )\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "\n",
        "print(onlyfiles)\n",
        "                \n",
        "for idx, special_split_sent_not_final_IN_path in enumerate(onlyfiles):\n",
        "    write_append_data_to_txt_file(\n",
        "        special_split_sent_not_final_IN_path_folder+'/'+ special_split_sent_not_final_IN_path,\n",
        "        \"<EOF>\"\n",
        "    )\n",
        "            "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
