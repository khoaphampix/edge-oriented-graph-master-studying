{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import json\n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')\n",
        "\n",
        "def write_item_to_dict(dict, key, val):\n",
        "  if dict.get(key, None) == None:\n",
        "    dict[key] = {}\n",
        "    dict[key] = val\n",
        "  return dict\n",
        "\n",
        "def update_json_to_json_file(j_path, d_dict):\n",
        "    json_object = json.load( open(j_path) )\n",
        "    # print(json_object)\n",
        "    d_all = {}\n",
        "    d_all.update(json_object)\n",
        "    d_all.update(d_dict)\n",
        "    # print(d_all)\n",
        "    with open(j_path, 'w') as f:\n",
        "        json.dump(d_all, f)\n",
        "\n",
        "def write_dict_to_json_file(j_path, d_dict):\n",
        "    # print(d_all)\n",
        "    with open(j_path, 'w') as f:\n",
        "        json.dump(d_dict, f)\n",
        "\n",
        "def open_json_file_as_dict(j_path):\n",
        "    json_object = json.load( open(j_path) )\n",
        "    return json_object\n",
        "\n",
        "\n",
        "\n",
        "def replace2symbol(string):\n",
        "    string =  string.replace('”', '\"')\\\n",
        "                    .replace('“', \"'\")\\\n",
        "                    .replace(' ', ' ')\\\n",
        "                    .replace('’', \"'\")\\\n",
        "                    .replace('–', '-')\\\n",
        "                    .replace('‘', \"'\")\\\n",
        "                    .replace('‑', '-')\\\n",
        "                    .replace('»', '\"')\\\n",
        "                    .replace('—', '-')\\\n",
        "                    .replace('¾', '')\\\n",
        "                    .replace('²', '')\\\n",
        "                    .replace('…', '')\\\n",
        "                    .replace('½', '')\\\n",
        "                    .replace('\\u200b', ' ')\\\n",
        "                    .replace(\"\\ufeff\", '')\\\n",
        "                    .replace(\" .|_\", ' .|')\\\n",
        "                    .replace(\" .|_ \", ' .| ')\\\n",
        "                    .replace(\"___\", '_')\\\n",
        "                    .replace(\"__\", '_')\\\n",
        "                    .replace(\"\t \", \"\t\")\\\n",
        "                    .replace(\" .. \", \".\")\\\n",
        "                    .replace(\" | \", \" \")\\\n",
        "                    .replace(\"-_\", \"_\")\\\n",
        "                    .replace(\" _\", \" \")\\\n",
        "                    .replace(\"_ \", \" \")\\\n",
        "                    .replace(\"_,_\", \",\")\\\n",
        "                    .replace(\"_,\", \",\")\\\n",
        "                    .replace(\",_\", \",\")\\\n",
        "                    .replace(\"! ! ! ! !\", \"!\")\\\n",
        "                    .replace(\"! ! ! !\", \"!\")\\\n",
        "                    .replace(\"! ! !\", \"!\")\\\n",
        "                    .replace(\"! !\", \"!\")\\\n",
        "\n",
        "                    \n",
        "                    \n",
        "\n",
        "\n",
        "    \n",
        "      \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    return string\n",
        "\n",
        "def replace_space(string):\n",
        "    string =  string.replace(\"    \", \" \")\\\n",
        "                    .replace(\"   \", \" \")\\\n",
        "                    .replace(\"  \", \" \")\n",
        "    return string\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set_word = \\\n",
        "# \t{'(', '̃', '”', 'ö', '̣', '&', '̉', '́', '/', '–', \"'\", 'ï', '.', ')', ';', '‘', 'Ð', ':', '’', '+', ' ', '̀', ',', '“', '\\u200b', '\\ufeff'}\n",
        "\n",
        "set_word = \\\n",
        "\t{'(', '\"', \"'\", ')', ';', ',', '\\u200b', '\\ufeff'}\n",
        "\n",
        "set_word_dict_spare_future = {\n",
        "\t# '–':\"_\", #  \" –_\"\n",
        "\t\";\": \"\",\n",
        "\t\"“\": \"\",\n",
        "\t\"”\": \"\",\n",
        "\t# \"/\": \"\", # if first sentence\n",
        "\t\"\\u200b\":\"\",\n",
        "\t\"\\ufeff\":\"\",\n",
        "\t\"(\": \"\",\n",
        "\t\")\": \"\",\n",
        "\t\".\": \"_\",\n",
        "}\n",
        "\n",
        "set_word_dict = {\n",
        "\t\".\": \"_\",\n",
        "\t\",\": \"_\",\n",
        "\t\";\": \"_\",\n",
        "\t\"-\": \"_\",\n",
        "\t\"+\": \"_\",\n",
        "\t\"\\\\\": \"_\",\n",
        "\t\"/\": \"_\",\n",
        "\t\"$\": \"_\",\n",
        "\t\"=\": \"_\",\n",
        "\t\"/\": \"_\",\n",
        "\t\">\": \"_\",\n",
        "\t\"@\": \"_\",\n",
        "\t\"&\": \"_\",\n",
        "\t\":\": \"_\",\n",
        "}\n",
        "\n",
        "\n",
        "# pattern = r\"\"\"([.>+/&,*\\-?:;!()@\\[\\]=%\\\"'])\"\"\"\n",
        "\n",
        "\n",
        "# \t{'(', '̃', '”', 'ö', '̣', '&', '̉', '́', '/', '–', \"'\", 'ï', '.', ')', ';', '‘', 'Ð', ':', '’', '+', ' ', '̀', ',', '“', '\\u200b', '\\ufeff'}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['23351996', '23351997', '23351998', '23352000', '23352001', '23352002', '23352003', '23352006', '23352009', '23352013', '23352014', '23352016', '23352018', '23352019', '23352020', '23352021', '23352024', '23352026', '23352027', '23352028', '23352030', '23352033', '23352065', '23352066', '23352070', '23352071', '23352073', '23352075', '23352078', '23352079', '23352081', '23352084', '23352085', '23352087', '23352089', '23352090', '23352099', '23352107', '23352110', '23352117', '23352122', '23352125', '23352126', '23352143', '23352161', '23352163', '23352173', '23352190', '23352200', '23352232', '23352236', '23352239', '23352240', '23352260', '23352265', '23352283', '23352292', '23352296', '23352297', '23352298', '23352299', '23352300', '23352301', '23352303', '23352305', '23352309', '23352314', '23352316', '23352317', '23352319', '23352320', '23352321', '23352322', '23352323', '23352326', '23352327', '23352328', '23352329', '23352331', '23352332', '23352334', '23352335', '23352337', '23352343', '23352345', '23352346', '23352347', '23352348', '23352352', '23352357', '23352359', '23352361', '23352363', '23352366', '23352370', '23352372', '23352373', '23352376', '23352378', '23352381', '23352382', '23352385', '23352390', '23352393', '23352396', '23352397', '23352401', '23352402', '23352405', '23352409', '23352410', '23352411', '23352412', '23352413', '23352414', '23352415', '23352416', '23352417', '23352419', '23352421', '23352425', '23352427', '23352428', '23352429', '23352432', '23352433', '23352434', '23352436', '23352437', '23352438', '23352440', '23352442', '23352443', '23352444', '23352445', '23352448', '23352449', '23352450', '23352451', '23352453', '23352456', '23352457', '23352460', '23352461', '23352462', '23352467', '23352468', '23352470', '23352471', '23352473', '23352476', '23352477', '23352480', '23352482', '23352486', '23352487', '23352488', '23352489', '23352491', '23352492', '23352494', '23352495', '23352497', '23352498', '23352499', '23352507', '23352508', '23352517', '23352518', '23352522', '23352523', '23352524', '23352526', '23352530', '23352532', '23352535', '23352536', '23352538', '23352539', '23352540', '23352543', '23352545', '23352546', '23352548', '23352550', '23352551', '23352552', '23352560', '23352563', '23352568', '23352571', '23352572', '23352573', '23352575', '23352577', '23352578', '23352580', '23352581', '23352583', '23352585', '23352586', '23352588', '23352589', '23352590', '23352591', '23352592', '23352593', '23352594', '23352596', '23352597', '23352600', '23352601', '23352602', '23352603', '23352605', '23352609', '23352610', '23352612', '23352614', '23352615', '23352617', '23352618', '23352619', '23352620', '23352623', '23352624', '23352625', '23352626', '23352628', '23352629', '23352631', '23352634', '23352635', '23352637', '23352640', '23352641', '23352642', '23352644', '23352648', '23352649', '23352650', '23352651', '23352653', '23352654', '23352733', '23352736']\n",
            "count_abnomal:  0\n"
          ]
        }
      ],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "# clear_file([shortEnt_in_longEntity_OUT_PATH])\n",
        "\n",
        "\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "onlyfiles = sorted(onlyfiles)\n",
        "print(onlyfiles)\n",
        "\n",
        "count_abnomal = 0\n",
        "for idx, doc_code in enumerate(onlyfiles):\n",
        "\n",
        "    \n",
        "    # if idx > 1:\n",
        "    #     break\n",
        "    # print(doc_code)\n",
        "    special_split_sent_not_final_IN_path = special_split_sent_not_final_IN_path_folder+'/'+ doc_code\n",
        "    with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "        doc_list = []\n",
        "        for org_docs_line in _org_docs_file:\n",
        "            org_docs_line = org_docs_line[:-1]\n",
        "            # org_docs_line = replace2symbol(org_docs_line)\n",
        "            # org_docs_line = replace_space(org_docs_line)\n",
        "            word_after_code =  org_docs_line[8:11]\n",
        "            if  word_after_code not in ('|a|', '|t|'):\n",
        "                line_ls_tab_split = org_docs_line.split('\\t')                \n",
        "                \"\"\"\n",
        "                    process to add entity to list entity\n",
        "                \"\"\"\n",
        "                if len(line_ls_tab_split) > 4 and  line_ls_tab_split[4] in \\\n",
        "                        [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "                    ent_word = line_ls_tab_split[3]\n",
        "                    new_ent_word = ent_word\n",
        "                    if any(char in set_word for char in ent_word):\n",
        "                        # print(char in set_word for char in ent_word)\n",
        "                        list_abnormal = []\n",
        "                        for c in set_word:\n",
        "                            if c in ent_word:\n",
        "                                list_abnormal.append(c)\n",
        "                        if list_abnormal:\n",
        "                            count_abnomal+=1\n",
        "                            # print(list_abnormal) \n",
        "                            # new_ent_word = \"\"\n",
        "                            for c in list_abnormal:\n",
        "                                new_ent_word = ent_word.replace(c,\"\")\n",
        "                                print(new_ent_word, len(new_ent_word))\n",
        "                                print(ent_word, len(ent_word))\n",
        "                            # print(doc_list[1])\n",
        "                            # print(doc_list[1].find(ent_word))\n",
        "                            # print(doc_list[1][doc_list[1].find(ent_word):doc_list[1].find(ent_word)+len(ent_word)])\n",
        "\n",
        "\n",
        "                    if any(char in set_word_dict for char in new_ent_word):\n",
        "                        list_abnormal = []\n",
        "                        for c in set_word_dict:\n",
        "                            if c in ent_word:\n",
        "                                list_abnormal.append(c)\n",
        "                        if list_abnormal:\n",
        "                            print(\"list_abnormal \", list_abnormal)\n",
        "                            count_abnomal+=1\n",
        "                            # print(list_abnormal) \n",
        "                            # new_ent_word = \"\"\n",
        "                            for c in list_abnormal:\n",
        "                                new_ent_word = new_ent_word.replace(c,set_word_dict[c])\n",
        "                                print(new_ent_word, len(new_ent_word))\n",
        "                                print(ent_word, len(ent_word))\n",
        "                    \n",
        "                    if new_ent_word[0] == \"_\":\n",
        "                        new_ent_word = new_ent_word[1:]\n",
        "\n",
        "                    if new_ent_word[-1] == \"_\":\n",
        "                        new_ent_word = new_ent_word[:-1]\n",
        "\n",
        "                    new_ent_word = new_ent_word.strip()\n",
        "\n",
        "                    if new_ent_word != ent_word:\n",
        "                        doc_list[1] = doc_list[1].replace(f\" {ent_word} \",f\" {new_ent_word} \")\n",
        "                        doc_list[1] = doc_list[1].replace(f\".|{ent_word} \",f\".|{new_ent_word} \")\n",
        "                        org_docs_line = org_docs_line.replace(ent_word,new_ent_word)\n",
        "\n",
        "                            # print(doc_list[1].find(ent_word))\n",
        "                            # print(doc_list[1].find(new_ent_word))                            \n",
        "\n",
        "                        # print(ent_word)\n",
        "                        # org_docs_line = org_docs_line.replace()\n",
        "\n",
        "\n",
        "            org_docs_line = replace2symbol(org_docs_line)\n",
        "            org_docs_line = replace_space(org_docs_line)            \n",
        "            doc_list.append(org_docs_line)\n",
        "\n",
        "        file_name = special_split_sent_not_final_IN_path_folder+\"/\"+doc_code\n",
        "        clear_file([file_name])        \n",
        "        for line_to_write in doc_list:\n",
        "            \"\"\"\n",
        "                replace abnormal words\n",
        "            \"\"\"\n",
        "            write_append_data_to_txt_file(file_name, line_to_write)\n",
        "        # pprint(doc_list)\n",
        "        # print(doc_list[1])\n",
        "        # print(doc_list[2])\n",
        "print(f\"count_abnomal:  {count_abnomal}\")\n",
        "        \n",
        "       \n",
        "# # Serializing json\n",
        "# json_object = json.dumps(shortEnt_in_longEntity_json, indent=4)\n",
        "# with open(shortEnt_in_longEntity_OUT_PATH, \"w\") as outfile:\n",
        "#     outfile.write(json_object)\n",
        "\n",
        "                                            \n",
        "                            \n",
        "                        # write_append_data_to_txt_file(processed_error_files_OUT_PATH, doc_code)\n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### START TO CREATE PROCESSED FILE\n",
        "# sample:\n",
        "625456\tObsolete ... preparations .|One case ... years .|\n",
        "\n",
        "1:CID:2\tR2L\tNON-CROSS\t27-31\t10-11\t\n",
        "    D002119\tcalcium carbon - ate\tChemical\t27\t31\t1\t\n",
        "    D006934\thypercalcaemia\tDisease\t10\t11\t1\t\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
