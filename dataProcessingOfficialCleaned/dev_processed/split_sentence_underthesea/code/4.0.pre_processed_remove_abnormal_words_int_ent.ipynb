{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import json\n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')\n",
        "\n",
        "def write_item_to_dict(dict, key, val):\n",
        "  if dict.get(key, None) == None:\n",
        "    dict[key] = {}\n",
        "    dict[key] = val\n",
        "  return dict\n",
        "\n",
        "def update_json_to_json_file(j_path, d_dict):\n",
        "    json_object = json.load( open(j_path) )\n",
        "    # print(json_object)\n",
        "    d_all = {}\n",
        "    d_all.update(json_object)\n",
        "    d_all.update(d_dict)\n",
        "    # print(d_all)\n",
        "    with open(j_path, 'w') as f:\n",
        "        json.dump(d_all, f)\n",
        "\n",
        "def write_dict_to_json_file(j_path, d_dict):\n",
        "    # print(d_all)\n",
        "    with open(j_path, 'w') as f:\n",
        "        json.dump(d_dict, f)\n",
        "\n",
        "def open_json_file_as_dict(j_path):\n",
        "    json_object = json.load( open(j_path) )\n",
        "    return json_object\n",
        "\n",
        "\n",
        "\n",
        "def replace2symbol(string):\n",
        "    string =  string.replace('”', '\"')\\\n",
        "                    .replace('“', \"'\")\\\n",
        "                    .replace(' ', ' ')\\\n",
        "                    .replace('’', \"'\")\\\n",
        "                    .replace('–', '-')\\\n",
        "                    .replace('‘', \"'\")\\\n",
        "                    .replace('‑', '-')\\\n",
        "                    .replace('»', '\"')\\\n",
        "                    .replace('—', '-')\\\n",
        "                    .replace('¾', '')\\\n",
        "                    .replace('²', '')\\\n",
        "                    .replace('…', '')\\\n",
        "                    .replace('½', '')\\\n",
        "                    .replace('\\u200b', ' ')\\\n",
        "                    .replace(\"\\ufeff\", '')\\\n",
        "                    .replace(\" .|_\", ' .|')\\\n",
        "                    .replace(\" .|_ \", ' .| ')\\\n",
        "                    .replace(\"___\", '_')\\\n",
        "                    .replace(\"__\", '_')\\\n",
        "                    .replace(\"\t \", \"\t\")\\\n",
        "                    .replace(\" .. \", \".\")\\\n",
        "                    .replace(\" | \", \" \")\\\n",
        "                    .replace(\"-_\", \"_\")\\\n",
        "                    .replace(\" _\", \" \")\\\n",
        "                    .replace(\"_ \", \" \")\n",
        "    \n",
        "\n",
        "    \n",
        "      \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    return string\n",
        "\n",
        "def replace_space(string):\n",
        "    string =  string.replace(\"    \", \" \")\\\n",
        "                    .replace(\"   \", \" \")\\\n",
        "                    .replace(\"  \", \" \")\n",
        "    return string\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set_word = \\\n",
        "# \t{'(', '̃', '”', 'ö', '̣', '&', '̉', '́', '/', '–', \"'\", 'ï', '.', ')', ';', '‘', 'Ð', ':', '’', '+', ' ', '̀', ',', '“', '\\u200b', '\\ufeff'}\n",
        "\n",
        "set_word = \\\n",
        "\t{'(', '\"', \"'\", ')', ';', ',', '\\u200b', '\\ufeff'}\n",
        "\n",
        "set_word_dict_spare_future = {\n",
        "\t# '–':\"_\", #  \" –_\"\n",
        "\t\";\": \"\",\n",
        "\t\"“\": \"\",\n",
        "\t\"”\": \"\",\n",
        "\t# \"/\": \"\", # if first sentence\n",
        "\t\"\\u200b\":\"\",\n",
        "\t\"\\ufeff\":\"\",\n",
        "\t\"(\": \"\",\n",
        "\t\")\": \"\",\n",
        "\t\".\": \"_\",\n",
        "}\n",
        "\n",
        "set_word_dict = {\n",
        "\t\".\": \"_\",\n",
        "\t\",\": \"_\",\n",
        "\t\";\": \"_\",\n",
        "\t\"-\": \"_\",\n",
        "\t\"+\": \"_\",\n",
        "\t\"\\\\\": \"_\",\n",
        "\t\"/\": \"_\",\n",
        "\t\"$\": \"_\",\n",
        "\t\"=\": \"_\",\n",
        "\t\"/\": \"_\",\n",
        "\t\">\": \"_\",\n",
        "\t\"@\": \"_\",\n",
        "\t\"&\": \"_\",\n",
        "\t\":\": \"_\",\n",
        "}\n",
        "\n",
        "\n",
        "# pattern = r\"\"\"([.>+/&,*\\-?:;!()@\\[\\]=%\\\"'])\"\"\"\n",
        "\n",
        "\n",
        "# \t{'(', '̃', '”', 'ö', '̣', '&', '̉', '́', '/', '–', \"'\", 'ï', '.', ')', ';', '‘', 'Ð', ':', '’', '+', ' ', '̀', ',', '“', '\\u200b', '\\ufeff'}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "# clear_file([shortEnt_in_longEntity_OUT_PATH])\n",
        "\n",
        "\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "onlyfiles = sorted(onlyfiles)\n",
        "print(onlyfiles)\n",
        "\n",
        "count_abnomal = 0\n",
        "for idx, doc_code in enumerate(onlyfiles):\n",
        "\n",
        "    \n",
        "    # if idx > 1:\n",
        "    #     break\n",
        "    # print(doc_code)\n",
        "    special_split_sent_not_final_IN_path = special_split_sent_not_final_IN_path_folder+'/'+ doc_code\n",
        "    with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "        doc_list = []\n",
        "        for org_docs_line in _org_docs_file:\n",
        "            org_docs_line = org_docs_line[:-1]\n",
        "            # org_docs_line = replace2symbol(org_docs_line)\n",
        "            # org_docs_line = replace_space(org_docs_line)\n",
        "            word_after_code =  org_docs_line[8:11]\n",
        "            if  word_after_code not in ('|a|', '|t|'):\n",
        "                line_ls_tab_split = org_docs_line.split('\\t')                \n",
        "                \"\"\"\n",
        "                    process to add entity to list entity\n",
        "                \"\"\"\n",
        "                if len(line_ls_tab_split) > 4 and  line_ls_tab_split[4] in \\\n",
        "                        [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "                    ent_word = line_ls_tab_split[3]\n",
        "                    new_ent_word = ent_word\n",
        "                    if any(char in set_word for char in ent_word):\n",
        "                        # print(char in set_word for char in ent_word)\n",
        "                        list_abnormal = []\n",
        "                        for c in set_word:\n",
        "                            if c in ent_word:\n",
        "                                list_abnormal.append(c)\n",
        "                        if list_abnormal:\n",
        "                            count_abnomal+=1\n",
        "                            # print(list_abnormal) \n",
        "                            # new_ent_word = \"\"\n",
        "                            for c in list_abnormal:\n",
        "                                new_ent_word = ent_word.replace(c,\"\")\n",
        "                                print(new_ent_word, len(new_ent_word))\n",
        "                                print(ent_word, len(ent_word))\n",
        "                            # print(doc_list[1])\n",
        "                            # print(doc_list[1].find(ent_word))\n",
        "                            # print(doc_list[1][doc_list[1].find(ent_word):doc_list[1].find(ent_word)+len(ent_word)])\n",
        "\n",
        "\n",
        "                    if any(char in set_word_dict for char in new_ent_word):\n",
        "                        list_abnormal = []\n",
        "                        for c in set_word_dict:\n",
        "                            if c in ent_word:\n",
        "                                list_abnormal.append(c)\n",
        "                        if list_abnormal:\n",
        "                            print(\"list_abnormal \", list_abnormal)\n",
        "                            count_abnomal+=1\n",
        "                            # print(list_abnormal) \n",
        "                            # new_ent_word = \"\"\n",
        "                            for c in list_abnormal:\n",
        "                                new_ent_word = new_ent_word.replace(c,set_word_dict[c])\n",
        "                                print(new_ent_word, len(new_ent_word))\n",
        "                                print(ent_word, len(ent_word))\n",
        "                    \n",
        "                    if new_ent_word[0] == \"_\":\n",
        "                        new_ent_word = new_ent_word[1:]\n",
        "\n",
        "                    if new_ent_word[-1] == \"_\":\n",
        "                        new_ent_word = new_ent_word[:-1]\n",
        "\n",
        "                    new_ent_word = new_ent_word.strip()\n",
        "\n",
        "                    if new_ent_word != ent_word:\n",
        "                        doc_list[1] = doc_list[1].replace(f\" {ent_word} \",f\" {new_ent_word} \")\n",
        "                        doc_list[1] = doc_list[1].replace(f\".|{ent_word} \",f\".|{new_ent_word} \")\n",
        "                        org_docs_line = org_docs_line.replace(ent_word,new_ent_word)\n",
        "\n",
        "                            # print(doc_list[1].find(ent_word))\n",
        "                            # print(doc_list[1].find(new_ent_word))                            \n",
        "\n",
        "                        # print(ent_word)\n",
        "                        # org_docs_line = org_docs_line.replace()\n",
        "\n",
        "\n",
        "            org_docs_line = replace2symbol(org_docs_line)\n",
        "            org_docs_line = replace_space(org_docs_line)            \n",
        "            doc_list.append(org_docs_line)\n",
        "\n",
        "        file_name = special_split_sent_not_final_IN_path_folder+\"/\"+doc_code\n",
        "        clear_file([file_name])        \n",
        "        for line_to_write in doc_list:\n",
        "            \"\"\"\n",
        "                replace abnormal words\n",
        "            \"\"\"\n",
        "            write_append_data_to_txt_file(file_name, line_to_write)\n",
        "        # pprint(doc_list)\n",
        "        # print(doc_list[1])\n",
        "        # print(doc_list[2])\n",
        "print(f\"count_abnomal:  {count_abnomal}\")\n",
        "        \n",
        "       \n",
        "# # Serializing json\n",
        "# json_object = json.dumps(shortEnt_in_longEntity_json, indent=4)\n",
        "# with open(shortEnt_in_longEntity_OUT_PATH, \"w\") as outfile:\n",
        "#     outfile.write(json_object)\n",
        "\n",
        "                                            \n",
        "                            \n",
        "                        # write_append_data_to_txt_file(processed_error_files_OUT_PATH, doc_code)\n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### START TO CREATE PROCESSED FILE\n",
        "# sample:\n",
        "625456\tObsolete ... preparations .|One case ... years .|\n",
        "\n",
        "1:CID:2\tR2L\tNON-CROSS\t27-31\t10-11\t\n",
        "    D002119\tcalcium carbon - ate\tChemical\t27\t31\t1\t\n",
        "    D006934\thypercalcaemia\tDisease\t10\t11\t1\t\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
