{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "import re\n",
        "import json \n",
        "\n",
        "special_split_sent_not_final_IN_path = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/CDR_DevelopmentSet.PubTator.txt\"\n",
        "# special_split_sent_not_final_IN_path = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/train_processed/CDR_TrainingSet.PubTator.txt\"\n",
        "for_processed_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "abnormal_chars_file_path = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/abnormal_chars_file\"\n",
        "clear_file([abnormal_chars_file_path])\n",
        "s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
        "s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
        "\n",
        "\n",
        "shortEnt_in_longEntity_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/shortEnt_in_longEntity.json\"\n",
        "\n",
        "\n",
        "f = open(shortEnt_in_longEntity_OUT_PATH)\n",
        "shortEnt_in_longEntity_dict = json.load(f)\n",
        "# print(data)\n",
        "\n",
        "\n",
        "def remove_accents(input_str):\n",
        "\ts = ''\n",
        "\tfor c in input_str:\n",
        "\t\tif c in s1:\n",
        "\t\t\ts += s0[s1.index(c)]\n",
        "\t\telse:\n",
        "\t\t\ts += c\n",
        "\treturn s\n",
        "\n",
        "with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "    n = 0\n",
        "    code_ls = set()\n",
        "    for line in _org_docs_file:\n",
        "        # if n > 23: break\n",
        "        # n+=1\n",
        "        if len(line) < 8:\n",
        "            continue\n",
        "        code = line[:8] # \n",
        "        file_name = for_processed_OUT_PATH+\"/\"+code\n",
        "        if line[8:11] == \"|t|\": # start=filename\n",
        "            clear_file([file_name])\n",
        "        \n",
        "        line_to_write = line[:-1]\n",
        "        \n",
        "        \"\"\"\n",
        "            replace long token ent\n",
        "        \"\"\"\n",
        "        if shortEnt_in_longEntity_dict.get(code, None) is not None:\n",
        "            long_ent_replace_ls = shortEnt_in_longEntity_dict.get(code, None)\n",
        "            for long_ent in long_ent_replace_ls:\n",
        "                ent = long_ent[\"parent\"]\n",
        "                ent_full = ent.replace(\" \",\"_\")\n",
        "                line_to_write = line_to_write.replace(ent, ent_full)\n",
        "                print(ent, ent_full)\n",
        "            \n",
        "        write_append_data_to_txt_file(file_name, line_to_write)\n",
        "        code_ls.add(code)\n",
        "        \"\"\" Use for fine abnormal chars\"\"\"\n",
        "        # line_ls_tab_split_org = line.split('\\t')\n",
        "        # print(\"line_ls_tab_split >>> \", line_ls_tab_split)\n",
        "        # if len(line_ls_tab_split_org) > 4 and  line_ls_tab_split_org[4] in \\\n",
        "        #         [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "        #     line_ls_tab_split = line_ls_tab_split_org[3].replace(\"_\",\" \")\n",
        "        #     line_ls_tab_split = line_ls_tab_split.replace(\" \",\"\")\n",
        "        #     line_ls_tab_split = remove_accents(line_ls_tab_split)\n",
        "        #     if not re.match(\"^[A-Za-z0-9_-]*$\", line_ls_tab_split):\n",
        "        #         write_append_data_to_txt_file(\n",
        "        #             abnormal_chars_file_path,\n",
        "        #             line_ls_tab_split_org[3]\n",
        "        #         )\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "\n",
        "print(onlyfiles)\n",
        "                \n",
        "for idx, special_split_sent_not_final_IN_path in enumerate(onlyfiles):\n",
        "    write_append_data_to_txt_file(\n",
        "        special_split_sent_not_final_IN_path_folder+'/'+ special_split_sent_not_final_IN_path,\n",
        "        \"<EOF>\"\n",
        "    )\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "processed_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/vlsp.data\"\n",
        "\n",
        "shortEnt_in_longEntity_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/shortEnt_in_longEntity.json\"\n",
        "\n",
        "clear_file([processed_OUT_PATH])\n",
        "clear_file([shortEnt_in_longEntity_OUT_PATH])\n",
        "\n",
        "\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "onlyfiles = sorted(onlyfiles)\n",
        "print(onlyfiles)\n",
        "\n",
        "error_count = 0\n",
        "all_docs = 0\n",
        "shortEnt_in_longEntity_json = {}\n",
        "for idx, doc_code in enumerate(onlyfiles):\n",
        "    # print(doc_code)\n",
        "    # if doc_code != '23352002':\n",
        "    #     continue\n",
        "\n",
        "    \n",
        "    all_docs+=1\n",
        "    # if idx > 3:\n",
        "    #     break\n",
        "\n",
        "    special_split_sent_not_final_IN_path = special_split_sent_not_final_IN_path_folder+'/'+ doc_code\n",
        "    with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "        ent_set = set()\n",
        "        doc_code = \"code\"\n",
        "        for org_docs_line in _org_docs_file:\n",
        "            # print(org_docs_line[:-1])\n",
        "            org_docs_line = org_docs_line[:-1]\n",
        "            word_after_code =  org_docs_line[8:11]\n",
        "            if word_after_code == '|t|':\n",
        "                doc_code = org_docs_line[0:8]  \n",
        "                pass\n",
        "            elif word_after_code == '|a|':\n",
        "                pass\n",
        "            else:\n",
        "                line_ls_tab_split = org_docs_line.split('\\t')                \n",
        "                \"\"\"\n",
        "                    process to add entity to list entity\n",
        "                \"\"\"\n",
        "                if len(line_ls_tab_split) > 4 and  line_ls_tab_split[4] in \\\n",
        "                        [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "                    ent = line_ls_tab_split[3]\n",
        "                    ent_set.add(ent)\n",
        "                    # print(ent)\n",
        "                    # print(ent)\n",
        "        # print(\"* \"*50)\n",
        "        # print(ent_set)\n",
        "        ent_list = list(ent_set)\n",
        "        \n",
        "        for ent_pivot in ent_list:\n",
        "            if ent_pivot == \"huyện Long_Hồ\":\n",
        "                print(doc_code, ent_pivot)\n",
        "            need_replace = False\n",
        "            for ent_split in ent_list:\n",
        "                if ent_pivot != ent_split:\n",
        "                    if ent_pivot in ent_split:\n",
        "                        \n",
        "                        if (ent_pivot in ent_split.split(\" \")):\n",
        "                            need_replace = True\n",
        "\n",
        "                        if need_replace == False:\n",
        "                            \"\"\"check deep case huyện Long_Hồ / TAND huyện Long_Hồ \"\"\"\n",
        "                            ent_split_new = ent_split.replace(\"ent_pivot\", \"\")\n",
        "                            if \" \" in ent_split_new:\n",
        "                                need_replace = True\n",
        "                            \n",
        "                        if need_replace:\n",
        "                            need_replace = False\n",
        "                            # print(f\"{idx} {doc_code} <{ent_pivot}> in <{ent_split}>)\")\n",
        "                            data = {\n",
        "                                \"child\" : ent_pivot,\n",
        "                                \"parent\": ent_split\n",
        "                            }\n",
        "                            print(data)\n",
        "                            if shortEnt_in_longEntity_json.get(doc_code, None) is None:\n",
        "                                shortEnt_in_longEntity_json[doc_code] = [data]\n",
        "                            else:\n",
        "                                shortEnt_in_longEntity_json[doc_code].append(data)\n",
        "                            \n",
        "\n",
        "# pprint(shortEnt_in_longEntity_json)\n",
        "\n",
        "import json\n",
        " \n",
        "# Serializing json\n",
        "json_object = json.dumps(shortEnt_in_longEntity_json, indent=4)\n",
        "with open(shortEnt_in_longEntity_OUT_PATH, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "                                            \n",
        "                            \n",
        "                        # write_append_data_to_txt_file(processed_error_files_OUT_PATH, doc_code)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "import re\n",
        "import json \n",
        "\n",
        "special_split_sent_not_final_IN_path = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/CDR_DevelopmentSet.PubTator.txt\"\n",
        "# special_split_sent_not_final_IN_path = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/train_processed/CDR_TrainingSet.PubTator.txt\"\n",
        "for_processed_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "abnormal_chars_file_path = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/abnormal_chars_file\"\n",
        "clear_file([abnormal_chars_file_path])\n",
        "s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
        "s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
        "\n",
        "\n",
        "shortEnt_in_longEntity_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/shortEnt_in_longEntity.json\"\n",
        "\n",
        "\n",
        "f = open(shortEnt_in_longEntity_OUT_PATH)\n",
        "shortEnt_in_longEntity_dict = json.load(f)\n",
        "# print(data)\n",
        "\n",
        "\n",
        "def remove_accents(input_str):\n",
        "\ts = ''\n",
        "\tfor c in input_str:\n",
        "\t\tif c in s1:\n",
        "\t\t\ts += s0[s1.index(c)]\n",
        "\t\telse:\n",
        "\t\t\ts += c\n",
        "\treturn s\n",
        "\n",
        "with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "    n = 0\n",
        "    code_ls = set()\n",
        "    for line in _org_docs_file:\n",
        "        # if n > 23: break\n",
        "        # n+=1\n",
        "        if len(line) < 8:\n",
        "            continue\n",
        "        code = line[:8] # \n",
        "        file_name = for_processed_OUT_PATH+\"/\"+code\n",
        "        if line[8:11] == \"|t|\": # start=filename\n",
        "            clear_file([file_name])\n",
        "        \n",
        "        line_to_write = line[:-1]\n",
        "        \n",
        "        \"\"\"\n",
        "            replace long token ent\n",
        "        \"\"\"\n",
        "        if shortEnt_in_longEntity_dict.get(code, None) is not None:\n",
        "            long_ent_replace_ls = shortEnt_in_longEntity_dict.get(code, None)\n",
        "            for long_ent in long_ent_replace_ls:\n",
        "                ent = long_ent[\"parent\"]\n",
        "                ent_full = ent.replace(\" \",\"_\")\n",
        "                line_to_write = line_to_write.replace(ent, ent_full)\n",
        "                print(ent, ent_full)\n",
        "            \n",
        "        write_append_data_to_txt_file(file_name, line_to_write)\n",
        "        code_ls.add(code)\n",
        "        \"\"\" Use for fine abnormal chars\"\"\"\n",
        "        line_ls_tab_split_org = line.split('\\t')\n",
        "        print(\"line_ls_tab_split >>> \", line_ls_tab_split)\n",
        "        if len(line_ls_tab_split_org) > 4 and  line_ls_tab_split_org[4] in \\\n",
        "                [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "            line_ls_tab_split = line_ls_tab_split_org[3].replace(\"_\",\" \")\n",
        "            line_ls_tab_split = line_ls_tab_split.replace(\" \",\"\")\n",
        "            line_ls_tab_split = remove_accents(line_ls_tab_split)\n",
        "            if not re.match(\"^[A-Za-z0-9_-]*$\", line_ls_tab_split):\n",
        "                write_append_data_to_txt_file(\n",
        "                    abnormal_chars_file_path,\n",
        "                    line_ls_tab_split_org[3]\n",
        "                )\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "\n",
        "print(onlyfiles)\n",
        "                \n",
        "for idx, special_split_sent_not_final_IN_path in enumerate(onlyfiles):\n",
        "    write_append_data_to_txt_file(\n",
        "        special_split_sent_not_final_IN_path_folder+'/'+ special_split_sent_not_final_IN_path,\n",
        "        \"<EOF>\"\n",
        "    )\n",
        "            "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### START TO CREATE PROCESSED FILE\n",
        "# sample:\n",
        "625456\tObsolete ... preparations .|One case ... years .|\n",
        "\n",
        "1:CID:2\tR2L\tNON-CROSS\t27-31\t10-11\t\n",
        "    D002119\tcalcium carbon - ate\tChemical\t27\t31\t1\t\n",
        "    D006934\thypercalcaemia\tDisease\t10\t11\t1\t\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
