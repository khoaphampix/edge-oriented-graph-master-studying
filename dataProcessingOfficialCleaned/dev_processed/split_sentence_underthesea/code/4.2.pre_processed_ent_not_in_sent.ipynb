{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import json\n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')\n",
        "\n",
        "def write_item_to_dict(dict, key, val):\n",
        "  if dict.get(key, None) == None:\n",
        "    dict[key] = {}\n",
        "    dict[key] = val\n",
        "  return dict\n",
        "\n",
        "def update_json_to_json_file(j_path, d_dict):\n",
        "    json_object = json.load( open(j_path) )\n",
        "    # print(json_object)\n",
        "    d_all = {}\n",
        "    d_all.update(json_object)\n",
        "    d_all.update(d_dict)\n",
        "    # print(d_all)\n",
        "    with open(j_path, 'w') as f:\n",
        "        json.dump(d_all, f)\n",
        "\n",
        "def write_dict_to_json_file(j_path, d_dict):\n",
        "    # print(d_all)\n",
        "    with open(j_path, 'w') as f:\n",
        "        json.dump(d_dict, f)\n",
        "\n",
        "def open_json_file_as_dict(j_path):\n",
        "    json_object = json.load( open(j_path) )\n",
        "    return json_object\n",
        "\n",
        "def replace_space(string):\n",
        "    string =  string.replace(\"    \", \" \")\\\n",
        "                    .replace(\"   \", \" \")\\\n",
        "                    .replace(\"  \", \" \")\n",
        "    return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    desc: create json file from text for replace \"entity NOT in sentences\"\n",
        "    IN: txt \n",
        "    OUT: json {'doc': 'ent_code': <data>}\n",
        "\"\"\"\n",
        "\n",
        "error_ent_folder = \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/error/manuall_edited\"\n",
        "error_ent_txt_file_path = f\"{error_ent_folder}/ent_not_in_set\"\n",
        "error_ent_json_file_path = f\"{error_ent_folder}/ent_not_in_set.json\"\n",
        "\n",
        "ent_not_in_sent_dict  = {}\n",
        "with open(error_ent_txt_file_path,'r') as _ent_file:\n",
        "    for line in _ent_file:\n",
        "        \n",
        "        splited_line = line[:-1].split(\"\\t\\t\") #.replace(\"\\xa0\", \" \")\n",
        "        doc_code = splited_line[0]\n",
        "        if line[:5] == '<new>' or line[:5] == '<man>':\n",
        "            # print(line[:5])\n",
        "            continue\n",
        "        \n",
        "        if len(splited_line) < 3:\n",
        "            continue\n",
        "        ent_code = splited_line[2]\n",
        "        ent = splited_line[3]\n",
        "        ent_org, ent_place = ent.split(\"|$|\")\n",
        "        # ent_org = ent_org.replace(\"\\xa0\", \" \") \n",
        "        word_related_to_ent_in_sent = splited_line[4:]\n",
        "        word_related_to_ent_in_sent = [i.split(\"|$|\") for i in word_related_to_ent_in_sent]\n",
        "        # print(splited_line)\n",
        "        # print(doc_code)\n",
        "        # print(ent_code)\n",
        "        # print(ent)\n",
        "        # print(\"- \"*30)\n",
        "        # print(ent_org, ent_place)\n",
        "        # print(word_related_to_ent_in_sent)\n",
        "        if ent_not_in_sent_dict.get(doc_code) == None:\n",
        "            ent_not_in_sent_dict[doc_code] = {}\n",
        "            ent_not_in_sent_dict[doc_code][ent_code] = {}\n",
        "            ent_not_in_sent_dict[doc_code][ent_code][\"ent\"] = {\"word\": ent_org, \"word_place\":ent_place}\n",
        "            ent_not_in_sent_dict[doc_code][ent_code][\"word_related_to_ent_in_sent\"] = word_related_to_ent_in_sent\n",
        "        else:\n",
        "            if ent_not_in_sent_dict[doc_code].get(ent_code, None) == None:\n",
        "                ent_not_in_sent_dict[doc_code][ent_code] = {}\n",
        "                ent_not_in_sent_dict[doc_code][ent_code][\"ent\"] = {\"word\": ent_org, \"word_place\":ent_place}\n",
        "                ent_not_in_sent_dict[doc_code][ent_code][\"word_related_to_ent_in_sent\"] = word_related_to_ent_in_sent\n",
        "            else:\n",
        "                ent_not_in_sent_dict[doc_code][ent_code][\"word_related_to_ent_in_sent\"] += word_related_to_ent_in_sent\n",
        "\n",
        "        # break\n",
        "write_dict_to_json_file(error_ent_json_file_path, ent_not_in_sent_dict)\n",
        "# print(ent_not_in_sent_dict[\"23352526\"][\"2335252600020\"][\"word_related_to_ent_in_sent\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_ent_dict = open_json_file_as_dict(error_ent_json_file_path)\n",
        "# pprint(error_ent_dict)\n",
        "\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "# onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "\n",
        "# print(onlyfiles)\n",
        "                \n",
        "# for idx, special_split_sent_not_final_IN_path in enumerate(onlyfiles):\n",
        "    # write_append_data_to_txt_file(\n",
        "    #     special_split_sent_not_final_IN_path_folder+'/'+ special_split_sent_not_final_IN_path,\n",
        "    #     \"<EOF>\"\n",
        "    # )\n",
        "\n",
        "for doc_code, doc_data in error_ent_dict.items():\n",
        "    doc_file_path = special_split_sent_not_final_IN_path_folder+'/'+ doc_code\n",
        "    # print(doc_code, isfile(doc_file_path))\n",
        "\n",
        "    if doc_code in ['23352552']:\n",
        "        print(\"ent_code \", ent_code)\n",
        "        # skip quang ninh\n",
        "        continue\n",
        "\n",
        "\n",
        "    for ent_code, child_data in doc_data.items():\n",
        "        \n",
        "        # if doc_code != \"23352018\":\n",
        "        #     continue\n",
        "\n",
        "        # ent_code = \"2335201800082\"\n",
        "        # doc_code = \"23352018\"\n",
        "        # doc_data = error_ent_dict[doc_code]\n",
        "        # pprint(doc_data)\n",
        "\n",
        "        doc_file_path = special_split_sent_not_final_IN_path_folder+'/'+ doc_code\n",
        "        all_new_line = []\n",
        "        with open(doc_file_path,'r') as _org_docs_file:\n",
        "            n = 0\n",
        "            code_ls = set()\n",
        "            for line in _org_docs_file:\n",
        "                # if len(line) < 8:\n",
        "                #     continue\n",
        "                line = line[:-1]\n",
        "                line = replace_space(line)\n",
        "                # file_name = for_processed_OUT_PATH+\"/\"+code\n",
        "                if line[8:11] == \"|t|\": # start=filename\n",
        "                    # print(line)\n",
        "                    pass\n",
        "                elif line[8:11] == \"|a|\": # start=filename\n",
        "                    for replace_meta_data in doc_data[ent_code][\"word_related_to_ent_in_sent\"]:\n",
        "                        if len(replace_meta_data) > 1 and replace_meta_data[1] != \"\":\n",
        "                            # print(line)\n",
        "                            line = line.replace(replace_meta_data[0], replace_meta_data[1])\n",
        "                            print(replace_meta_data[0])\n",
        "\n",
        "                else:\n",
        "                    line_ls_tab_split = line.split('\\t')                \n",
        "                    if len(line_ls_tab_split) > 4 and  line_ls_tab_split[4] in [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "                        if line_ls_tab_split[5] == ent_code:\n",
        "                            # print(\"- \"*50)\n",
        "                            # print(line)\n",
        "                            ent_meta_data = doc_data[ent_code][\"ent\"]\n",
        "                            if ent_meta_data[\"word_place\"] != \"\":\n",
        "                                line = line.replace(ent_meta_data[\"word\"], ent_meta_data[\"word_place\"])\n",
        "                                print(ent_meta_data[\"word\"])\n",
        "\n",
        "                all_new_line.append(line)\n",
        "\n",
        "\n",
        "        # pprint(all_new_line)\n",
        "        clear_file([doc_file_path])\n",
        "        for l in all_new_line:\n",
        "            write_append_data_to_txt_file(doc_file_path, l)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = \" ƒê \"\n",
        "for c in txt: \n",
        "    print(c, type(c), ord(c), hex(ord(c)), c.encode('utf-8'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### START TO CREATE PROCESSED FILE\n",
        "# sample:\n",
        "625456\tObsolete ... preparations .|One case ... years .|\n",
        "\n",
        "1:CID:2\tR2L\tNON-CROSS\t27-31\t10-11\t\n",
        "    D002119\tcalcium carbon - ate\tChemical\t27\t31\t1\t\n",
        "    D006934\thypercalcaemia\tDisease\t10\t11\t1\t\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
