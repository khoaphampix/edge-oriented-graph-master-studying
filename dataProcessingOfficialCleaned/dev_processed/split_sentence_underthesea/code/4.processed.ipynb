{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')\n",
        "\n",
        "def update_json_to_json_file(d_dict, j_path):\n",
        "    json_object = json.load( open(j_path) )\n",
        "    # print(json_object)\n",
        "    d_all = {}\n",
        "    d_all.update(json_object)\n",
        "    d_all.update(d_dict)\n",
        "    # print(d_all)\n",
        "    with open(j_path, 'w') as f:\n",
        "        json.dump(d_all, f)\n",
        "\n",
        "\n",
        "import re\n",
        "def find_word_pos_in_set(s, w_find):\n",
        "    try:\n",
        "        return [w.start() for w in re.finditer(w_find, s)]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "def find_related_word_in_sent(sent, txt):\n",
        "    \"\"\"\n",
        "        find word related to the ent in sentence\n",
        "        IN: ent, sent\n",
        "        OUT list [related, replace_related_expected_word]\n",
        "    \"\"\"\n",
        "    sent_split = sent.replace(\"_\",\" \")\n",
        "    txt_split = txt.replace(\"_\",\" \")\n",
        "    txt_len = len(txt)\n",
        "    start_pos = find_word_pos_in_set(sent_split, txt_split[0])\n",
        "    list_word_in_sent = []\n",
        "\n",
        "    for p in start_pos:\n",
        "        try:\n",
        "        # if 1:\n",
        "            if sent_split[p:p+txt_len] == txt_split:\n",
        "                related_word = sent[p-1:p+txt_len+1]\n",
        "                related_word_replace = related_word.replace(\"_\",\" \").replace(txt_split, txt_split.replace(\" \",\"_\"))\n",
        "                list_word_in_sent.append([related_word,related_word_replace])\n",
        "        except:\n",
        "            pass\n",
        "    return list_word_in_sent\n",
        "\n",
        "# sent = \"Oceanbank_Phòng giao dịch Đông_Đô_ ), Nguyễn_Thị_Loan Phòng guyễn_Thị_Loan \"\n",
        "# txt = \"Phòng giao_dịch Đông_Đô\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from os import listdir, remove\n",
        "# from os.path import isfile, join, isdir\n",
        "# from pprint import pprint\n",
        "# import re\n",
        "\n",
        "# special_split_sent_not_final_IN_path = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/CDR_DevelopmentSet.PubTator.txt\"\n",
        "# # special_split_sent_not_final_IN_path = \\\n",
        "# # \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/train_processed/CDR_TrainingSet.PubTator.txt\"\n",
        "# for_processed_OUT_PATH = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "# abnormal_chars_file_path = \\\n",
        "#     \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/abnormal_chars_file\"\n",
        "# clear_file([abnormal_chars_file_path])\n",
        "# s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
        "# s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
        "# def remove_accents(input_str):\n",
        "# \ts = ''\n",
        "# \tfor c in input_str:\n",
        "# \t\tif c in s1:\n",
        "# \t\t\ts += s0[s1.index(c)]\n",
        "# \t\telse:\n",
        "# \t\t\ts += c\n",
        "# \treturn s\n",
        "\n",
        "# with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "#     n = 0\n",
        "#     code_ls = set()\n",
        "#     for line in _org_docs_file:\n",
        "#         # if n > 23: break\n",
        "#         # n+=1\n",
        "#         if len(line) < 8:\n",
        "#             continue\n",
        "#         code = line[:8] # \n",
        "#         file_name = for_processed_OUT_PATH+\"/\"+code\n",
        "#         if line[8:11] == \"|t|\": # start=filename\n",
        "#             clear_file([file_name])\n",
        "#         write_append_data_to_txt_file(file_name, line[:-1])\n",
        "#         code_ls.add(code)\n",
        "#         \"\"\" Use for fine abnormal chars\"\"\"\n",
        "#         # line_ls_tab_split_org = line.split('\\t')\n",
        "#         # print(\"line_ls_tab_split >>> \", line_ls_tab_split)\n",
        "#         # if len(line_ls_tab_split_org) > 4 and  line_ls_tab_split_org[4] in \\\n",
        "#         #         [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "#         #     line_ls_tab_split = line_ls_tab_split_org[3].replace(\"_\",\" \")\n",
        "#         #     line_ls_tab_split = line_ls_tab_split.replace(\" \",\"\")\n",
        "#         #     line_ls_tab_split = remove_accents(line_ls_tab_split)\n",
        "#         #     if not re.match(\"^[A-Za-z0-9_-]*$\", line_ls_tab_split):\n",
        "#         #         write_append_data_to_txt_file(\n",
        "#         #             abnormal_chars_file_path,\n",
        "#         #             line_ls_tab_split_org[3]\n",
        "#         #         )\n",
        "\n",
        "# special_split_sent_not_final_IN_path_folder = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "# onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "\n",
        "# print(onlyfiles)\n",
        "                \n",
        "# for idx, special_split_sent_not_final_IN_path in enumerate(onlyfiles):\n",
        "#     write_append_data_to_txt_file(\n",
        "#         special_split_sent_not_final_IN_path_folder+'/'+ special_split_sent_not_final_IN_path,\n",
        "#         \"<EOF>\"\n",
        "#     )\n",
        "            "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### START TO CREATE PROCESSED FILE\n",
        "# sample:\n",
        "625456\tObsolete ... preparations .|One case ... years .|\n",
        "\n",
        "1:CID:2\tR2L\tNON-CROSS\t27-31\t10-11\t\n",
        "    D002119\tcalcium carbon - ate\tChemical\t27\t31\t1\t\n",
        "    D006934\thypercalcaemia\tDisease\t10\t11\t1\t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import itertools\n",
        "OUT_error_not_found_ent_in_msg_FOLDER_path = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/error/error_not_found_ent_in_msg_to_half_manual_edit_words_in_doc\"\n",
        "clear_file([f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/txt.txt\"])\n",
        "\n",
        "\n",
        "def get_NR_pair(data_dict_ents, data_dict_rels):\n",
        "    data_rels_pair_set = [set(i) for k, v in data_dict_rels.items() for i in v ]\n",
        "    classify_pairs_dict = {\n",
        "            \"PERSON-LOCATION\" : (\"PERSON\", \"LOCATION\"),\n",
        "            \"ORGANIZATION-LOCATION\" : (\"ORGANIZATION\", \"LOCATION\"),\n",
        "            \"LOCATION-LOCATION\" : (\"LOCATION\", \"LOCATION\"),\n",
        "            \"ORGANIZATION-ORGANIZATION\" : (\"ORGANIZATION\", \"ORGANIZATION\"),\n",
        "            \"PERSON-PERSON\" : (\"PERSON\", \"PERSON\"),\n",
        "            \"PERSON-ORGANIZATION\" : (\"PERSON\", \"ORGANIZATION\"),\n",
        "            \"LOCATION-ORGANIZATION\" : (\"LOCATION\", \"ORGANIZATION\")\n",
        "    }\n",
        "\n",
        "    ent_type_with_data_dict = \\\n",
        "                        {   \"ORGANIZATION\" : [],\n",
        "                            \"PERSON\" : [],\n",
        "                            \"LOCATION\" : []\n",
        "                        }\n",
        "\n",
        "    for ent_id, ent_meta_data in data_dict_ents.items():\n",
        "        data = ent_meta_data[\"data\"]\n",
        "        ent_type = data[0][3]\n",
        "        ent_type_with_data_dict[ent_type].append(ent_id)\n",
        "\n",
        "\n",
        "    # find the pair with rule \"pair for classified\"\n",
        "    total_pairs = []\n",
        "    for classify_pairs_name, classify_pairs in classify_pairs_dict.items():\n",
        "        ent_1 = ent_type_with_data_dict[classify_pairs[0]]\n",
        "        ent_2 = ent_type_with_data_dict[classify_pairs[1]]\n",
        "        # all_pair = set(itertools.product(*[ent_1 , ent_2]))\n",
        "        all_pair = [(i,j) for i in ent_1 for j in ent_2]\n",
        "\n",
        "        new_pair = [set(pair) for pair in all_pair  \n",
        "                        if((set(pair) not in data_rels_pair_set) and len(set(pair))==2 )\n",
        "                    ]\n",
        "        total_pairs+=new_pair\n",
        "        # print(\"- \"*50)\n",
        "        # print(classify_pairs_name, len(all_pair), len(new_pair))\n",
        "        # print(all_pair)\n",
        "        # print(new_pair)\n",
        "        # break\n",
        "\n",
        "    total_NR_pairs = []\n",
        "    for i in total_pairs:\n",
        "        il = list(i)\n",
        "        ent_1_in_sent = [ i[2] for i in data_dict_ents[il[0]][\"data\"]]\n",
        "        ent_2_in_sent = [ i[2] for i in data_dict_ents[il[1]][\"data\"]]\n",
        "        cross_sent = set(ent_1_in_sent).intersection(ent_2_in_sent)\n",
        "        if cross_sent:\n",
        "            # print(\"=\"*50)\n",
        "            # print(il[0], ent_1_in_sent)\n",
        "            # print(il[1], ent_2_in_sent)\n",
        "            # print(cross_sent)\n",
        "            # get list entity cross sent\n",
        "            list_ent_1_in_cross_sent = [ i+[il[0]] for i in data_dict_ents[il[0]][\"data\"] \n",
        "                                            if i[2] in cross_sent\n",
        "                                        ]\n",
        "            \n",
        "            list_ent_2_in_cross_sent = [ i+[il[1]] for i in data_dict_ents[il[1]][\"data\"] \n",
        "                                            if i[2] in cross_sent\n",
        "                                        ]\n",
        "            _pair = [(i,j) for i in list_ent_1_in_cross_sent for j in list_ent_2_in_cross_sent]        \n",
        "            total_NR_pairs.append(_pair)\n",
        "            # pprint(all_pair)\n",
        "    return total_NR_pairs\n",
        "\n",
        "def process_NR_pair_to_text(nr_pairs_ls, processes_ent_line_dict):\n",
        "    \"\"\" create \"processed-txt\" for NR pair\n",
        "    IN:  \n",
        "        nr_pairs_ls:\n",
        "                    [\n",
        "                        (   [94, 95, 3, 'LOCATION', '2335248900117'],\n",
        "                            [102, 103, 3, 'PERSON', '2335248900126']\n",
        "                        ),\n",
        "                        (   [112, 113, 3, 'LOCATION', '2335248900117'],\n",
        "                            [102, 103, 3, 'PERSON', '2335248900126']\n",
        "                        )\n",
        "                    ]\n",
        "        processes_ent_line_dict:  \n",
        "                    '2335248900217': '2335248900217\\tAsensio\\tPERSON\\t177\\t178\\t6' ...\n",
        "    OUT:\n",
        "                    1:NR:2\tL2R\tNON-CROSS\t161-162\t177-178\t\n",
        "                        2335248900156\tZidane|Zidane|Zidane|Zidane\tPERSON\t126:161:168:214\t127:162:169:215\t4:6:6:8\t\n",
        "                        2335248900217\tAsensio\tPERSON\t177\t178\t6\n",
        "    \"\"\"\n",
        "\n",
        "    list_NR_processed_completed_text_ls=[]\n",
        "    for nr_pairs in nr_pairs_ls:\n",
        "        for pair in nr_pairs:\n",
        "            r2l_or_l2r = \"L2R\"\n",
        "            rel = f\"1:NR:2\\t{r2l_or_l2r}\\tNON-CROSS\\t{pair[0][0]}-{pair[0][1]}\\t{pair[1][0]}-{pair[1][1]}\"\n",
        "            ent_1 = processes_ent_line_dict[pair[0][4]]\n",
        "            ent_2 = processes_ent_line_dict[pair[1][4]]\n",
        "            full_txt = f\"{rel}\\t{ent_1}\\t{ent_2}\"\n",
        "            list_NR_processed_completed_text_ls.append(full_txt)\n",
        "    return list_NR_processed_completed_text_ls\n",
        "\n",
        "# nr_pairs_ls = get_NR_pair(data_dict_ents, data_dict_rels)\n",
        "# list_NR_processed_completed_text_ls = process_NR_pair_to_text(nr_pairs_ls, processes_ent_line_dict)\n",
        "# print(list_NR_processed_completed_text_ls[0])\n",
        "\n",
        "\n",
        "def create_range_sentence_list(sents_ls):\n",
        "                \n",
        "    \"\"\" \n",
        "        create list sentence range to use for get pair has relation\n",
        "        IN sentent list [\"<sent-1>\", \"<sent-2>\"...]\n",
        "        OUT range of sent by word [(0,10)(11,25),(26,70)...]\n",
        "    \"\"\"\n",
        "    \n",
        "    range_sent = [[0, len(sents_ls[0].split(\" \"))]]\n",
        "    i = 0\n",
        "    prev_len = 0\n",
        "    current_len = 0\n",
        "    for s in sents_ls:\n",
        "        # ignore 1st\n",
        "        if i == 0:\n",
        "            i+=1\n",
        "            continue\n",
        "        current_len = range_sent[i-1][1]\n",
        "        prev_len = current_len + len(s.split(\" \"))\n",
        "        range_sent.append([current_len, prev_len])\n",
        "        i+=1   \n",
        "    return range_sent\n",
        "\n",
        "\n",
        "                \n",
        "def proccess_full_prepare_line_to_processed_text(full_data_to_processed):\n",
        "\n",
        "    exit_with_err =  full_data_to_processed[\"exit_with_err\"]\n",
        "    total_ent_a_doc =  full_data_to_processed[\"total_ent_a_doc\"]\n",
        "    current_code =  full_data_to_processed[\"current_code\"]\n",
        "    prev_code =  full_data_to_processed[\"prev_code\"]\n",
        "    passage =  full_data_to_processed[\"passage\"]\n",
        "    rel_dict =  full_data_to_processed[\"rel_dict\"]\n",
        "    ent_dict =  full_data_to_processed[\"ent_dict\"]\n",
        "    passage_sents_ls =  full_data_to_processed[\"passage_sents_ls\"]\n",
        "    range_sent =  full_data_to_processed[\"range_sent\"]\n",
        "    ent_dict_count_by_label = full_data_to_processed[\"ent_dict_count_by_label\"]\n",
        "\n",
        "\n",
        "    real_count_NOT_equal_len_ent = False\n",
        "    \n",
        "    total_ent_dict_count = 0\n",
        "    \n",
        "    list_ent_not_in_passage = f\"{current_code}\\t\\t: nof found ent in passage\"\n",
        "    # pprint(ent_dict)\n",
        "\n",
        "    for k,v in ent_dict.items():\n",
        "        total_ent_dict_count+=v['real_count']\n",
        "        # if v[\"real_count\"] != len(v[\"data\"]):\n",
        "        #     # print(v[\"real_count\"])\n",
        "        #     # print(len(v[\"data\"]))\n",
        "        #     real_count_NOT_equal_len_ent = True\n",
        "        #     break\n",
        "\n",
        "        # not create dict of error ent (not seen in sent) anymore \n",
        "        err_ent_dict = {}\n",
        "        # err_ent_dict[current_code] = {}\n",
        "        if v.get(\"data\", None) == None:\n",
        "            # print(\"* \"*10)\n",
        "            # print(\"* \"*10)\n",
        "            # print(\"* \"*10)\n",
        "            # print(k,v)\n",
        "            ent_no_sent = ent_dict_count_by_label[k][\"word\"]\n",
        "            ent_no_sent_new = ent_no_sent.replace(\" \",\"_\")\n",
        "            list_ent_not_in_passage+=f\"\\t\\t{k}\\t\\t{ent_no_sent}|$|{ent_no_sent_new}\"\n",
        "            # err_ent_dict[current_code][k] = {}\n",
        "            # err_ent_dict[current_code][k][\"word\"] = [ent_no_sent]\n",
        "            # err_ent_dict[current_code][k][\"related_word_in_sent\"] = []\n",
        "\n",
        "            # org find related word to ent in sent\n",
        "            # for w in passage_sents_ls:\n",
        "            #     ent_no_sent_spl = ent_no_sent.replace(\"_\", \" \").replace(\".\", \" \")\n",
        "            #     w_spl = w.replace(\"_\", \" \").replace(\".\", \" \")\n",
        "            #     if ent_no_sent_spl in w_spl:\n",
        "            #         w_n = w.replace(\"_\",\" \")                    \n",
        "            #         w_n = w_n.replace(ent_no_sent.replace(\"_\",\" \"), ent_no_sent_new)\n",
        "            #         print(\"w, w_n >>> \",w, w_n, ent_no_sent, len(ent_no_sent))\n",
        "            #         list_ent_not_in_passage+=f\"\\t\\t{w}|$|{w_n}\"\n",
        "            \n",
        "            ls_related = find_related_word_in_sent(passage, ent_no_sent)\n",
        "            # print(\"ls_related \", ls_related)\n",
        "\n",
        "            for rl in ls_related:\n",
        "                w, w_n = rl\n",
        "                list_ent_not_in_passage+=f\"\\t\\t{w}|$|{w_n}\"\n",
        "\n",
        "                    # err_ent_dict[current_code][k][\"related_word_in_sent\"].append([w])\n",
        "        # print(\"* \"*50)\n",
        "        # print(v.get(\"data\", None))\n",
        "        # print(v.get(\"data\", None))\n",
        "        # print(\"* \"*50)\n",
        "        # print(\"* \"*50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if list_ent_not_in_passage != f\"{current_code}\\t\\t: nof found ent in passage\":\n",
        "            # print(\"* \"*50)\n",
        "            # print(\"* \"*50)\n",
        "            # print(err_ent_dict)\n",
        "            write_append_data_to_txt_file(f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/txt.txt\",list_ent_not_in_passage)\n",
        "            # update_json_to_json_file(err_ent_dict, f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/json.json\")\n",
        "\n",
        "            return list_ent_not_in_passage\n",
        "        \"\"\"\n",
        "            this will handling the enity \"Not found in the passage\" ( defualt value = -100)\n",
        "        \"\"\"\n",
        "        # for w in v[\"data\"]:\n",
        "        #     if w[-1] == -100:\n",
        "        #         exit_with_err = \"err no sent id\"\n",
        "        #         return exit_with_err\n",
        "\n",
        "        if v[\"real_count\"] != len(v[\"data\"]):\n",
        "            # print(v[\"real_count\"])\n",
        "            # print(len(v[\"data\"]))\n",
        "            real_count_NOT_equal_len_ent = True\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    if exit_with_err != \"\":\n",
        "        print(exit_with_err)\n",
        "        print(\"^ \"*50)\n",
        "        return exit_with_err\n",
        "\n",
        "\n",
        "    if real_count_NOT_equal_len_ent:\n",
        "        exit_with_err = \"REAL COUNT ERROR \"*10\n",
        "        return exit_with_err\n",
        "\n",
        "        \n",
        "    \"\"\"\n",
        "        Agree that:\n",
        "            - residual entity\n",
        "            - if miss entity --> these entity(code) not belong to relation \n",
        "    \"\"\"\n",
        "    # print(\"???? \"*40)\n",
        "    # print(total_ent_a_doc, total_ent_dict_count)\n",
        "    if(total_ent_a_doc != total_ent_dict_count):\n",
        "\n",
        "        # print(\"* *50\")\n",
        "        # pprint(rel_dict)\n",
        "        # pprint(ent_dict)\n",
        "        # pprint(ent_dict_count_by_label)\n",
        "        # print(\"* *50\")\n",
        "\n",
        "\n",
        "        set_ent_has_rel = set(i for k,v in rel_dict.items() for item in v for i in item)\n",
        "        ent_in_rel_break_rule = False\n",
        "        for k, v in ent_dict.items():\n",
        "            # for i in v[\"words\"]:\n",
        "            #     print(i)\n",
        "\n",
        "            # print(\"detail \")\n",
        "            # print(\"- \"*40)\n",
        "            if ent_dict_count_by_label[k][\"label_ent_count\"] != v[\"real_count\"]:\n",
        "                if k in set_ent_has_rel:\n",
        "                    if v[\"real_count\"] < ent_dict_count_by_label[k][\"label_ent_count\"]:\n",
        "                        # ent_in_rel_break_rule = True\n",
        "                        w =v[\"words\"][0]\n",
        " \n",
        "                        ln = ent_dict_count_by_label[k]\n",
        "                        rn = v[\"real_count\"]\n",
        "                        print(f\"{w}: label{ln}, but real is {rn}\")\n",
        "                        # print(w)\n",
        "\n",
        "        # sum_label_ent = sum(ent_dict_count_by_label.values())\n",
        "        # print(f\"[sum_label_ent: {sum_label_ent}] total_ent_a_doc: {total_ent_a_doc} != total_ent_dict_count(real_in_doc): {total_ent_dict_count}\")\n",
        "        if ent_in_rel_break_rule:\n",
        "            exit_with_err = \"TOTAL ENT ERROR \"*10\n",
        "            return exit_with_err\n",
        "    \n",
        "    \"\"\"\n",
        "        handling mapping relation ent and w2f\n",
        "    \"\"\"\n",
        "    processes_ent_line_dict = {}\n",
        "    for ent_code, ent_ls in ent_dict.items():             \n",
        "        start_idx = \"\"\n",
        "        end_idx = \"\"\n",
        "        sent_idx = \"\"\n",
        "        ent_words_merge = \"\"                \n",
        "        \n",
        "        # ignore relation cannot find on passage because of (_) and split(\" \")\n",
        "        err_not_found_ent_in_passage = False\n",
        "        if ent_ls.get(\"data\", None):\n",
        "            for k0, v0 in rel_dict.items():\n",
        "                if ent_code in v0:\n",
        "                    err_not_found_ent_in_passage = True\n",
        "                    exit_with_err = \"cannot find on passage\"\n",
        "                    break\n",
        "        else:\n",
        "            if err_not_found_ent_in_passage == False:\n",
        "                continue\n",
        "        \n",
        "        if exit_with_err != \"\":\n",
        "            return exit_with_err\n",
        "            \n",
        "        for ent in ent_ls[\"data\"]:\n",
        "            start_idx += \":\"+str(ent[0])\n",
        "            end_idx += \":\"+str(ent[1])\n",
        "            sent_idx += \":\"+str(ent[2])\n",
        "            ent_words_merge +=(\"|\"+ \" \".join(passage_sents_ls[ent[0]:ent[1]]))\n",
        "        ent_type = ent_ls[\"data\"][0][3]\n",
        "        processes_ent_line_dict[ent_code] = f\"{ent_code}\\t{ent_words_merge[1:]}\\t{ent_type}\\t{start_idx[1:]}\\t{end_idx[1:]}\\t{sent_idx[1:]}\"\n",
        "                \n",
        "    \"\"\"\n",
        "        process sentence by sentene\n",
        "\n",
        "    Returns:\n",
        "        save data of sent: doc, passage_split_list, ent, relation\n",
        "    \"\"\"\n",
        "    pair = \"\"\n",
        "    processes_rel_pair_line_list = []\n",
        "    \n",
        "\n",
        "    # print(\"* *50\")\n",
        "    # print(\"* *50\")\n",
        "    # print(\"* *50\")\n",
        "    # print(\"* *50\")\n",
        "    # pprint(rel_dict)\n",
        "\n",
        "    for rel_name, ent_pair_ls in rel_dict.items():\n",
        "        for ent_pair in ent_pair_ls:\n",
        "            ent_first_ls = ent_dict[ent_pair[0]][\"data\"]\n",
        "            ent_sec_ls = ent_dict[ent_pair[1]][\"data\"]\n",
        "            pair = \"1:<error relation pair line>:2\"\n",
        "            for ent in ent_first_ls:\n",
        "                ent_sec_part = list(filter(lambda ent_sec: ent_sec[2] == ent[2], ent_sec_ls))\n",
        "                if ent_sec_part:\n",
        "                    ent_sec_part = ent_sec_part[0]\n",
        "                    r2l_or_l2r = \"L2R\"\n",
        "                    pair = f\"1:{rel_name}:2\\t{r2l_or_l2r}\\tNON-CROSS\\t{ent[0]}-{ent[1]}\\t{ent_sec_part[0]}-{ent_sec_part[1]}\"\n",
        "                    break \n",
        "                \n",
        "            if pair == \"1:<error relation pair line>:2\":\n",
        "                exit_with_err = \"entity not in the passage\"                          \n",
        "                return exit_with_err\n",
        "            processed_rel_pair_line = \\\n",
        "                f\"{pair}\\t{(processes_ent_line_dict[ent_pair[0]])}\\t{(processes_ent_line_dict[ent_pair[1]])}\"     \n",
        "            processes_rel_pair_line_list.append(processed_rel_pair_line)\n",
        "    return processes_rel_pair_line_list, processes_ent_line_dict\n",
        "\n",
        "\n",
        "\n",
        "def process_entity(full_data_to_processed):\n",
        "    exit_with_err =  full_data_to_processed[\"exit_with_err\"]\n",
        "    total_ent_a_doc =  full_data_to_processed[\"total_ent_a_doc\"]\n",
        "    current_code =  full_data_to_processed[\"current_code\"]\n",
        "    prev_code =  full_data_to_processed[\"prev_code\"]\n",
        "    passage =  full_data_to_processed[\"passage\"]\n",
        "    rel_dict =  full_data_to_processed[\"rel_dict\"]\n",
        "    ent_dict =  full_data_to_processed[\"ent_dict\"]\n",
        "    passage_sents_ls =  full_data_to_processed[\"passage_sents_ls\"]\n",
        "    range_sent =  full_data_to_processed[\"range_sent\"]\n",
        "    line_ls_tab_split =  full_data_to_processed[\"line_ls_tab_split\"]\n",
        "    # ent_dict_count_by_label = full_data_to_processed[\"ent_dict_count_by_label\"]\n",
        "\n",
        "\n",
        "\n",
        "    total_ent_a_doc+=1\n",
        "    ent_code = line_ls_tab_split[5][:-1]\n",
        "    ent_word = line_ls_tab_split[3]\n",
        "    # pprint(ent_dict)\n",
        "    # print(\"ent_word \", ent_word)\n",
        "    # if ent_dict_count_by_label.get(ent_code, None) == None:\n",
        "    #     ent_dict_count_by_label[ent_code] = 0\n",
        "    # else:\n",
        "    #     ent_dict_count_by_label[ent_code] +=1\n",
        "\n",
        "    if ent_dict.get(ent_code, None) == None: \n",
        "        ent_dict[ent_code] = {}\n",
        "        ent_dict[ent_code][\"real_count\"] = 0\n",
        "\n",
        "        # if ent_word == \"Tây Ban_Nha\":\n",
        "        #     print(\"1 \"*100)\n",
        "        #     print(ent_dict[ent_code])\n",
        "            \n",
        "    else:\n",
        "        # inore checked words, \"Real_Madrid\" but not \"Real\"\n",
        "        checked_workd_Real_Madrid = False\n",
        "\n",
        "        # handle entity error append to list\n",
        "        if ent_dict[ent_code].get(\"data\", None) == None:\n",
        "            # print(\"* \"*10)\n",
        "            # print(\"? \"*10)\n",
        "            # print(\"* \"*10)\n",
        "            list_ent_not_in_passage = f\"{current_code}\\t\\t: nof found ent in passage\"             \n",
        "            ent_no_sent = ent_word\n",
        "            ent_no_sent_new = ent_no_sent.replace(\" \",\"_\")\n",
        "            list_ent_not_in_passage+=f\"\\t\\t{ent_code}\\t\\t{ent_no_sent}|$|{ent_no_sent_new}\"\n",
        "        \n",
        "            ls_related = find_related_word_in_sent(passage, ent_no_sent)\n",
        "            # print(\"ls_related \", ls_related)\n",
        "            for rl in ls_related:\n",
        "                w, w_n = rl\n",
        "                list_ent_not_in_passage+=f\"\\t\\t{w}|$|{w_n}\"\n",
        "            write_append_data_to_txt_file(f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/txt.txt\",list_ent_not_in_passage)\n",
        "            # update_json_to_json_file(err_ent_dict, f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/json.json\")\n",
        "\n",
        "            print(\"list_ent_not_in_passage \", list_ent_not_in_passage)\n",
        "            return list_ent_not_in_passage\n",
        "\n",
        "\n",
        "        for we in ent_dict[ent_code][\"data\"]:\n",
        "            \n",
        "            if passage_sents_ls[we[0]:we[1]] == ent_word.split(\" \"):\n",
        "                # \"Real_Madrid\" but may not \"Real\"\n",
        "                # print(\"Real Madrid but may not Real\")\n",
        "                # print(passage_sents_ls[we[0]:we[1]],ent_word)\n",
        "                checked_workd_Real_Madrid = True\n",
        "                break\n",
        "        if checked_workd_Real_Madrid:\n",
        "            # continue\n",
        "            \n",
        "            \n",
        "            full_data_to_processed = \\\n",
        "                {\n",
        "                    \"exit_with_err\": exit_with_err,\n",
        "                    \"total_ent_a_doc\": total_ent_a_doc,\n",
        "                    \"current_code\": current_code,\n",
        "                    \"prev_code\": prev_code,\n",
        "                    \"passage\": passage,\n",
        "                    \"rel_dict\": rel_dict,\n",
        "                    \"ent_dict\": ent_dict,\n",
        "                    \"passage_sents_ls\": passage_sents_ls,\n",
        "                    \"range_sent\": range_sent,\n",
        "                    # \"ent_dict_count_by_label\":ent_dict_count_by_label\n",
        "            }\n",
        "                        \n",
        "            \n",
        "            return full_data_to_processed\n",
        "        # print(\"RealMadrid but may not Real\")\n",
        "        # print(ent_dict[ent_code][\"data\"],ent_word)\n",
        "\n",
        "    \"\"\" \n",
        "        start to check entity & get index in sentence\n",
        "        \n",
        "    \"\"\"                        \n",
        "    has_ent_in_sent = ent_word.split(\" \")\n",
        "    \n",
        "    # print(\">> has_ent_in_sent \", has_ent_in_sent)\n",
        "    \n",
        "    # return if not found here --> clean data again\n",
        "    not_found_ent_in_sent = True\n",
        "    for idx_w, word in enumerate(passage_sents_ls):\n",
        "        if (has_ent_in_sent[0] == word):\n",
        "            \n",
        "            # print(\"iter word in sent ...\")\n",
        "            # print(has_ent_in_sent[0], word)\n",
        "            end_expect_idx = idx_w+len(has_ent_in_sent)\n",
        "            if has_ent_in_sent ==passage_sents_ls[idx_w:end_expect_idx]:\n",
        "                sent_idx = -100\n",
        "                # if ent_word == \"Real_Sociedad\":\n",
        "                #     print(idx_w,end_expect_idx)\n",
        "                \n",
        "                for id_range, _range in enumerate(range_sent):  \n",
        "                    # print(range(_range[0], _range[1]))                                  \n",
        "                    if idx_w in range(_range[0], _range[1]) and \\\n",
        "                                end_expect_idx in range(_range[0], _range[1]):\n",
        "                        sent_idx = id_range\n",
        "                        break \n",
        "                                      \n",
        "                ent_data = [idx_w, end_expect_idx, sent_idx, line_ls_tab_split[4]]\n",
        "                # print(\"ent_data \", passage_sents_ls[idx_w:end_expect_idx][0], ent_data)                               \n",
        "                if ent_dict[ent_code].get(\"data\", None) == None:                  \n",
        "                    ent_dict[ent_code][\"data\"] = [ent_data]\n",
        "                    ent_dict[ent_code][\"words\"] = []\n",
        "                    ent_dict[ent_code][\"words\"].append(ent_word)\n",
        "                else:\n",
        "                    ent_dict[ent_code][\"data\"].append(ent_data)  \n",
        "                    ent_dict[ent_code][\"words\"].append(ent_word)\n",
        "\n",
        "                ent_dict[ent_code][\"real_count\"] +=1\n",
        "                not_found_ent_in_sent = False\n",
        "\n",
        "    \"\"\"\n",
        "        Skip handling \"entity not in the passage\"\n",
        "            - To save the \"error\" ent to docs --> solve all in once\n",
        "            - will handle at last processed process\n",
        "    \n",
        "    \"\"\"\n",
        "    # if not_found_ent_in_sent:\n",
        "    #     exit_with_err = \"entity not in the passage\"    \n",
        "    #     print(\"ent err ...\")                                                                \n",
        "    #     return exit_with_err\n",
        "\n",
        "    full_data_to_processed = \\\n",
        "        {\n",
        "            \"exit_with_err\": exit_with_err,\n",
        "            \"total_ent_a_doc\": total_ent_a_doc,\n",
        "            \"current_code\": current_code,\n",
        "            \"prev_code\": prev_code,\n",
        "            \"passage\": passage,\n",
        "            \"rel_dict\": rel_dict,\n",
        "            \"ent_dict\": ent_dict,\n",
        "            \"passage_sents_ls\": passage_sents_ls,\n",
        "            \"range_sent\": range_sent,\n",
        "            # \"ent_dict_count_by_label\":ent_dict_count_by_label\n",
        "        }\n",
        "    return full_data_to_processed\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "processed_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/vlsp.data\"\n",
        "\n",
        "processed_error_files_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/processed_error_files.data\"\n",
        "\n",
        "clear_file([processed_OUT_PATH])\n",
        "clear_file([processed_error_files_OUT_PATH])\n",
        "\n",
        "classify_pairs = [  'PERSON-LOCATION',      'ORGANIZATION-LOCATION', \n",
        "                    'LOCATION-LOCATION',    'ORGANIZATION-ORGANIZATION', \n",
        "                    'PERSON-PERSON', \n",
        "                    'PERSON-ORGANIZATION',  'LOCATION-ORGANIZATION']\n",
        "\n",
        "\n",
        "\n",
        "def read_file(special_split_sent_not_final_IN_path, processed_OUT_PATH):\n",
        "    n = 0\n",
        "    with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "        exit_with_err = \"\"\n",
        "        total_ent_a_doc = 0\n",
        "        current_code = None\n",
        "        prev_code = None\n",
        "        passage = \"\"\n",
        "        rel_dict = {}\n",
        "        ent_dict = {}\n",
        "        ent_dict_count_by_label = {}\n",
        "        passage_sents_ls = []\n",
        "        range_sent = []\n",
        "        for org_docs_line in _org_docs_file:\n",
        "            if exit_with_err:\n",
        "                print(\"exit_with_err>> \", exit_with_err)\n",
        "                # break\n",
        "                return exit_with_err\n",
        "            # if n > 12: break\n",
        "            # n+=1\n",
        "            # print(f\"{n}.>>> {org_docs_line[:-1]}\")\n",
        "            real_count_NOT_equal_len_ent = False\n",
        "            # if lentorg_docs_line == \"<EOF>\":\n",
        "            if len(org_docs_line) < 10:\n",
        "                # print(\">> len(org_docs_line) == 1:\", current_code)\n",
        "                \"\"\"\n",
        "                    read all line \n",
        "                    now, handling to create processED data & reset object in here\n",
        "                \"\"\"\n",
        "                full_data_to_processed = \\\n",
        "                    {\n",
        "                        \"exit_with_err\": exit_with_err,\n",
        "                        \"total_ent_a_doc\": total_ent_a_doc,\n",
        "                        \"current_code\": current_code,\n",
        "                        \"prev_code\": prev_code,\n",
        "                        \"passage\": passage,\n",
        "                        \"rel_dict\": rel_dict,\n",
        "                        \"ent_dict\": ent_dict,\n",
        "                        \"passage_sents_ls\": passage_sents_ls,\n",
        "                        \"range_sent\": range_sent,\n",
        "                        \"ent_dict_count_by_label\": ent_dict_count_by_label\n",
        "\n",
        "                    }\n",
        "\n",
        "                return_processed_text = \\\n",
        "                    proccess_full_prepare_line_to_processed_text(full_data_to_processed)\n",
        "                if type(return_processed_text) == 'str' or len(return_processed_text) != 2:\n",
        "                    \"\"\" error \"\"\"\n",
        "                    print(\"er \"*40)\n",
        "                    print(\"return_processed_text \", return_processed_text)\n",
        "                    return return_processed_text\n",
        "                processes_rel_pair_line_list, processes_ent_line_dict = return_processed_text\n",
        "\n",
        "                nr_pairs_ls = get_NR_pair(ent_dict, rel_dict)\n",
        "                list_NR_processed_completed_text_ls = process_NR_pair_to_text(nr_pairs_ls, processes_ent_line_dict)\n",
        "                processes_rel_pair_line_list += list_NR_processed_completed_text_ls\n",
        "                processes_rel_pair_line_list_line = \"\\t\".join(processes_rel_pair_line_list)\n",
        "                full_processed_line = f\"{current_code}\\t{passage[:-1]}\\t{processes_rel_pair_line_list_line}\"\n",
        "                write_append_data_to_txt_file(processed_OUT_PATH, full_processed_line)\n",
        "                # print(\"end / \"*10)\n",
        "                return \"no-err\"\n",
        "\n",
        "            code = org_docs_line[0:8]  \n",
        "            current_code = code  \n",
        "            word_after_code =  org_docs_line[8:11]\n",
        "            org_docs_line = org_docs_line.replace(\"\\xc2\\xa0\", \" \")\n",
        "            org_docs_line = org_docs_line.replace(\"\\xa0\", \" \")\n",
        "            \n",
        "            if word_after_code == '|t|':\n",
        "                pass\n",
        "            elif word_after_code == '|a|':\n",
        "                passage = org_docs_line[11:]\n",
        "                sents_ls = passage.split(\".|\")\n",
        "                passage_sents_ls = (\" \".join(sents_ls)).split(\" \")\n",
        "                range_sent = create_range_sentence_list(sents_ls)\n",
        "            else:\n",
        "                line_ls_tab_split = org_docs_line.split('\\t')                \n",
        "                \"\"\"\n",
        "                    process to add entity to list entity\n",
        "                \"\"\"\n",
        "                if len(line_ls_tab_split) > 4 and  line_ls_tab_split[4] in \\\n",
        "                        [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "                    \n",
        "\n",
        "                    ent_code = line_ls_tab_split[5][:-1]\n",
        "                    if ent_dict_count_by_label.get(ent_code, None) == None:\n",
        "                        ent_dict_count_by_label[ent_code] = {}\n",
        "                        ent_dict_count_by_label[ent_code][\"label_ent_count\"] = 0                    \n",
        "                        ent_dict_count_by_label[ent_code][\"word\"] = line_ls_tab_split[3]\n",
        "                    else:\n",
        "                        ent_dict_count_by_label[ent_code][\"label_ent_count\"] +=1\n",
        "\n",
        "                    full_data_to_processed = \\\n",
        "                        {   \n",
        "                            \"exit_with_err\": exit_with_err,\n",
        "                            \"total_ent_a_doc\": total_ent_a_doc,\n",
        "                            \"current_code\": current_code,\n",
        "                            \"prev_code\": prev_code,\n",
        "                            \"passage\": passage,\n",
        "                            \"rel_dict\": rel_dict,\n",
        "                            \"ent_dict\": ent_dict,\n",
        "                            \"passage_sents_ls\": passage_sents_ls,\n",
        "                            \"range_sent\": range_sent,\n",
        "                            \"line_ls_tab_split\": line_ls_tab_split,\n",
        "                            # \"ent_dict_count_by_label\": ent_dict_count_by_label\n",
        "                        }\n",
        "                    full_data_to_processed = process_entity(full_data_to_processed)\n",
        "                    if full_data_to_processed == \"entity not in the passage\" or type(full_data_to_processed) == 'str':\n",
        "                        \"\"\" error \"\"\"\n",
        "                        print(\"[ent error]\",\"e \"*30)\n",
        "                        print(\"Error process entity \", full_data_to_processed)\n",
        "                        return full_data_to_processed\n",
        "                    # print(\"? \"*40)\n",
        "                    # print(\"? \"*40)\n",
        "                    # print(full_data_to_processed)\n",
        "                    exit_with_err =  full_data_to_processed[\"exit_with_err\"]\n",
        "                    total_ent_a_doc =  full_data_to_processed[\"total_ent_a_doc\"]\n",
        "                    current_code =  full_data_to_processed[\"current_code\"]\n",
        "                    prev_code =  full_data_to_processed[\"prev_code\"]\n",
        "                    passage =  full_data_to_processed[\"passage\"]\n",
        "                    rel_dict =  full_data_to_processed[\"rel_dict\"]\n",
        "                    ent_dict =  full_data_to_processed[\"ent_dict\"]\n",
        "                    passage_sents_ls =  full_data_to_processed[\"passage_sents_ls\"]\n",
        "                    range_sent =  full_data_to_processed[\"range_sent\"]\n",
        "                    # ent_dict_count_by_label = full_data_to_processed[\"ent_dict_count_by_label\"]\n",
        "                                                \n",
        "                else:\n",
        "                    if len(line_ls_tab_split) != 4:\n",
        "                        exit_with_err = \"entity line is invalid\"\n",
        "                        return exit_with_err\n",
        "\n",
        "                    if line_ls_tab_split[1] not in \\\n",
        "                        [\"AFFILIATION\", \"LOCATED\", \"PART – WHOLE\", \"PERSONAL - SOCIAL\"]:\n",
        "                        exit_with_err = \"entity type name is wrong\"\n",
        "                        return exit_with_err\n",
        "                    print(\"line_ls_tab_split \", line_ls_tab_split)\n",
        "\n",
        "                    if line_ls_tab_split[2] in ['None', \"None\\n\"] or line_ls_tab_split[3] in ['None', \"None\\n\"]:\n",
        "                        print(\"-\"*30, \" found NONE \",\"-\"*30)\n",
        "                        continue\n",
        "\n",
        "\n",
        "                    if rel_dict.get(line_ls_tab_split[1], None) == None:             \n",
        "                        rel_dict[line_ls_tab_split[1]] = [(line_ls_tab_split[2], line_ls_tab_split[3][:-1])]\n",
        "                    else:\n",
        "                        rel_dict[line_ls_tab_split[1]].append( (line_ls_tab_split[2], line_ls_tab_split[3][:-1]) )\n",
        "                \n",
        "    return \"no-err\"\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "onlyfiles = sorted(onlyfiles)\n",
        "print(onlyfiles)\n",
        "\n",
        "\n",
        "error_count = 0\n",
        "all_docs = 0\n",
        "error_doc_list_has_name = []\n",
        "error_doc_list_trigger_except = []\n",
        "# clear_file([f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/txt.txt\"])\n",
        "for idx, doc_code in enumerate(onlyfiles):\n",
        "    all_docs+=1\n",
        "\n",
        "\n",
        "    # if doc_code != '23354253':\n",
        "    #     continue\n",
        "\n",
        "\n",
        "    error_doc_list_trigger_except =  []\n",
        "    error_doc_list_has_name =  ['23351426', '23351515', '23351635', '23351700', '23351837', '23351960', '23351992', '23352683', '23352695', '23352857', '23353874', '23353950', '23354055', '23354285', '23354695', '23354796', '23354880', '23355228', '23356715', '23356907', '23357765']\n",
        "\n",
        "    # error_doc_list_has_name =  ['23352568']\n",
        "\n",
        "    # error_doc_list_has_name = ['23352568']\n",
        "    # if not(doc_code in error_doc_list_has_name):\n",
        "    #     continue\n",
        "\n",
        "\n",
        "    ## Exception files ( not yet processed)\n",
        "    if not(doc_code in error_doc_list_trigger_except):\n",
        "        print(f\"ignore: {doc_code}\")\n",
        "        continue\n",
        "\n",
        "\n",
        "    error_doc_list_trigger_except =  []\n",
        "    error_doc_list_has_name = []\n",
        "\n",
        "    special_split_sent_not_final_IN_path = special_split_sent_not_final_IN_path_folder+'/'+ doc_code\n",
        "    print(\"\\n\", idx, \"processing docs: \", doc_code)\n",
        "    # if idx > 1:\n",
        "    #     break\n",
        "    # try:\n",
        "    if 1:\n",
        "        val = read_file(special_split_sent_not_final_IN_path, processed_OUT_PATH)\n",
        "        if val == 'no-err':\n",
        "            print(doc_code)\n",
        "            error_count+=1\n",
        "        else:\n",
        "            print(\"error type \", val)\n",
        "            error_doc_list_has_name.append(doc_code)\n",
        "            write_append_data_to_txt_file(processed_error_files_OUT_PATH, doc_code)\n",
        "    # except:\n",
        "    #     write_append_data_to_txt_file(processed_error_files_OUT_PATH, doc_code)\n",
        "    #     print(\"error: \", doc_code)\n",
        "    #     error_doc_list_trigger_except.append(doc_code)\n",
        "\n",
        "print(\"error_doc_list_trigger_except = \", error_doc_list_trigger_except)\n",
        "print(\"error_doc_list_has_name = \", error_doc_list_has_name)\n",
        "print(\"LEN error_doc_list_trigger_except: \", len(error_doc_list_trigger_except), \"LEN error_doc_list_has_name: \", len(error_doc_list_has_name))\n",
        "\n",
        "print(f\"valid_files_count / all {error_count}/{all_docs} ~ {round(error_count*100/all_docs,2)}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
