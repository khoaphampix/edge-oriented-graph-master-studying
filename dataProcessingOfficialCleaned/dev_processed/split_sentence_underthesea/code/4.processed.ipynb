{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')\n",
        "\n",
        "def update_json_to_json_file(d_dict, j_path):\n",
        "    json_object = json.load( open(j_path) )\n",
        "    # print(json_object)\n",
        "    d_all = {}\n",
        "    d_all.update(json_object)\n",
        "    d_all.update(d_dict)\n",
        "    # print(d_all)\n",
        "    with open(j_path, 'w') as f:\n",
        "        json.dump(d_all, f)\n",
        "\n",
        "\n",
        "import re\n",
        "def find_word_pos_in_set(s, w_find):\n",
        "    try:\n",
        "        return [w.start() for w in re.finditer(w_find, s)]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "def find_related_word_in_sent(sent, txt):\n",
        "    \"\"\"\n",
        "        find word related to the ent in sentence\n",
        "        IN: ent, sent\n",
        "        OUT list [related, replace_related_expected_word]\n",
        "    \"\"\"\n",
        "    sent_split = sent.replace(\"_\",\" \")\n",
        "    txt_split = txt.replace(\"_\",\" \")\n",
        "    txt_len = len(txt)\n",
        "    start_pos = find_word_pos_in_set(sent_split, txt_split[0])\n",
        "    list_word_in_sent = []\n",
        "\n",
        "    for p in start_pos:\n",
        "        try:\n",
        "        # if 1:\n",
        "            if sent_split[p:p+txt_len] == txt_split:\n",
        "                related_word = sent[p-1:p+txt_len+1]\n",
        "                related_word_replace = related_word.replace(\"_\",\" \").replace(txt_split, txt_split.replace(\" \",\"_\"))\n",
        "                list_word_in_sent.append([related_word,related_word_replace])\n",
        "        except:\n",
        "            pass\n",
        "    return list_word_in_sent\n",
        "\n",
        "# sent = \"Oceanbank_Phòng giao dịch Đông_Đô_ ), Nguyễn_Thị_Loan Phòng guyễn_Thị_Loan \"\n",
        "# txt = \"Phòng giao_dịch Đông_Đô\"\n",
        "\n",
        "\n",
        "\n",
        "def replace2space(string):\n",
        "    spaces = [\"\\r\", '\\xa0', '\\xe2\\x80\\x85', '\\xc2\\xa0', '\\u2009', '\\u2002', '\\u200a', '\\u2005', '\\u2003', '\\u2006',\n",
        "              'Ⅲ', '…', 'Ⅴ', \"\\u202f\"]\n",
        "\n",
        "    for i in spaces:\n",
        "        string = string.replace(i, ' ')\n",
        "    return string\n",
        "\n",
        "\n",
        "def replace2symbol(string):\n",
        "    string =  string.replace('”', '\"')\\\n",
        "                    .replace('“', \"'\")\\\n",
        "                    .replace(' ', ' ')\\\n",
        "                    .replace('’', \"'\")\\\n",
        "                    .replace('–', '-')\\\n",
        "                    .replace('‘', \"'\")\\\n",
        "                    .replace('‑', '-')\\\n",
        "                    .replace('»', '\"')\\\n",
        "                    .replace('—', '-')\\\n",
        "                    .replace('¾', '')\\\n",
        "                    .replace('²', '')\\\n",
        "                    .replace('…', '')\\\n",
        "                    .replace('½', '')\\\n",
        "                    .replace('\\u200b', ' ')\\\n",
        "                    .replace(\"\\ufeff\", '')\n",
        "\n",
        "    return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_ignore_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    # merged_lines = ','.join([line.strip() for line in lines])\n",
        "    # return merged_lines\n",
        "    list_lines = [line.strip().replace(\"\\n\",\"\") for line in lines]\n",
        "    return list_lines\n",
        "\n",
        "\n",
        "big_file_path = \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/ignore_docs/ignore_big_docs\"\n",
        "ignore_docs_list = read_ignore_file(big_file_path)\n",
        "\n",
        "\n",
        "accept_doc_NOT_big_path = \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/ignore_docs/accept_doc_NOT_big\"\n",
        "accept_doc_NOT_big = read_ignore_file(accept_doc_NOT_big_path)\n",
        "# print(accept_doc_NOT_big)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from os import listdir, remove\n",
        "# from os.path import isfile, join, isdir\n",
        "# from pprint import pprint\n",
        "# import re\n",
        "\n",
        "# special_split_sent_not_final_IN_path = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/CDR_DevelopmentSet.PubTator.txt\"\n",
        "# # special_split_sent_not_final_IN_path = \\\n",
        "# # \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/train_processed/CDR_TrainingSet.PubTator.txt\"\n",
        "# for_processed_OUT_PATH = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "# abnormal_chars_file_path = \\\n",
        "#     \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/abnormal_chars_file\"\n",
        "# clear_file([abnormal_chars_file_path])\n",
        "# s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n",
        "# s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n",
        "# def remove_accents(input_str):\n",
        "# \ts = ''\n",
        "# \tfor c in input_str:\n",
        "# \t\tif c in s1:\n",
        "# \t\t\ts += s0[s1.index(c)]\n",
        "# \t\telse:\n",
        "# \t\t\ts += c\n",
        "# \treturn s\n",
        "\n",
        "# with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "#     n = 0\n",
        "#     code_ls = set()\n",
        "#     for line in _org_docs_file:\n",
        "#         # if n > 23: break\n",
        "#         # n+=1\n",
        "#         if len(line) < 8:\n",
        "#             continue\n",
        "#         code = line[:8] # \n",
        "#         file_name = for_processed_OUT_PATH+\"/\"+code\n",
        "#         if line[8:11] == \"|t|\": # start=filename\n",
        "#             clear_file([file_name])\n",
        "#         write_append_data_to_txt_file(file_name, line[:-1])\n",
        "#         code_ls.add(code)\n",
        "#         \"\"\" Use for fine abnormal chars\"\"\"\n",
        "#         # line_ls_tab_split_org = line.split('\\t')\n",
        "#         # print(\"line_ls_tab_split >>> \", line_ls_tab_split)\n",
        "#         # if len(line_ls_tab_split_org) > 4 and  line_ls_tab_split_org[4] in \\\n",
        "#         #         [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "#         #     line_ls_tab_split = line_ls_tab_split_org[3].replace(\"_\",\" \")\n",
        "#         #     line_ls_tab_split = line_ls_tab_split.replace(\" \",\"\")\n",
        "#         #     line_ls_tab_split = remove_accents(line_ls_tab_split)\n",
        "#         #     if not re.match(\"^[A-Za-z0-9_-]*$\", line_ls_tab_split):\n",
        "#         #         write_append_data_to_txt_file(\n",
        "#         #             abnormal_chars_file_path,\n",
        "#         #             line_ls_tab_split_org[3]\n",
        "#         #         )\n",
        "\n",
        "# special_split_sent_not_final_IN_path_folder = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "# onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "\n",
        "# print(onlyfiles)\n",
        "                \n",
        "# for idx, special_split_sent_not_final_IN_path in enumerate(onlyfiles):\n",
        "#     write_append_data_to_txt_file(\n",
        "#         special_split_sent_not_final_IN_path_folder+'/'+ special_split_sent_not_final_IN_path,\n",
        "#         \"<EOF>\"\n",
        "#     )\n",
        "            "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### START TO CREATE PROCESSED FILE\n",
        "# sample:\n",
        "625456\tObsolete ... preparations .|One case ... years .|\n",
        "\n",
        "1:CID:2\tR2L\tNON-CROSS\t27-31\t10-11\t\n",
        "    D002119\tcalcium carbon - ate\tChemical\t27\t31\t1\t\n",
        "    D006934\thypercalcaemia\tDisease\t10\t11\t1\t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import itertools\n",
        "OUT_error_not_found_ent_in_msg_FOLDER_path = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/error/error_not_found_ent_in_msg_to_half_manual_edit_words_in_doc\"\n",
        "clear_file([f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/txt.txt\"])\n",
        "\n",
        "\n",
        "def get_NR_pair(data_dict_ents, data_dict_rels):\n",
        "    data_rels_pair_set = [set(i) for k, v in data_dict_rels.items() for i in v ]\n",
        "    classify_pairs_dict = {\n",
        "            \"PERSON-LOCATION\" : (\"PERSON\", \"LOCATION\"),\n",
        "            \"ORGANIZATION-LOCATION\" : (\"ORGANIZATION\", \"LOCATION\"),\n",
        "            \"LOCATION-LOCATION\" : (\"LOCATION\", \"LOCATION\"),\n",
        "            \"ORGANIZATION-ORGANIZATION\" : (\"ORGANIZATION\", \"ORGANIZATION\"),\n",
        "            \"PERSON-PERSON\" : (\"PERSON\", \"PERSON\"),\n",
        "            \"PERSON-ORGANIZATION\" : (\"PERSON\", \"ORGANIZATION\"),\n",
        "            \"LOCATION-ORGANIZATION\" : (\"LOCATION\", \"ORGANIZATION\")\n",
        "    }\n",
        "\n",
        "    ent_type_with_data_dict = \\\n",
        "                        {   \"ORGANIZATION\" : [],\n",
        "                            \"PERSON\" : [],\n",
        "                            \"LOCATION\" : []\n",
        "                        }\n",
        "\n",
        "    for ent_id, ent_meta_data in data_dict_ents.items():\n",
        "        data = ent_meta_data[\"data\"]\n",
        "        ent_type = data[0][3]\n",
        "        ent_type_with_data_dict[ent_type].append(ent_id)\n",
        "\n",
        "\n",
        "    # find the pair with rule \"pair for classified\"\n",
        "    total_pairs = []\n",
        "    for classify_pairs_name, classify_pairs in classify_pairs_dict.items():\n",
        "        ent_1 = ent_type_with_data_dict[classify_pairs[0]]\n",
        "        ent_2 = ent_type_with_data_dict[classify_pairs[1]]\n",
        "        # all_pair = set(itertools.product(*[ent_1 , ent_2]))\n",
        "        all_pair = [(i,j) for i in ent_1 for j in ent_2]\n",
        "\n",
        "        new_pair = [set(pair) for pair in all_pair  \n",
        "                        if((set(pair) not in data_rels_pair_set) and len(set(pair))==2 )\n",
        "                    ]\n",
        "        total_pairs+=new_pair\n",
        "        # print(\"- \"*50)\n",
        "        # print(classify_pairs_name, len(all_pair), len(new_pair))\n",
        "        # print(all_pair)\n",
        "        # print(new_pair)\n",
        "        # break\n",
        "\n",
        "    total_NR_pairs = []\n",
        "    for i in total_pairs:\n",
        "        il = list(i)\n",
        "        ent_1_in_sent = [ i[2] for i in data_dict_ents[il[0]][\"data\"]]\n",
        "        ent_2_in_sent = [ i[2] for i in data_dict_ents[il[1]][\"data\"]]\n",
        "        cross_sent = set(ent_1_in_sent).intersection(ent_2_in_sent)\n",
        "        if cross_sent:\n",
        "            # print(\"=\"*50)\n",
        "            # print(il[0], ent_1_in_sent)\n",
        "            # print(il[1], ent_2_in_sent)\n",
        "            # print(cross_sent)\n",
        "            # get list entity cross sent\n",
        "            list_ent_1_in_cross_sent = [ i+[il[0]] for i in data_dict_ents[il[0]][\"data\"] \n",
        "                                            if i[2] in cross_sent\n",
        "                                        ]\n",
        "            \n",
        "            list_ent_2_in_cross_sent = [ i+[il[1]] for i in data_dict_ents[il[1]][\"data\"] \n",
        "                                            if i[2] in cross_sent\n",
        "                                        ]\n",
        "            _pair = [(i,j) for i in list_ent_1_in_cross_sent for j in list_ent_2_in_cross_sent]        \n",
        "            total_NR_pairs.append(_pair)\n",
        "            # pprint(all_pair)\n",
        "    return total_NR_pairs\n",
        "\n",
        "def process_NR_pair_to_text(nr_pairs_ls, processes_ent_line_dict):\n",
        "    \"\"\" create \"processed-txt\" for NR pair\n",
        "    IN:  \n",
        "        nr_pairs_ls:\n",
        "                    [\n",
        "                        (   [94, 95, 3, 'LOCATION', '2335248900117'],\n",
        "                            [102, 103, 3, 'PERSON', '2335248900126']\n",
        "                        ),\n",
        "                        (   [112, 113, 3, 'LOCATION', '2335248900117'],\n",
        "                            [102, 103, 3, 'PERSON', '2335248900126']\n",
        "                        )\n",
        "                    ]\n",
        "        processes_ent_line_dict:  \n",
        "                    '2335248900217': '2335248900217\\tAsensio\\tPERSON\\t177\\t178\\t6' ...\n",
        "    OUT:\n",
        "                    1:NR:2\tL2R\tNON-CROSS\t161-162\t177-178\t\n",
        "                        2335248900156\tZidane|Zidane|Zidane|Zidane\tPERSON\t126:161:168:214\t127:162:169:215\t4:6:6:8\t\n",
        "                        2335248900217\tAsensio\tPERSON\t177\t178\t6\n",
        "    \"\"\"\n",
        "\n",
        "    list_NR_processed_completed_text_ls=[]\n",
        "    for nr_pairs in nr_pairs_ls:\n",
        "        for pair in nr_pairs:\n",
        "            r2l_or_l2r = \"L2R\"\n",
        "            rel = f\"1:NR:2\\t{r2l_or_l2r}\\tNON-CROSS\\t{pair[0][0]}-{pair[0][1]}\\t{pair[1][0]}-{pair[1][1]}\"\n",
        "            ent_1 = processes_ent_line_dict[pair[0][4]]\n",
        "            ent_2 = processes_ent_line_dict[pair[1][4]]\n",
        "            full_txt = f\"{rel}\\t{ent_1}\\t{ent_2}\"\n",
        "            list_NR_processed_completed_text_ls.append(full_txt)\n",
        "    return list_NR_processed_completed_text_ls\n",
        "\n",
        "# nr_pairs_ls = get_NR_pair(data_dict_ents, data_dict_rels)\n",
        "# list_NR_processed_completed_text_ls = process_NR_pair_to_text(nr_pairs_ls, processes_ent_line_dict)\n",
        "# print(list_NR_processed_completed_text_ls[0])\n",
        "\n",
        "\n",
        "def create_range_sentence_list(sents_ls):\n",
        "                \n",
        "    \"\"\" \n",
        "        create list sentence range to use for get pair has relation\n",
        "        IN sentent list [\"<sent-1>\", \"<sent-2>\"...]\n",
        "        OUT range of sent by word [(0,10)(11,25),(26,70)...]\n",
        "    \"\"\"\n",
        "    \n",
        "    range_sent = [[0, len(sents_ls[0].split(\" \"))]]\n",
        "    i = 0\n",
        "    prev_len = 0\n",
        "    current_len = 0\n",
        "    for s in sents_ls:\n",
        "        # ignore 1st\n",
        "        if i == 0:\n",
        "            i+=1\n",
        "            continue\n",
        "        current_len = range_sent[i-1][1]\n",
        "        prev_len = current_len + len(s.split(\" \"))\n",
        "        range_sent.append([current_len, prev_len])\n",
        "        i+=1   \n",
        "    return range_sent\n",
        "\n",
        "\n",
        "                \n",
        "def proccess_full_prepare_line_to_processed_text(full_data_to_processed):\n",
        "\n",
        "    exit_with_err =  full_data_to_processed[\"exit_with_err\"]\n",
        "    total_ent_a_doc =  full_data_to_processed[\"total_ent_a_doc\"]\n",
        "    current_code =  full_data_to_processed[\"current_code\"]\n",
        "    prev_code =  full_data_to_processed[\"prev_code\"]\n",
        "    passage =  full_data_to_processed[\"passage\"]\n",
        "    rel_dict =  full_data_to_processed[\"rel_dict\"]\n",
        "    ent_dict =  full_data_to_processed[\"ent_dict\"]\n",
        "    passage_sents_ls =  full_data_to_processed[\"passage_sents_ls\"]\n",
        "    range_sent =  full_data_to_processed[\"range_sent\"]\n",
        "    ent_dict_count_by_label = full_data_to_processed[\"ent_dict_count_by_label\"]\n",
        "\n",
        "\n",
        "    real_count_NOT_equal_len_ent = False\n",
        "    \n",
        "    total_ent_dict_count = 0\n",
        "    \n",
        "    list_ent_not_in_passage = f\"{current_code}\\t\\t: nof found ent in passage\"\n",
        "    # pprint(ent_dict)\n",
        "\n",
        "    for k,v in ent_dict.items():\n",
        "        total_ent_dict_count+=v['real_count']\n",
        "        # if v[\"real_count\"] != len(v[\"data\"]):\n",
        "        #     # print(v[\"real_count\"])\n",
        "        #     # print(len(v[\"data\"]))\n",
        "        #     real_count_NOT_equal_len_ent = True\n",
        "        #     break\n",
        "\n",
        "        # not create dict of error ent (not seen in sent) anymore \n",
        "        err_ent_dict = {}\n",
        "        # err_ent_dict[current_code] = {}\n",
        "        if v.get(\"data\", None) == None:\n",
        "            # print(\"* \"*10)\n",
        "            # print(\"* \"*10)\n",
        "            # print(\"* \"*10)\n",
        "            # print(k,v)\n",
        "            ent_no_sent = ent_dict_count_by_label[k][\"word\"]\n",
        "            ent_no_sent_new = ent_no_sent.replace(\" \",\"_\")\n",
        "            list_ent_not_in_passage+=f\"\\t\\t{k}\\t\\t{ent_no_sent}|$|{ent_no_sent_new}\"\n",
        "            # err_ent_dict[current_code][k] = {}\n",
        "            # err_ent_dict[current_code][k][\"word\"] = [ent_no_sent]\n",
        "            # err_ent_dict[current_code][k][\"related_word_in_sent\"] = []\n",
        "\n",
        "            # org find related word to ent in sent\n",
        "            # for w in passage_sents_ls:\n",
        "            #     ent_no_sent_spl = ent_no_sent.replace(\"_\", \" \").replace(\".\", \" \")\n",
        "            #     w_spl = w.replace(\"_\", \" \").replace(\".\", \" \")\n",
        "            #     if ent_no_sent_spl in w_spl:\n",
        "            #         w_n = w.replace(\"_\",\" \")                    \n",
        "            #         w_n = w_n.replace(ent_no_sent.replace(\"_\",\" \"), ent_no_sent_new)\n",
        "            #         print(\"w, w_n >>> \",w, w_n, ent_no_sent, len(ent_no_sent))\n",
        "            #         list_ent_not_in_passage+=f\"\\t\\t{w}|$|{w_n}\"\n",
        "            \n",
        "            ls_related = find_related_word_in_sent(passage, ent_no_sent)\n",
        "            # print(\"ls_related \", ls_related)\n",
        "\n",
        "            for rl in ls_related:\n",
        "                w, w_n = rl\n",
        "                list_ent_not_in_passage+=f\"\\t\\t{w}|$|{w_n}\"\n",
        "\n",
        "                    # err_ent_dict[current_code][k][\"related_word_in_sent\"].append([w])\n",
        "        # print(\"* \"*50)\n",
        "        # print(v.get(\"data\", None))\n",
        "        # print(v.get(\"data\", None))\n",
        "        # print(\"* \"*50)\n",
        "        # print(\"* \"*50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if list_ent_not_in_passage != f\"{current_code}\\t\\t: nof found ent in passage\":\n",
        "            # print(\"* \"*50)\n",
        "            # print(\"* \"*50)\n",
        "            # print(err_ent_dict)\n",
        "            write_append_data_to_txt_file(f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/txt.txt\",list_ent_not_in_passage)\n",
        "            # update_json_to_json_file(err_ent_dict, f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/json.json\")\n",
        "\n",
        "            return list_ent_not_in_passage\n",
        "        \"\"\"\n",
        "            this will handling the enity \"Not found in the passage\" ( defualt value = -100)\n",
        "        \"\"\"\n",
        "        # for w in v[\"data\"]:\n",
        "        #     if w[-1] == -100:\n",
        "        #         exit_with_err = \"err no sent id\"\n",
        "        #         return exit_with_err\n",
        "\n",
        "        if v[\"real_count\"] != len(v[\"data\"]):\n",
        "            # print(v[\"real_count\"])\n",
        "            # print(len(v[\"data\"]))\n",
        "            real_count_NOT_equal_len_ent = True\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    if exit_with_err != \"\":\n",
        "        print(exit_with_err)\n",
        "        print(\"^ \"*50)\n",
        "        return exit_with_err\n",
        "\n",
        "\n",
        "    if real_count_NOT_equal_len_ent:\n",
        "        exit_with_err = \"REAL COUNT ERROR \"*10\n",
        "        return exit_with_err\n",
        "\n",
        "        \n",
        "    \"\"\"\n",
        "        Agree that:\n",
        "            - residual entity\n",
        "            - if miss entity --> these entity(code) not belong to relation \n",
        "    \"\"\"\n",
        "    # print(\"???? \"*40)\n",
        "    # print(total_ent_a_doc, total_ent_dict_count)\n",
        "    if(total_ent_a_doc != total_ent_dict_count):\n",
        "\n",
        "        # print(\"*\"*50)\n",
        "        # pprint(rel_dict)\n",
        "        # pprint(ent_dict)\n",
        "        # pprint(ent_dict_count_by_label)\n",
        "        # print(\"*\"*50)\n",
        "\n",
        "\n",
        "        set_ent_has_rel = set(i for k,v in rel_dict.items() for item in v for i in item)\n",
        "        ent_in_rel_break_rule = False\n",
        "        for k, v in ent_dict.items():\n",
        "            # for i in v[\"words\"]:\n",
        "            #     print(i)\n",
        "\n",
        "            # print(\"detail \")\n",
        "            # print(\"- \"*40)\n",
        "            if ent_dict_count_by_label[k][\"label_ent_count\"] != v[\"real_count\"]:\n",
        "                if k in set_ent_has_rel:\n",
        "                    if v[\"real_count\"] < ent_dict_count_by_label[k][\"label_ent_count\"]:\n",
        "                        # ent_in_rel_break_rule = True\n",
        "                        w =v[\"words\"][0]\n",
        " \n",
        "                        ln = ent_dict_count_by_label[k]\n",
        "                        rn = v[\"real_count\"]\n",
        "                        print(f\"{w}: label{ln}, but real is {rn}\")\n",
        "                        # print(w)\n",
        "\n",
        "        # sum_label_ent = sum(ent_dict_count_by_label.values())\n",
        "        # print(f\"[sum_label_ent: {sum_label_ent}] total_ent_a_doc: {total_ent_a_doc} != total_ent_dict_count(real_in_doc): {total_ent_dict_count}\")\n",
        "        if ent_in_rel_break_rule:\n",
        "            exit_with_err = \"TOTAL ENT ERROR \"*10\n",
        "            return exit_with_err\n",
        "    \n",
        "    \"\"\"\n",
        "        handling mapping relation ent and w2f\n",
        "    \"\"\"\n",
        "    processes_ent_line_dict = {}\n",
        "    for ent_code, ent_ls in ent_dict.items():             \n",
        "        start_idx = \"\"\n",
        "        end_idx = \"\"\n",
        "        sent_idx = \"\"\n",
        "        ent_words_merge = \"\"                \n",
        "        \n",
        "        # ignore relation cannot find on passage because of (_) and split(\" \")\n",
        "        err_not_found_ent_in_passage = False\n",
        "        if ent_ls.get(\"data\", None):\n",
        "            for k0, v0 in rel_dict.items():\n",
        "                if ent_code in v0:\n",
        "                    err_not_found_ent_in_passage = True\n",
        "                    exit_with_err = \"cannot find on passage\"\n",
        "                    break\n",
        "        else:\n",
        "            if err_not_found_ent_in_passage == False:\n",
        "                continue\n",
        "        \n",
        "        if exit_with_err != \"\":\n",
        "            return exit_with_err\n",
        "            \n",
        "        for ent in ent_ls[\"data\"]:\n",
        "            start_idx += \":\"+str(ent[0])\n",
        "            end_idx += \":\"+str(ent[1])\n",
        "            sent_idx += \":\"+str(ent[2])\n",
        "            ent_words_merge +=(\"|\"+ \" \".join(passage_sents_ls[ent[0]:ent[1]]))\n",
        "        ent_type = ent_ls[\"data\"][0][3]\n",
        "        processes_ent_line_dict[ent_code] = f\"{ent_code}\\t{ent_words_merge[1:]}\\t{ent_type}\\t{start_idx[1:]}\\t{end_idx[1:]}\\t{sent_idx[1:]}\"\n",
        "                \n",
        "    \"\"\"\n",
        "        process sentence by sentene\n",
        "\n",
        "    Returns:\n",
        "        save data of sent: doc, passage_split_list, ent, relation\n",
        "    \"\"\"\n",
        "    pair = \"\"\n",
        "    processes_rel_pair_line_list = []\n",
        "\n",
        "    # print(\"*\"*50)\n",
        "    # print(\"*\"*50)\n",
        "    # pprint(ent_dict)\n",
        "    # print(\"*\"*50)\n",
        "    # print(\"*\"*50)\n",
        "    # pprint(rel_dict)\n",
        "\n",
        "    for rel_name, ent_pair_ls in rel_dict.items():\n",
        "        for ent_pair in ent_pair_ls:\n",
        "            # print(\"*\"*50)\n",
        "            # print(\"ent_pair >>> \", rel_name, ent_pair)\n",
        "            ent_first_ls = ent_dict[ent_pair[0]][\"data\"]\n",
        "            ent_sec_ls = ent_dict[ent_pair[1]][\"data\"]\n",
        "\n",
        "            if ent_first_ls:\n",
        "                ent_first_ls.sort(key=lambda x: x[2])\n",
        "            if ent_sec_ls:\n",
        "                ent_sec_ls.sort(key=lambda x: x[2])\n",
        "\n",
        "            pair = \"1:<error relation pair line>:2\"\n",
        "            # print(\"ent_first_ls \", [i[2] for i in ent_first_ls])\n",
        "            # print(\"ent_sec_ls \", [i[2] for i in ent_sec_ls])\n",
        "            # print(\"error_doc_list_has_name_print \", error_doc_list_has_name_print)\n",
        "            for ent in ent_first_ls:\n",
        "                ent_sec_part = list(filter(lambda ent_sec: ent_sec[2] == ent[2], ent_sec_ls))\n",
        "                if ent_sec_part:\n",
        "                    ent_sec_part = ent_sec_part[0]\n",
        "                    r2l_or_l2r = \"L2R\"\n",
        "                    pair = f\"1:{rel_name}:2\\t{r2l_or_l2r}\\tNON-CROSS\\t{ent[0]}-{ent[1]}\\t{ent_sec_part[0]}-{ent_sec_part[1]}\"\n",
        "                    break \n",
        "                \n",
        "            if pair == \"1:<error relation pair line>:2\":\n",
        "                exit_with_err = \"entity not in the passage\" \n",
        "                # HARD CODE to skipp the \n",
        "                \n",
        "                if error_doc_list_has_name_print_BYPASS_ERR and \\\n",
        "                            current_code in error_doc_list_has_name_print:\n",
        "                    error_doc_list_has_name_print_BYPASS_ls.append(f\"doc: {current_code}, ent_pair: {rel_name} {ent_pair}\")\n",
        "                    continue\n",
        "\n",
        "                return exit_with_err\n",
        "            processed_rel_pair_line = \\\n",
        "                f\"{pair}\\t{(processes_ent_line_dict[ent_pair[0]])}\\t{(processes_ent_line_dict[ent_pair[1]])}\"     \n",
        "            processes_rel_pair_line_list.append(processed_rel_pair_line)\n",
        "    return processes_rel_pair_line_list, processes_ent_line_dict\n",
        "\n",
        "\n",
        "\n",
        "def process_entity(full_data_to_processed):\n",
        "    exit_with_err =  full_data_to_processed[\"exit_with_err\"]\n",
        "    total_ent_a_doc =  full_data_to_processed[\"total_ent_a_doc\"]\n",
        "    current_code =  full_data_to_processed[\"current_code\"]\n",
        "    prev_code =  full_data_to_processed[\"prev_code\"]\n",
        "    passage =  full_data_to_processed[\"passage\"]\n",
        "    rel_dict =  full_data_to_processed[\"rel_dict\"]\n",
        "    ent_dict =  full_data_to_processed[\"ent_dict\"]\n",
        "    passage_sents_ls =  full_data_to_processed[\"passage_sents_ls\"]\n",
        "    range_sent =  full_data_to_processed[\"range_sent\"]\n",
        "    line_ls_tab_split =  full_data_to_processed[\"line_ls_tab_split\"]\n",
        "    # ent_dict_count_by_label = full_data_to_processed[\"ent_dict_count_by_label\"]\n",
        "\n",
        "\n",
        "\n",
        "    total_ent_a_doc+=1\n",
        "    ent_code = line_ls_tab_split[5][:-1]\n",
        "    ent_word = line_ls_tab_split[3]\n",
        "    # pprint(ent_dict)\n",
        "    # print(\"ent_word \", ent_word)\n",
        "    # if ent_dict_count_by_label.get(ent_code, None) == None:\n",
        "    #     ent_dict_count_by_label[ent_code] = 0\n",
        "    # else:\n",
        "    #     ent_dict_count_by_label[ent_code] +=1\n",
        "\n",
        "    if ent_dict.get(ent_code, None) == None: \n",
        "        ent_dict[ent_code] = {}\n",
        "        ent_dict[ent_code][\"real_count\"] = 0\n",
        "\n",
        "        # if ent_word == \"Tây Ban_Nha\":\n",
        "        #     print(\"1 \"*100)\n",
        "        #     print(ent_dict[ent_code])\n",
        "            \n",
        "    else:\n",
        "        # inore checked words, \"Real_Madrid\" but not \"Real\"\n",
        "        checked_workd_Real_Madrid = False\n",
        "        \n",
        "\n",
        "        # handle entity error append to list\n",
        "        if ent_dict[ent_code].get(\"data\", None) == None:\n",
        "\n",
        "        \n",
        "            # print(\"* \"*10)\n",
        "            # print(\"? \"*10)\n",
        "            # print(\"* \"*10)\n",
        "\n",
        "            list_ent_not_in_passage = f\"{current_code}\\t\\t: nof found ent in passage\"             \n",
        "            ent_no_sent = ent_word\n",
        "            ent_no_sent_new = ent_no_sent.replace(\" \",\"_\")\n",
        "            list_ent_not_in_passage+=f\"\\t\\t{ent_code}\\t\\t{ent_no_sent}|$|{ent_no_sent_new}\"\n",
        "        \n",
        "            ls_related = find_related_word_in_sent(passage, ent_no_sent)\n",
        "            # print(\"ls_related \", ls_related)\n",
        "            for rl in ls_related:\n",
        "                w, w_n = rl\n",
        "                list_ent_not_in_passage+=f\"\\t\\t{w}|$|{w_n}\"\n",
        "            write_append_data_to_txt_file(f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/txt.txt\",list_ent_not_in_passage)\n",
        "            # update_json_to_json_file(err_ent_dict, f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/json.json\")\n",
        "\n",
        "            print(\"list_ent_not_in_passage \", list_ent_not_in_passage)\n",
        "            return list_ent_not_in_passage\n",
        "\n",
        "\n",
        "        for we in ent_dict[ent_code][\"data\"]:\n",
        "            \n",
        "            if passage_sents_ls[we[0]:we[1]] == ent_word.split(\" \"):\n",
        "                # \"Real_Madrid\" but may not \"Real\"\n",
        "                # print(\"Real Madrid but may not Real\")\n",
        "                # print(passage_sents_ls[we[0]:we[1]],ent_word)\n",
        "                checked_workd_Real_Madrid = True\n",
        "                break\n",
        "        if checked_workd_Real_Madrid:\n",
        "            # continue\n",
        "            \n",
        "            \n",
        "            full_data_to_processed = \\\n",
        "                {\n",
        "                    \"exit_with_err\": exit_with_err,\n",
        "                    \"total_ent_a_doc\": total_ent_a_doc,\n",
        "                    \"current_code\": current_code,\n",
        "                    \"prev_code\": prev_code,\n",
        "                    \"passage\": passage,\n",
        "                    \"rel_dict\": rel_dict,\n",
        "                    \"ent_dict\": ent_dict,\n",
        "                    \"passage_sents_ls\": passage_sents_ls,\n",
        "                    \"range_sent\": range_sent,\n",
        "                    # \"ent_dict_count_by_label\":ent_dict_count_by_label\n",
        "            }\n",
        "                        \n",
        "            \n",
        "            return full_data_to_processed\n",
        "        # print(\"RealMadrid but may not Real\")\n",
        "        # print(ent_dict[ent_code][\"data\"],ent_word)\n",
        "\n",
        "    \"\"\" \n",
        "        start to check entity & get index in sentence\n",
        "        \n",
        "    \"\"\"                        \n",
        "    has_ent_in_sent = ent_word.split(\" \")\n",
        "    \n",
        "    # print(\">> has_ent_in_sent \", has_ent_in_sent)\n",
        "    \n",
        "    # return if not found here --> clean data again\n",
        "    not_found_ent_in_sent = True\n",
        "    for idx_w, word in enumerate(passage_sents_ls):\n",
        "        if (has_ent_in_sent[0] == word):\n",
        "            \n",
        "            # print(\"iter word in sent ...\")\n",
        "            # print(has_ent_in_sent[0], word)\n",
        "            end_expect_idx = idx_w+len(has_ent_in_sent)\n",
        "            if has_ent_in_sent ==passage_sents_ls[idx_w:end_expect_idx]:\n",
        "                sent_idx = -100\n",
        "                # if ent_word == \"Real_Sociedad\":\n",
        "                #     print(idx_w,end_expect_idx)\n",
        "                \n",
        "                for id_range, _range in enumerate(range_sent):  \n",
        "                    # print(range(_range[0], _range[1]))                                  \n",
        "                    if idx_w in range(_range[0], _range[1]) and \\\n",
        "                                end_expect_idx in range(_range[0], _range[1]):\n",
        "                        sent_idx = id_range\n",
        "                        break \n",
        "                                      \n",
        "                ent_data = [idx_w, end_expect_idx, sent_idx, line_ls_tab_split[4]]\n",
        "                # print(\"ent_data \", passage_sents_ls[idx_w:end_expect_idx][0], ent_data)                               \n",
        "                if ent_dict[ent_code].get(\"data\", None) == None:                  \n",
        "                    ent_dict[ent_code][\"data\"] = [ent_data]\n",
        "                    ent_dict[ent_code][\"words\"] = []\n",
        "                    ent_dict[ent_code][\"words\"].append(ent_word)\n",
        "                else:\n",
        "                    ent_dict[ent_code][\"data\"].append(ent_data)  \n",
        "                    ent_dict[ent_code][\"words\"].append(ent_word)\n",
        "\n",
        "                ent_dict[ent_code][\"real_count\"] +=1\n",
        "                not_found_ent_in_sent = False\n",
        "\n",
        "    \"\"\"\n",
        "        Skip handling \"entity not in the passage\"\n",
        "            - To save the \"error\" ent to docs --> solve all in once\n",
        "            - will handle at last processed process\n",
        "    \n",
        "    \"\"\"\n",
        "    # if not_found_ent_in_sent:\n",
        "    #     exit_with_err = \"entity not in the passage\"    \n",
        "    #     print(\"ent err ...\")                                                                \n",
        "    #     return exit_with_err\n",
        "\n",
        "    full_data_to_processed = \\\n",
        "        {\n",
        "            \"exit_with_err\": exit_with_err,\n",
        "            \"total_ent_a_doc\": total_ent_a_doc,\n",
        "            \"current_code\": current_code,\n",
        "            \"prev_code\": prev_code,\n",
        "            \"passage\": passage,\n",
        "            \"rel_dict\": rel_dict,\n",
        "            \"ent_dict\": ent_dict,\n",
        "            \"passage_sents_ls\": passage_sents_ls,\n",
        "            \"range_sent\": range_sent,\n",
        "            # \"ent_dict_count_by_label\":ent_dict_count_by_label\n",
        "        }\n",
        "    return full_data_to_processed\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['23351113', '23351164', '23351190', '23351214', '23351225', '23351260', '23351307', '23351316', '23351318', '23351385', '23351391', '23351392', '23351393', '23351394', '23351416', '23351422', '23351424', '23351425', '23351426', '23351427', '23351430', '23351431', '23351432', '23351434', '23351435', '23351436', '23351437', '23351438', '23351440', '23351460', '23351489', '23351493', '23351494', '23351510', '23351511', '23351514', '23351515', '23351516', '23351518', '23351519', '23351521', '23351522', '23351524', '23351542', '23351543', '23351554', '23351555', '23351556', '23351561', '23351562', '23351563', '23351564', '23351566', '23351567', '23351569', '23351571', '23351574', '23351576', '23351578', '23351579', '23351581', '23351582', '23351595', '23351607', '23351611', '23351612', '23351615', '23351617', '23351627', '23351632', '23351635', '23351636', '23351642', '23351645', '23351647', '23351649', '23351650', '23351651', '23351652', '23351653', '23351672', '23351700', '23351719', '23351749', '23351778', '23351809', '23351814', '23351815', '23351817', '23351820', '23351831', '23351834', '23351837', '23351839', '23351841', '23351846', '23351848', '23351849', '23351851', '23351852', '23351853', '23351856', '23351858', '23351861', '23351864', '23351887', '23351888', '23351923', '23351931', '23351933', '23351937', '23351939', '23351941', '23351946', '23351947', '23351948', '23351949', '23351950', '23351952', '23351956', '23351959', '23351960', '23351961', '23351963', '23351965', '23351967', '23351969', '23351970', '23351971', '23351974', '23351978', '23351979', '23351981', '23351982', '23351983', '23351985', '23351987', '23351988', '23351990', '23351992', '23351994', '23351995', '23352656', '23352659', '23352662', '23352663', '23352665', '23352671', '23352674', '23352675', '23352676', '23352677', '23352681', '23352682', '23352683', '23352684', '23352686', '23352687', '23352690', '23352693', '23352695', '23352696', '23352701', '23352702', '23352704', '23352706', '23352707', '23352708', '23352710', '23352713', '23352715', '23352717', '23352718', '23352719', '23352720', '23352725', '23352730', '23352731', '23352738', '23352739', '23352743', '23352746', '23352748', '23352750', '23352751', '23352752', '23352753', '23352754', '23352755', '23352757', '23352765', '23352769', '23352774', '23352777', '23352778', '23352781', '23352785', '23352787', '23352792', '23352795', '23352800', '23352802', '23352804', '23352806', '23352807', '23352814', '23352820', '23352821', '23352822', '23352824', '23352825', '23352829', '23352830', '23352831', '23352844', '23352845', '23352849', '23352853', '23352856', '23352857', '23352870', '23352871', '23352872', '23352874', '23352876', '23352878', '23352880', '23352883', '23352886', '23352887', '23352892', '23352894', '23352896', '23352899', '23352900', '23353721', '23353723', '23353727', '23353732', '23353755', '23353760', '23353773', '23353779', '23353780', '23353785', '23353786', '23353787', '23353791', '23353794', '23353799', '23353801', '23353825', '23353830', '23353834', '23353838', '23353841', '23353842', '23353846', '23353849', '23353857', '23353860', '23353861', '23353863', '23353864', '23353867', '23353872', '23353874', '23353878', '23353901', '23353904', '23353913', '23353931', '23353944', '23353945', '23353950', '23353954', '23353967', '23353973', '23353975', '23353976', '23353995', '23353996', '23354010', '23354027', '23354028', '23354042', '23354045', '23354055', '23354082', '23354085', '23354089', '23354103', '23354126', '23354130', '23354166', '23354202', '23354219', '23354244', '23354253', '23354265', '23354285', '23354288', '23354310', '23354318', '23354320', '23354336', '23354396', '23354400', '23354442', '23354450', '23354460', '23354474', '23354516', '23354536', '23354538', '23354544', '23354545', '23354575', '23354619', '23354627', '23354648', '23354656', '23354695', '23354697', '23354698', '23354699', '23354717', '23354718', '23354738', '23354739', '23354751', '23354793', '23354796', '23354803', '23354816', '23354831', '23354879', '23354880', '23354881', '23354910', '23354912', '23354916', '23354920', '23354935', '23354944', '23354946', '23354953', '23354956', '23354977', '23354982', '23355001', '23355040', '23355061', '23355064', '23355095', '23355132', '23355228', '23355250', '23355254', '23355290', '23355416', '23355434', '23355470', '23355557', '23355571', '23355626', '23355656', '23355773', '23355817', '23355858', '23355917', '23355935', '23355988', '23356093', '23356193', '23356205', '23356221', '23356245', '23356247', '23356295', '23356299', '23356314', '23356315', '23356329', '23356339', '23356494', '23356511', '23356604', '23356624', '23356638', '23356715', '23356716', '23356724', '23356731', '23356745', '23356765', '23356767', '23356771', '23356782', '23356798', '23356858', '23356874', '23356885', '23356887', '23356902', '23356907', '23356915', '23356918', '23356933', '23356960', '23356961', '23356992', '23356993', '23357028', '23357037', '23357062', '23357081', '23357094', '23357095', '23357097', '23357120', '23357135', '23357151', '23357167', '23357190', '23357240', '23357258', '23357263', '23357266', '23357288', '23357308', '23357309', '23357329', '23357336', '23357341', '23357344', '23357389', '23357394', '23357396', '23357443', '23357457', '23357471', '23357489', '23357534', '23357544', '23357550', '23357652', '23357711', '23357741', '23357752', '23357765', '23357809', '23357851', '23357937', '23357994', '23358011', '23358097', '23358104', '23358261', '23366716', '23366722', '23366740', '23366751', '23366765']\n",
            "0 processing docs:  23351113\n",
            "1 processing docs:  23351164\n",
            "2 processing docs:  23351190\n",
            "3 processing docs:  23351214\n",
            "4 processing docs:  23351225\n",
            "5 processing docs:  23351260\n",
            "**** ignore this doc 23351307 (TOO-BIG)\n",
            "7 processing docs:  23351316\n",
            "8 processing docs:  23351318\n",
            "9 processing docs:  23351385\n",
            "10 processing docs:  23351391\n",
            "11 processing docs:  23351392\n",
            "12 processing docs:  23351393\n",
            "13 processing docs:  23351394\n",
            "14 processing docs:  23351416\n",
            "15 processing docs:  23351422\n",
            "16 processing docs:  23351424\n",
            "17 processing docs:  23351425\n",
            "**** ignore this doc 23351426 (TOO-BIG)\n",
            "19 processing docs:  23351427\n",
            "**** ignore this doc 23351430 (TOO-BIG)\n",
            "21 processing docs:  23351431\n",
            "22 processing docs:  23351432\n",
            "23 processing docs:  23351434\n",
            "24 processing docs:  23351435\n",
            "25 processing docs:  23351436\n",
            "26 processing docs:  23351437\n",
            "27 processing docs:  23351438\n",
            "28 processing docs:  23351440\n",
            "29 processing docs:  23351460\n",
            "30 processing docs:  23351489\n",
            "31 processing docs:  23351493\n",
            "32 processing docs:  23351494\n",
            "33 processing docs:  23351510\n",
            "34 processing docs:  23351511\n",
            "35 processing docs:  23351514\n",
            "**** ignore this doc 23351515 (TOO-BIG)\n",
            "37 processing docs:  23351516\n",
            "38 processing docs:  23351518\n",
            "39 processing docs:  23351519\n",
            "------------------------------  found NONE  ------------------------------\n",
            "40 processing docs:  23351521\n",
            "41 processing docs:  23351522\n",
            "42 processing docs:  23351524\n",
            "43 processing docs:  23351542\n",
            "44 processing docs:  23351543\n",
            "45 processing docs:  23351554\n",
            "46 processing docs:  23351555\n",
            "47 processing docs:  23351556\n",
            "48 processing docs:  23351561\n",
            "49 processing docs:  23351562\n",
            "50 processing docs:  23351563\n",
            "51 processing docs:  23351564\n",
            "52 processing docs:  23351566\n",
            "53 processing docs:  23351567\n",
            "54 processing docs:  23351569\n",
            "Kubi: label{'label_ent_count': 7, 'word': 'Kubi'}, but real is 5\n",
            "55 processing docs:  23351571\n",
            "56 processing docs:  23351574\n",
            "57 processing docs:  23351576\n",
            "58 processing docs:  23351578\n",
            "59 processing docs:  23351579\n",
            "60 processing docs:  23351581\n",
            "61 processing docs:  23351582\n",
            "62 processing docs:  23351595\n",
            "63 processing docs:  23351607\n",
            "**** ignore this doc 23351611 (TOO-BIG)\n",
            "65 processing docs:  23351612\n",
            "Neymar: label{'label_ent_count': 10, 'word': 'Neymar'}, but real is 8\n",
            "66 processing docs:  23351615\n",
            "67 processing docs:  23351617\n",
            "68 processing docs:  23351627\n",
            "69 processing docs:  23351632\n",
            "**** ignore this doc 23351635 (TOO-BIG)\n",
            "71 processing docs:  23351636\n",
            "72 processing docs:  23351642\n",
            "73 processing docs:  23351645\n",
            "74 processing docs:  23351647\n",
            "75 processing docs:  23351649\n",
            "76 processing docs:  23351650\n",
            "77 processing docs:  23351651\n",
            "78 processing docs:  23351652\n",
            "79 processing docs:  23351653\n",
            "80 processing docs:  23351672\n",
            "**** ignore this doc 23351700 (TOO-BIG)\n",
            "82 processing docs:  23351719\n",
            "83 processing docs:  23351749\n",
            "84 processing docs:  23351778\n",
            "Chí_Bảo: label{'label_ent_count': 6, 'word': 'Chí_Bảo'}, but real is 5\n",
            "85 processing docs:  23351809\n",
            "86 processing docs:  23351814\n",
            "**** ignore this doc 23351815 (TOO-BIG)\n",
            "88 processing docs:  23351817\n",
            "89 processing docs:  23351820\n",
            "90 processing docs:  23351831\n",
            "91 processing docs:  23351834\n",
            "**** ignore this doc 23351837 (TOO-BIG)\n",
            "93 processing docs:  23351839\n",
            "94 processing docs:  23351841\n",
            "Nguyễn_Tùng_Lâm: label{'label_ent_count': 3, 'word': 'Nguyễn_Tùng_Lâm'}, but real is 2\n",
            "95 processing docs:  23351846\n",
            "96 processing docs:  23351848\n",
            "97 processing docs:  23351849\n",
            "98 processing docs:  23351851\n",
            "99 processing docs:  23351852\n",
            "100 processing docs:  23351853\n",
            "101 processing docs:  23351856\n",
            "102 processing docs:  23351858\n",
            "103 processing docs:  23351861\n",
            "104 processing docs:  23351864\n",
            "105 processing docs:  23351887\n",
            "106 processing docs:  23351888\n",
            "107 processing docs:  23351923\n",
            "108 processing docs:  23351931\n",
            "109 processing docs:  23351933\n",
            "**** ignore this doc 23351937 (TOO-BIG)\n",
            "111 processing docs:  23351939\n",
            "112 processing docs:  23351941\n",
            "**** ignore this doc 23351946 (TOO-BIG)\n",
            "114 processing docs:  23351947\n",
            "115 processing docs:  23351948\n",
            "116 processing docs:  23351949\n",
            "117 processing docs:  23351950\n",
            "118 processing docs:  23351952\n",
            "119 processing docs:  23351956\n",
            "120 processing docs:  23351959\n",
            "**** ignore this doc 23351960 (TOO-BIG)\n",
            "122 processing docs:  23351961\n",
            "123 processing docs:  23351963\n",
            "124 processing docs:  23351965\n",
            "125 processing docs:  23351967\n",
            "126 processing docs:  23351969\n",
            "127 processing docs:  23351970\n",
            "128 processing docs:  23351971\n",
            "129 processing docs:  23351974\n",
            "130 processing docs:  23351978\n",
            "131 processing docs:  23351979\n",
            "132 processing docs:  23351981\n",
            "133 processing docs:  23351982\n",
            "134 processing docs:  23351983\n",
            "**** ignore this doc 23351985 (TOO-BIG)\n",
            "136 processing docs:  23351987\n",
            "137 processing docs:  23351988\n",
            "**** ignore this doc 23351990 (TOO-BIG)\n",
            "**** ignore this doc 23351992 (TOO-BIG)\n",
            "140 processing docs:  23351994\n",
            "141 processing docs:  23351995\n",
            "142 processing docs:  23352656\n",
            "143 processing docs:  23352659\n",
            "144 processing docs:  23352662\n",
            "------------------------------  found NONE  ------------------------------\n",
            "145 processing docs:  23352663\n",
            "**** ignore this doc 23352665 (TOO-BIG)\n",
            "147 processing docs:  23352671\n",
            "148 processing docs:  23352674\n",
            "149 processing docs:  23352675\n",
            "150 processing docs:  23352676\n",
            "151 processing docs:  23352677\n",
            "152 processing docs:  23352681\n",
            "153 processing docs:  23352682\n",
            "**** ignore this doc 23352683 (TOO-BIG)\n",
            "155 processing docs:  23352684\n",
            "156 processing docs:  23352686\n",
            "157 processing docs:  23352687\n",
            "**** ignore this doc 23352690 (TOO-BIG)\n",
            "159 processing docs:  23352693\n",
            "160 processing docs:  23352695\n",
            "161 processing docs:  23352696\n",
            "162 processing docs:  23352701\n",
            "163 processing docs:  23352702\n",
            "164 processing docs:  23352704\n",
            "165 processing docs:  23352706\n",
            "**** ignore this doc 23352707 (TOO-BIG)\n",
            "167 processing docs:  23352708\n",
            "168 processing docs:  23352710\n",
            "169 processing docs:  23352713\n",
            "170 processing docs:  23352715\n",
            "171 processing docs:  23352717\n",
            "172 processing docs:  23352718\n",
            "173 processing docs:  23352719\n",
            "174 processing docs:  23352720\n",
            "175 processing docs:  23352725\n",
            "176 processing docs:  23352730\n",
            "177 processing docs:  23352731\n",
            "**** ignore this doc 23352738 (TOO-BIG)\n",
            "179 processing docs:  23352739\n",
            "180 processing docs:  23352743\n",
            "181 processing docs:  23352746\n",
            "182 processing docs:  23352748\n",
            "183 processing docs:  23352750\n",
            "184 processing docs:  23352751\n",
            "185 processing docs:  23352752\n",
            "**** ignore this doc 23352753 (TOO-BIG)\n",
            "187 processing docs:  23352754\n",
            "188 processing docs:  23352755\n",
            "189 processing docs:  23352757\n",
            "190 processing docs:  23352765\n",
            "191 processing docs:  23352769\n",
            "192 processing docs:  23352774\n",
            "**** ignore this doc 23352777 (TOO-BIG)\n",
            "194 processing docs:  23352778\n",
            "**** ignore this doc 23352781 (TOO-BIG)\n",
            "196 processing docs:  23352785\n",
            "197 processing docs:  23352787\n",
            "198 processing docs:  23352792\n",
            "199 processing docs:  23352795\n",
            "200 processing docs:  23352800\n",
            "201 processing docs:  23352802\n",
            "202 processing docs:  23352804\n",
            "203 processing docs:  23352806\n",
            "204 processing docs:  23352807\n",
            "**** ignore this doc 23352814 (TOO-BIG)\n",
            "206 processing docs:  23352820\n",
            "207 processing docs:  23352821\n",
            "208 processing docs:  23352822\n",
            "209 processing docs:  23352824\n",
            "210 processing docs:  23352825\n",
            "211 processing docs:  23352829\n",
            "212 processing docs:  23352830\n",
            "213 processing docs:  23352831\n",
            "214 processing docs:  23352844\n",
            "215 processing docs:  23352845\n",
            "216 processing docs:  23352849\n",
            "217 processing docs:  23352853\n",
            "**** ignore this doc 23352856 (TOO-BIG)\n",
            "**** ignore this doc 23352857 (TOO-BIG)\n",
            "220 processing docs:  23352870\n",
            "221 processing docs:  23352871\n",
            "222 processing docs:  23352872\n",
            "223 processing docs:  23352874\n",
            "224 processing docs:  23352876\n",
            "225 processing docs:  23352878\n",
            "226 processing docs:  23352880\n",
            "227 processing docs:  23352883\n",
            "228 processing docs:  23352886\n",
            "229 processing docs:  23352887\n",
            "230 processing docs:  23352892\n",
            "231 processing docs:  23352894\n",
            "232 processing docs:  23352896\n",
            "233 processing docs:  23352899\n",
            "234 processing docs:  23352900\n",
            "235 processing docs:  23353721\n",
            "236 processing docs:  23353723\n",
            "237 processing docs:  23353727\n",
            "238 processing docs:  23353732\n",
            "239 processing docs:  23353755\n",
            "240 processing docs:  23353760\n",
            "241 processing docs:  23353773\n",
            "242 processing docs:  23353779\n",
            "243 processing docs:  23353780\n",
            "244 processing docs:  23353785\n",
            "245 processing docs:  23353786\n",
            "246 processing docs:  23353787\n",
            "247 processing docs:  23353791\n",
            "248 processing docs:  23353794\n",
            "249 processing docs:  23353799\n",
            "250 processing docs:  23353801\n",
            "251 processing docs:  23353825\n",
            "252 processing docs:  23353830\n",
            "253 processing docs:  23353834\n",
            "254 processing docs:  23353838\n",
            "255 processing docs:  23353841\n",
            "256 processing docs:  23353842\n",
            "257 processing docs:  23353846\n",
            "258 processing docs:  23353849\n",
            "259 processing docs:  23353857\n",
            "260 processing docs:  23353860\n",
            "Trần_Minh_Điển: label{'label_ent_count': 3, 'word': 'Trần_Minh_Điển'}, but real is 1\n",
            "261 processing docs:  23353861\n",
            "262 processing docs:  23353863\n",
            "263 processing docs:  23353864\n",
            "264 processing docs:  23353867\n",
            "265 processing docs:  23353872\n",
            "**** ignore this doc 23353874 (TOO-BIG)\n",
            "267 processing docs:  23353878\n",
            "268 processing docs:  23353901\n",
            "269 processing docs:  23353904\n",
            "270 processing docs:  23353913\n",
            "271 processing docs:  23353931\n",
            "------------------------------  found NONE  ------------------------------\n",
            "272 processing docs:  23353944\n",
            "273 processing docs:  23353945\n",
            "**** ignore this doc 23353950 (TOO-BIG)\n",
            "275 processing docs:  23353954\n",
            "276 processing docs:  23353967\n",
            "277 processing docs:  23353973\n",
            "278 processing docs:  23353975\n",
            "**** ignore this doc 23353976 (TOO-BIG)\n",
            "280 processing docs:  23353995\n",
            "281 processing docs:  23353996\n",
            "282 processing docs:  23354010\n",
            "283 processing docs:  23354027\n",
            "284 processing docs:  23354028\n",
            "285 processing docs:  23354042\n",
            "286 processing docs:  23354045\n",
            "**** ignore this doc 23354055 (TOO-BIG)\n",
            "288 processing docs:  23354082\n",
            "289 processing docs:  23354085\n",
            "290 processing docs:  23354089\n",
            "291 processing docs:  23354103\n",
            "292 processing docs:  23354126\n",
            "293 processing docs:  23354130\n",
            "294 processing docs:  23354166\n",
            "295 processing docs:  23354202\n",
            "296 processing docs:  23354219\n",
            "297 processing docs:  23354244\n",
            "298 processing docs:  23354253\n",
            "------------------------------  found NONE  ------------------------------\n",
            "299 processing docs:  23354265\n",
            "**** ignore this doc 23354285 (TOO-BIG)\n",
            "301 processing docs:  23354288\n",
            "302 processing docs:  23354310\n",
            "303 processing docs:  23354318\n",
            "304 processing docs:  23354320\n",
            "305 processing docs:  23354336\n",
            "306 processing docs:  23354396\n",
            "307 processing docs:  23354400\n",
            "308 processing docs:  23354442\n",
            "309 processing docs:  23354450\n",
            "310 processing docs:  23354460\n",
            "311 processing docs:  23354474\n",
            "**** ignore this doc 23354516 (TOO-BIG)\n",
            "313 processing docs:  23354536\n",
            "314 processing docs:  23354538\n",
            "315 processing docs:  23354544\n",
            "316 processing docs:  23354545\n",
            "317 processing docs:  23354575\n",
            "318 processing docs:  23354619\n",
            "319 processing docs:  23354627\n",
            "320 processing docs:  23354648\n",
            "321 processing docs:  23354656\n",
            "**** ignore this doc 23354695 (TOO-BIG)\n",
            "323 processing docs:  23354697\n",
            "324 processing docs:  23354698\n",
            "325 processing docs:  23354699\n",
            "326 processing docs:  23354717\n",
            "327 processing docs:  23354718\n",
            "328 processing docs:  23354738\n",
            "329 processing docs:  23354739\n",
            "330 processing docs:  23354751\n",
            "331 processing docs:  23354793\n",
            "**** ignore this doc 23354796 (TOO-BIG)\n",
            "333 processing docs:  23354803\n",
            "334 processing docs:  23354816\n",
            "335 processing docs:  23354831\n",
            "336 processing docs:  23354879\n",
            "**** ignore this doc 23354880 (TOO-BIG)\n",
            "338 processing docs:  23354881\n",
            "339 processing docs:  23354910\n",
            "340 processing docs:  23354912\n",
            "341 processing docs:  23354916\n",
            "342 processing docs:  23354920\n",
            "343 processing docs:  23354935\n",
            "344 processing docs:  23354944\n",
            "345 processing docs:  23354946\n",
            "346 processing docs:  23354953\n",
            "347 processing docs:  23354956\n",
            "348 processing docs:  23354977\n",
            "349 processing docs:  23354982\n",
            "**** ignore this doc 23355001 (TOO-BIG)\n",
            "351 processing docs:  23355040\n",
            "352 processing docs:  23355061\n",
            "353 processing docs:  23355064\n",
            "354 processing docs:  23355095\n",
            "355 processing docs:  23355132\n",
            "**** ignore this doc 23355228 (TOO-BIG)\n",
            "357 processing docs:  23355250\n",
            "358 processing docs:  23355254\n",
            "**** ignore this doc 23355290 (TOO-BIG)\n",
            "360 processing docs:  23355416\n",
            "361 processing docs:  23355434\n",
            "362 processing docs:  23355470\n",
            "363 processing docs:  23355557\n",
            "Bộ GD-ĐT: label{'label_ent_count': 5, 'word': 'Bộ GD-ĐT'}, but real is 3\n",
            "364 processing docs:  23355571\n",
            "365 processing docs:  23355626\n",
            "366 processing docs:  23355656\n",
            "367 processing docs:  23355773\n",
            "368 processing docs:  23355817\n",
            "369 processing docs:  23355858\n",
            "370 processing docs:  23355917\n",
            "371 processing docs:  23355935\n",
            "372 processing docs:  23355988\n",
            "373 processing docs:  23356093\n",
            "374 processing docs:  23356193\n",
            "375 processing docs:  23356205\n",
            "376 processing docs:  23356221\n",
            "377 processing docs:  23356245\n",
            "378 processing docs:  23356247\n",
            "379 processing docs:  23356295\n",
            "380 processing docs:  23356299\n",
            "381 processing docs:  23356314\n",
            "382 processing docs:  23356315\n",
            "383 processing docs:  23356329\n",
            "384 processing docs:  23356339\n",
            "385 processing docs:  23356494\n",
            "386 processing docs:  23356511\n",
            "387 processing docs:  23356604\n",
            "388 processing docs:  23356624\n",
            "389 processing docs:  23356638\n",
            "**** ignore this doc 23356715 (TOO-BIG)\n",
            "391 processing docs:  23356716\n",
            "392 processing docs:  23356724\n",
            "393 processing docs:  23356731\n",
            "394 processing docs:  23356745\n",
            "395 processing docs:  23356765\n",
            "396 processing docs:  23356767\n",
            "397 processing docs:  23356771\n",
            "398 processing docs:  23356782\n",
            "399 processing docs:  23356798\n",
            "**** ignore this doc 23356858 (TOO-BIG)\n",
            "401 processing docs:  23356874\n",
            "402 processing docs:  23356885\n",
            "403 processing docs:  23356887\n",
            "404 processing docs:  23356902\n",
            "**** ignore this doc 23356907 (TOO-BIG)\n",
            "406 processing docs:  23356915\n",
            "407 processing docs:  23356918\n",
            "408 processing docs:  23356933\n",
            "409 processing docs:  23356960\n",
            "410 processing docs:  23356961\n",
            "411 processing docs:  23356992\n",
            "412 processing docs:  23356993\n",
            "413 processing docs:  23357028\n",
            "414 processing docs:  23357037\n",
            "415 processing docs:  23357062\n",
            "416 processing docs:  23357081\n",
            "417 processing docs:  23357094\n",
            "418 processing docs:  23357095\n",
            "419 processing docs:  23357097\n",
            "420 processing docs:  23357120\n",
            "421 processing docs:  23357135\n",
            "422 processing docs:  23357151\n",
            "423 processing docs:  23357167\n",
            "424 processing docs:  23357190\n",
            "425 processing docs:  23357240\n",
            "426 processing docs:  23357258\n",
            "427 processing docs:  23357263\n",
            "428 processing docs:  23357266\n",
            "429 processing docs:  23357288\n",
            "430 processing docs:  23357308\n",
            "431 processing docs:  23357309\n",
            "432 processing docs:  23357329\n",
            "433 processing docs:  23357336\n",
            "434 processing docs:  23357341\n",
            "435 processing docs:  23357344\n",
            "436 processing docs:  23357389\n",
            "437 processing docs:  23357394\n",
            "438 processing docs:  23357396\n",
            "439 processing docs:  23357443\n",
            "440 processing docs:  23357457\n",
            "441 processing docs:  23357471\n",
            "442 processing docs:  23357489\n",
            "443 processing docs:  23357534\n",
            "444 processing docs:  23357544\n",
            "445 processing docs:  23357550\n",
            "446 processing docs:  23357652\n",
            "447 processing docs:  23357711\n",
            "448 processing docs:  23357741\n",
            "449 processing docs:  23357752\n",
            "**** ignore this doc 23357765 (TOO-BIG)\n",
            "451 processing docs:  23357809\n",
            "452 processing docs:  23357851\n",
            "453 processing docs:  23357937\n",
            "454 processing docs:  23357994\n",
            "455 processing docs:  23358011\n",
            "456 processing docs:  23358097\n",
            "457 processing docs:  23358104\n",
            "458 processing docs:  23358261\n",
            "459 processing docs:  23366716\n",
            "460 processing docs:  23366722\n",
            "461 processing docs:  23366740\n",
            "462 processing docs:  23366751\n",
            "463 processing docs:  23366765\n",
            "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
            "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
            "ignore_docs_list =  []\n",
            "accept_doc_NOT_big =  ['23352702', '23353954', '23351846', '23353872', '23352701', '23353849', '23352769', '23353913', '23353878', '23354045', '23352748', '23351612', '23353867', '23351749', '23352718', '23351778', '23352804', '23351887', '23354126', '23351571', '23351933', '23352824', '23352743', '23352792', '23352887', '23351436', '23353931', '23354816', '23353785', '23352750', '23352746', '23352659', '23351851', '23354320', '23351645', '23356295', '23352751', '23352765', '23351566', '23351987', '23351556', '23351424', '23351947', '23354619', '23351931', '23351856', '23355557', '23351969', '23356245', '23351394', '23351564', '23354474', '23351839', '23352849', '23351632', '23357752', '23351970', '23352878', '23351672', '23354627', '23356961', '23352677', '23351316', '23357652', '23354803', '23357308', '23354935', '23351995', '23351427', '23353901', '23352684', '23351434', '23351888', '23355773', '23351982', '23354460', '23356798', '23352706', '23352671', '23354400', '23351518', '23352825', '23351864', '23353864', '23351524', '23352739', '23354881', '23351567', '23352853', '23351554', '23351647', '23352693', '23352695', '23354318', '23352807', '23352752', '23354253', '23352883', '23351834', '23353973', '23354082', '23357489', '23351578', '23351636', '23354538', '23351164', '23352662', '23354699', '23351416', '23352696', '23354219', '23356329', '23351988', '23351952', '23354698', '23351607', '23351939', '23356918', '23351848', '23351516', '23356887', '23354656', '23351983', '23356247', '23351979', '23352675', '23353975', '23357240', '23351649', '23354739', '23354910', '23352730', '23354130', '23351959', '23354396', '23351493', '23354982', '23351190', '23351858', '23357028', '23351965', '23354956', '23351514', '23352822', '23356771', '23354697', '23355061', '23351579', '23351956', '23356902', '23357809', '23351981', '23353904', '23354879', '23351494', '23352774', '23351994', '23352715', '23357190', '23351391', '23353779', '23356511', '23351519', '23352787', '23351422', '23352894', '23351563', '23352725', '23354648', '23353846', '23356885', '23357095', '23353967', '23354751', '23353760', '23351260', '23352845', '23354575', '23353786', '23355817', '23358011', '23351582', '23352719', '23353861', '23354085', '23354920', '23351522', '23351651', '23351652', '23353721', '23354336', '23351521', '23356874', '23352687', '23352755', '23353787', '23356915', '23351425', '23353860', '23355917', '23355434', '23352656', '23352830', '23353727', '23355571', '23357120', '23352874', '23354265', '23354953', '23351562', '23351642', '23351978', '23353857', '23353863', '23352800', '23351719', '23352663', '23352802', '23353830', '23354103', '23355040', '23357094', '23351437', '23351963', '23354166', '23357151', '23351814', '23351817', '23351841', '23352686', '23352731', '23351113', '23351615', '23356624', '23356724', '23355656', '23352896', '23352899', '23353780', '23356093', '23356494', '23357329', '23355064', '23356315', '23351435', '23351511', '23352757', '23351440', '23352872', '23357389', '23351432', '23351650', '23357344', '23366765', '23351214', '23351510', '23353791', '23355935', '23357266', '23366740', '23351431', '23351861', '23351948', '23352681', '23352829', '23354916', '23357443', '23352778', '23352820', '23352892', '23355416', '23351923', '23352710', '23352880', '23351576', '23352676', '23353944', '23354738', '23355095', '23357037', '23353838', '23353842', '23355132', '23357167', '23366716', '23351569', '23351653', '23351974', '23352876', '23352886', '23355470', '23356638', '23357336', '23357741', '23358261', '23351318', '23351555', '23351561', '23351581', '23351627', '23351831', '23351961', '23352785', '23352844', '23354288', '23351542', '23351574', '23355626', '23356221', '23357097', '23351950', '23351967', '23352831', '23353801', '23354310', '23354544', '23354977', '23355254', '23357081', '23351460', '23351820', '23352806', '23352821', '23353834', '23354202', '23355858', '23356193', '23357457', '23357534', '23358097', '23351849', '23351853', '23351971', '23352682', '23353996', '23354450', '23354944', '23354946', '23356745', '23357396', '23357471', '23366751', '23351385', '23351489', '23351809', '23352704', '23352795', '23353773', '23353794', '23353825', '23353995', '23354027', '23354718', '23354793', '23354831', '23355988', '23356205', '23356604', '23356765', '23356767', '23356782', '23356960', '23356992', '23356993', '23357394', '23357851', '23358104', '23351225', '23351392', '23351393', '23351438', '23351543', '23351595', '23351617', '23351852', '23351941', '23351949', '23352674', '23352708', '23352713', '23352717', '23352720', '23352754', '23352870', '23352871', '23352900', '23353723', '23353732', '23353755', '23353799', '23353841', '23353945', '23354010', '23354028', '23354042', '23354089', '23354244', '23354442', '23354536', '23354545', '23354717', '23354912', '23355250', '23356299', '23356314', '23356339', '23356716', '23356731', '23356933', '23357062', '23357135', '23357258', '23357263', '23357288', '23357309', '23357341', '23357544', '23357550', '23357711', '23357937', '23357994', '23366722']\n",
            "error_doc_list_trigger_except_print =  []\n",
            "error_doc_list_has_name_print =  []\n",
            ">>> error_doc_list_has_name_print_BYPASS_ls >>>\n",
            "[]\n",
            ">>> LEN error_doc_list_trigger_except:  0 LEN error_doc_list_has_name:  0\n",
            "valid_files_count / accept_doc_NOT_big 422/422 ~ 100.0%\n",
            "accept_doc_NOT_big / all_docs: 422/464 ~ 90.95%\n",
            "final_processed_file / VLSP_TRAIN_raw: 422/505 ~ 83.56%\n",
            "processed_code 422  {'23353846', '23354545', '23354288', '23357550', '23351987', '23351950', '23352822', '23355254', '23358011', '23354450', '23351861', '23352820', '23354028', '23351961', '23353975', '23351438', '23351636', '23356245', '23351564', '23351858', '23354219', '23357809', '23351542', '23353780', '23357151', '23354831', '23354803', '23351581', '23351422', '23352806', '23351971', '23351214', '23351841', '23352871', '23353838', '23352656', '23351852', '23357937', '23353945', '23352701', '23352755', '23352702', '23351434', '23352892', '23354648', '23351494', '23352883', '23352829', '23354656', '23352824', '23352718', '23357309', '23352757', '23352676', '23351555', '23351846', '23355817', '23354082', '23357266', '23354953', '23351514', '23354575', '23351851', '23351566', '23352825', '23355470', '23351988', '23353785', '23355095', '23355988', '23366722', '23356874', '23354881', '23356339', '23352743', '23351647', '23352778', '23354910', '23351642', '23352785', '23357389', '23357489', '23351579', '23354536', '23355064', '23351864', '23355626', '23353834', '23353944', '23354956', '23351394', '23351849', '23352739', '23355250', '23353830', '23352802', '23351427', '23352681', '23352876', '23357190', '23352774', '23351979', '23354793', '23353901', '23353931', '23356961', '23357120', '23356745', '23357341', '23357288', '23353842', '23354265', '23354460', '23352896', '23351556', '23352659', '23351749', '23352878', '23353794', '23355061', '23353863', '23354619', '23357095', '23353995', '23356511', '23357094', '23352715', '23351974', '23351385', '23357097', '23351567', '23354318', '23352821', '23356902', '23351425', '23351982', '23351814', '23352730', '23356765', '23353904', '23351460', '23351543', '23352754', '23354166', '23357443', '23351967', '23355132', '23351554', '23357329', '23351834', '23352804', '23352731', '23351981', '23351516', '23352752', '23351489', '23355557', '23356295', '23355656', '23352751', '23351391', '23352708', '23356933', '23355773', '23351190', '23352686', '23352830', '23351947', '23351436', '23351576', '23352719', '23356716', '23352807', '23354538', '23353755', '23357336', '23352725', '23351652', '23352831', '23366751', '23356992', '23351965', '23354879', '23354244', '23357544', '23353841', '23352748', '23352717', '23366716', '23352696', '23351435', '23351316', '23352710', '23353801', '23354751', '23354400', '23352674', '23353786', '23353973', '23351994', '23352706', '23351574', '23354320', '23351578', '23351817', '23351848', '23351649', '23351949', '23352720', '23358104', '23354544', '23354697', '23355571', '23353864', '23354982', '23351933', '23351853', '23356299', '23351978', '23356624', '23351437', '23352880', '23352693', '23354310', '23353732', '23356247', '23357135', '23354130', '23354699', '23352662', '23357167', '23353773', '23352900', '23351627', '23352853', '23354944', '23352845', '23351653', '23357258', '23356771', '23357851', '23355434', '23351820', '23351607', '23356993', '23352795', '23353878', '23356315', '23353721', '23351983', '23356885', '23354698', '23352682', '23356915', '23351839', '23351524', '23353849', '23352894', '23354396', '23351948', '23352872', '23352675', '23352695', '23351113', '23356918', '23355040', '23351970', '23354916', '23351431', '23353913', '23356960', '23356093', '23351510', '23355858', '23351651', '23351521', '23357994', '23351931', '23366740', '23351959', '23357062', '23351969', '23357394', '23351416', '23354202', '23352684', '23351392', '23354912', '23352769', '23357240', '23355935', '23352671', '23357741', '23353996', '23351963', '23354103', '23353872', '23354010', '23354336', '23352844', '23357037', '23354920', '23353825', '23351888', '23358097', '23351615', '23358261', '23354627', '23353860', '23354977', '23351952', '23352887', '23352746', '23357263', '23357308', '23352765', '23354717', '23351432', '23351595', '23351563', '23351562', '23351809', '23354718', '23351582', '23354042', '23351318', '23351440', '23351831', '23351522', '23351778', '23351569', '23357028', '23357396', '23366765', '23352750', '23353723', '23356193', '23351887', '23352787', '23354085', '23351645', '23356314', '23352713', '23354935', '23352849', '23353787', '23352899', '23353779', '23354126', '23356329', '23352792', '23354946', '23352874', '23356604', '23352800', '23353791', '23353867', '23351493', '23354474', '23352870', '23351923', '23353861', '23353857', '23353799', '23351424', '23352886', '23351225', '23356782', '23351941', '23351650', '23354738', '23356724', '23354816', '23352704', '23351519', '23351956', '23352687', '23354089', '23353967', '23351856', '23357534', '23351719', '23351511', '23351939', '23357652', '23351612', '23353954', '23356638', '23351571', '23351672', '23351393', '23356767', '23357081', '23355416', '23351518', '23351617', '23357711', '23352663', '23354442', '23354739', '23353760', '23351632', '23351995', '23352677', '23351164', '23354253', '23357344', '23356494', '23356731', '23351561', '23351260', '23354027', '23356221', '23356205', '23357457', '23356887', '23357471', '23353727', '23354045', '23355917', '23356798', '23357752'}\n"
          ]
        }
      ],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "processed_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/vlsp.data\"\n",
        "\n",
        "processed_error_files_OUT_PATH = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/processed_error_files.data\"\n",
        "\n",
        "clear_file([processed_OUT_PATH])\n",
        "clear_file([processed_error_files_OUT_PATH])\n",
        "\n",
        "classify_pairs = [  'PERSON-LOCATION',      'ORGANIZATION-LOCATION', \n",
        "                    'LOCATION-LOCATION',    'ORGANIZATION-ORGANIZATION', \n",
        "                    'PERSON-PERSON', \n",
        "                    'PERSON-ORGANIZATION',  'LOCATION-ORGANIZATION']\n",
        "\n",
        "\n",
        "\n",
        "def read_file(special_split_sent_not_final_IN_path, processed_OUT_PATH):\n",
        "    n = 0\n",
        "    with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "        exit_with_err = \"\"\n",
        "        total_ent_a_doc = 0\n",
        "        current_code = None\n",
        "        prev_code = None\n",
        "        passage = \"\"\n",
        "        rel_dict = {}\n",
        "        ent_dict = {}\n",
        "        ent_dict_count_by_label = {}\n",
        "        passage_sents_ls = []\n",
        "        range_sent = []\n",
        "        for org_docs_line in _org_docs_file:\n",
        "            if exit_with_err:\n",
        "                print(\"exit_with_err>> \", exit_with_err)\n",
        "                # break\n",
        "                return exit_with_err\n",
        "            # if n > 12: break\n",
        "            # n+=1\n",
        "            # print(f\"{n}.>>> {org_docs_line[:-1]}\")\n",
        "            real_count_NOT_equal_len_ent = False\n",
        "            # if lentorg_docs_line == \"<EOF>\":\n",
        "            if len(org_docs_line) < 10:\n",
        "                # print(\">> len(org_docs_line) == 1:\", current_code)\n",
        "                \"\"\"\n",
        "                    read all line \n",
        "                    now, handling to create processED data & reset object in here\n",
        "                \"\"\"\n",
        "                full_data_to_processed = \\\n",
        "                    {\n",
        "                        \"exit_with_err\": exit_with_err,\n",
        "                        \"total_ent_a_doc\": total_ent_a_doc,\n",
        "                        \"current_code\": current_code,\n",
        "                        \"prev_code\": prev_code,\n",
        "                        \"passage\": passage,\n",
        "                        \"rel_dict\": rel_dict,\n",
        "                        \"ent_dict\": ent_dict,\n",
        "                        \"passage_sents_ls\": passage_sents_ls,\n",
        "                        \"range_sent\": range_sent,\n",
        "                        \"ent_dict_count_by_label\": ent_dict_count_by_label\n",
        "\n",
        "                    }\n",
        "\n",
        "                return_processed_text = \\\n",
        "                    proccess_full_prepare_line_to_processed_text(full_data_to_processed)\n",
        "                if type(return_processed_text) == 'str' or len(return_processed_text) != 2:\n",
        "                    \"\"\" error \"\"\"\n",
        "                    print(\"er \"*40)\n",
        "                    print(\"return_processed_text \", return_processed_text)\n",
        "                    return return_processed_text\n",
        "                processes_rel_pair_line_list, processes_ent_line_dict = return_processed_text\n",
        "\n",
        "                nr_pairs_ls = get_NR_pair(ent_dict, rel_dict)\n",
        "                list_NR_processed_completed_text_ls = process_NR_pair_to_text(nr_pairs_ls, processes_ent_line_dict)\n",
        "                processes_rel_pair_line_list += list_NR_processed_completed_text_ls\n",
        "                processes_rel_pair_line_list_line = \"\\t\".join(processes_rel_pair_line_list)\n",
        "                full_processed_line = f\"{current_code}\\t{passage[:-1]}\\t{processes_rel_pair_line_list_line}\"\n",
        "                write_append_data_to_txt_file(processed_OUT_PATH, full_processed_line)\n",
        "                # print(\"end / \"*10)\n",
        "                return \"no-err\"\n",
        "\n",
        "            code = org_docs_line[0:8]  \n",
        "            current_code = code  \n",
        "            word_after_code =  org_docs_line[8:11]\n",
        "            # org_docs_line = org_docs_line.replace(\"\\xc2\\xa0\", \" \")\n",
        "            # org_docs_line = org_docs_line.replace(\"\\xa0\", \" \")\n",
        "            # org_docs_line = replace2symbol(org_docs_line)\n",
        "            org_docs_line = org_docs_line.replace(\"    \", \" \")\n",
        "            org_docs_line = org_docs_line.replace(\"   \", \" \")\n",
        "            org_docs_line = org_docs_line.replace(\"  \", \" \")\n",
        "\n",
        "            \n",
        "            if word_after_code == '|t|':\n",
        "                pass\n",
        "            elif word_after_code == '|a|':\n",
        "                passage = org_docs_line[11:]\n",
        "                sents_ls = passage.split(\".|\")\n",
        "                passage_sents_ls = (\" \".join(sents_ls)).split(\" \")\n",
        "                range_sent = create_range_sentence_list(sents_ls)\n",
        "            else:\n",
        "                line_ls_tab_split = org_docs_line.split('\\t')                \n",
        "                \"\"\"\n",
        "                    process to add entity to list entity\n",
        "                \"\"\"\n",
        "                if len(line_ls_tab_split) > 4 and  line_ls_tab_split[4] in \\\n",
        "                        [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]:\n",
        "                    \n",
        "\n",
        "                    ent_code = line_ls_tab_split[5][:-1]\n",
        "                    if ent_dict_count_by_label.get(ent_code, None) == None:\n",
        "                        ent_dict_count_by_label[ent_code] = {}\n",
        "                        ent_dict_count_by_label[ent_code][\"label_ent_count\"] = 0                    \n",
        "                        ent_dict_count_by_label[ent_code][\"word\"] = line_ls_tab_split[3]\n",
        "                    else:\n",
        "                        ent_dict_count_by_label[ent_code][\"label_ent_count\"] +=1\n",
        "\n",
        "                    full_data_to_processed = \\\n",
        "                        {   \n",
        "                            \"exit_with_err\": exit_with_err,\n",
        "                            \"total_ent_a_doc\": total_ent_a_doc,\n",
        "                            \"current_code\": current_code,\n",
        "                            \"prev_code\": prev_code,\n",
        "                            \"passage\": passage,\n",
        "                            \"rel_dict\": rel_dict,\n",
        "                            \"ent_dict\": ent_dict,\n",
        "                            \"passage_sents_ls\": passage_sents_ls,\n",
        "                            \"range_sent\": range_sent,\n",
        "                            \"line_ls_tab_split\": line_ls_tab_split,\n",
        "                            # \"ent_dict_count_by_label\": ent_dict_count_by_label\n",
        "                        }\n",
        "                    full_data_to_processed = process_entity(full_data_to_processed)\n",
        "                    if full_data_to_processed == \"entity not in the passage\" or type(full_data_to_processed) == 'str':\n",
        "                        \"\"\" error \"\"\"\n",
        "                        print(\"[ent error]\",\"e \"*30)\n",
        "                        print(\"Error process entity \", full_data_to_processed)\n",
        "                        return full_data_to_processed\n",
        "                    # print(\"? \"*40)\n",
        "                    # print(\"? \"*40)\n",
        "                    # print(full_data_to_processed)\n",
        "                    exit_with_err =  full_data_to_processed[\"exit_with_err\"]\n",
        "                    total_ent_a_doc =  full_data_to_processed[\"total_ent_a_doc\"]\n",
        "                    current_code =  full_data_to_processed[\"current_code\"]\n",
        "                    prev_code =  full_data_to_processed[\"prev_code\"]\n",
        "                    passage =  full_data_to_processed[\"passage\"]\n",
        "                    rel_dict =  full_data_to_processed[\"rel_dict\"]\n",
        "                    ent_dict =  full_data_to_processed[\"ent_dict\"]\n",
        "                    passage_sents_ls =  full_data_to_processed[\"passage_sents_ls\"]\n",
        "                    range_sent =  full_data_to_processed[\"range_sent\"]\n",
        "                    # ent_dict_count_by_label = full_data_to_processed[\"ent_dict_count_by_label\"]\n",
        "                                                \n",
        "                else:\n",
        "                    if len(line_ls_tab_split) != 4:\n",
        "                        print(\"line_ls_tab_split \", len(line_ls_tab_split))\n",
        "                        print(\"line_ls_tab_split \", line_ls_tab_split)\n",
        "                        exit_with_err = \"entity line is invalid\"\n",
        "                        return exit_with_err\n",
        "\n",
        "                    if line_ls_tab_split[1] not in \\\n",
        "                        [\"AFFILIATION\", \"LOCATED\", \"PART - WHOLE\", \"PERSONAL - SOCIAL\"]:\n",
        "                        exit_with_err = \"entity type name is wrong\"\n",
        "                        return exit_with_err\n",
        "                    # print(\"line_ls_tab_split \", line_ls_tab_split)\n",
        "                    if line_ls_tab_split[2] in ['None', \"None\\n\"] or line_ls_tab_split[3] in ['None', \"None\\n\"]:\n",
        "                        print(\"-\"*30, \" found NONE \",\"-\"*30)\n",
        "                        continue\n",
        "\n",
        "\n",
        "                    if rel_dict.get(line_ls_tab_split[1], None) == None:             \n",
        "                        rel_dict[line_ls_tab_split[1]] = [(line_ls_tab_split[2], line_ls_tab_split[3][:-1])]\n",
        "                    else:\n",
        "                        rel_dict[line_ls_tab_split[1]].append( (line_ls_tab_split[2], line_ls_tab_split[3][:-1]) )\n",
        "                \n",
        "    return \"no-err\"\n",
        "\n",
        "train_list = ['23351113', '23351164', '23351190', '23351214', '23351225', '23351260', '23351307', '23351316', '23351318', '23351385', '23351391', '23351392', '23351393', '23351394', '23351416', '23351422', '23351424', '23351425', '23351426', '23351427', '23351430', '23351431', '23351432', '23351433', '23351434', '23351435', '23351436', '23351437', '23351438', '23351440', '23351460', '23351489', '23351493', '23351494', '23351510', '23351511', '23351514', '23351515', '23351516', '23351518', '23351519', '23351521', '23351522', '23351524', '23351542', '23351543', '23351549', '23351554', '23351555', '23351556', '23351561', '23351562', '23351563', '23351564', '23351566', '23351567', '23351569', '23351571', '23351574', '23351576', '23351578', '23351579', '23351581', '23351582', '23351595', '23351607', '23351610', '23351611', '23351612', '23351615', '23351617', '23351627', '23351632', '23351635', '23351636', '23351642', '23351645', '23351647', '23351649', '23351650', '23351651', '23351652', '23351653', '23351672', '23351700', '23351719', '23351749', '23351778', '23351809', '23351814', '23351815', '23351817', '23351820', '23351831', '23351834', '23351837', '23351839', '23351841', '23351846', '23351848', '23351849', '23351851', '23351852', '23351853', '23351856', '23351858', '23351861', '23351864', '23351887', '23351888', '23351923', '23351931', '23351933', '23351937', '23351939', '23351941', '23351943', '23351945', '23351946', '23351947', '23351948', '23351949', '23351950', '23351951', '23351952', '23351956', '23351959', '23351960', '23351961', '23351963', '23351965', '23351967', '23351969', '23351970', '23351971', '23351974', '23351976', '23351978', '23351979', '23351981', '23351982', '23351983', '23351984', '23351985', '23351987', '23351988', '23351990', '23351992', '23351994', '23351995', '23352656', '23352659', '23352662', '23352663', '23352665', '23352671', '23352674', '23352675', '23352676', '23352677', '23352681', '23352682', '23352683', '23352684', '23352686', '23352687', '23352690', '23352693', '23352695', '23352696', '23352701', '23352702', '23352704', '23352706', '23352707', '23352708', '23352710', '23352713', '23352715', '23352717', '23352718', '23352719', '23352720', '23352725', '23352730', '23352731', '23352738', '23352739', '23352743', '23352746', '23352747', '23352748', '23352750', '23352751', '23352752', '23352753', '23352754', '23352755', '23352757', '23352761', '23352765', '23352769', '23352774', '23352777', '23352778', '23352781', '23352785', '23352787', '23352792', '23352795', '23352800', '23352802', '23352804', '23352806', '23352807', '23352814', '23352816', '23352820', '23352821', '23352822', '23352824', '23352825', '23352829', '23352830', '23352831', '23352844', '23352845', '23352849', '23352853', '23352856', '23352857', '23352870', '23352871', '23352872', '23352874', '23352876', '23352878', '23352880', '23352883', '23352886', '23352887', '23352892', '23352894', '23352896', '23352899', '23352900', '23353721', '23353723', '23353727', '23353732', '23353739', '23353755', '23353757', '23353760', '23353763', '23353773', '23353779', '23353780', '23353785', '23353786', '23353787', '23353791', '23353794', '23353799', '23353801', '23353824', '23353825', '23353830', '23353834', '23353838', '23353840', '23353841', '23353842', '23353846', '23353849', '23353857', '23353860', '23353861', '23353863', '23353864', '23353867', '23353872', '23353874', '23353878', '23353891', '23353901', '23353904', '23353913', '23353916', '23353931', '23353944', '23353945', '23353950', '23353954', '23353967', '23353973', '23353975', '23353976', '23353995', '23353996', '23354010', '23354027', '23354028', '23354030', '23354032', '23354034', '23354042', '23354045', '23354055', '23354065', '23354082', '23354085', '23354088', '23354089', '23354091', '23354092', '23354093', '23354098', '23354103', '23354126', '23354130', '23354166', '23354202', '23354219', '23354244', '23354253', '23354265', '23354285', '23354288', '23354310', '23354318', '23354320', '23354336', '23354396', '23354400', '23354442', '23354450', '23354460', '23354474', '23354516', '23354536', '23354538', '23354544', '23354545', '23354575', '23354619', '23354627', '23354648', '23354656', '23354695', '23354697', '23354698', '23354699', '23354717', '23354718', '23354719', '23354738', '23354739', '23354751', '23354780', '23354793', '23354796', '23354803', '23354816', '23354831', '23354879', '23354880', '23354881', '23354910', '23354912', '23354916', '23354920', '23354935', '23354944', '23354946', '23354953', '23354956', '23354977', '23354982', '23355001', '23355040', '23355061', '23355064', '23355095', '23355132', '23355228', '23355250', '23355254', '23355290', '23355416', '23355434', '23355470', '23355557', '23355571', '23355626', '23355656', '23355773', '23355817', '23355858', '23355917', '23355935', '23355988', '23356093', '23356193', '23356205', '23356221', '23356245', '23356247', '23356295', '23356299', '23356314', '23356315', '23356329', '23356339', '23356494', '23356505', '23356511', '23356574', '23356604', '23356622', '23356624', '23356638', '23356715', '23356716', '23356724', '23356731', '23356745', '23356765', '23356767', '23356771', '23356782', '23356793', '23356798', '23356858', '23356874', '23356885', '23356887', '23356902', '23356906', '23356907', '23356915', '23356918', '23356933', '23356960', '23356961', '23356992', '23356993', '23356998', '23357000', '23357028', '23357037', '23357062', '23357063', '23357081', '23357094', '23357095', '23357097', '23357120', '23357135', '23357151', '23357167', '23357190', '23357233', '23357240', '23357258', '23357263', '23357266', '23357288', '23357308', '23357309', '23357329', '23357336', '23357341', '23357344', '23357389', '23357394', '23357396', '23357443', '23357457', '23357471', '23357489', '23357491', '23357534', '23357544', '23357550', '23357652', '23357711', '23357741', '23357752', '23357765', '23357779', '23357809', '23357851', '23357897', '23357937', '23357994', '23358011', '23358086', '23358097', '23358104', '23358261', '23366716', '23366722', '23366740', '23366751', '23366765']\n",
        "dev_list = ['23351996', '23351997', '23351998', '23352000', '23352001', '23352002', '23352003', '23352006', '23352009', '23352013', '23352014', '23352016', '23352018', '23352019', '23352020', '23352021', '23352024', '23352026', '23352027', '23352028', '23352030', '23352033', '23352065', '23352066', '23352070', '23352071', '23352073', '23352075', '23352078', '23352079', '23352081', '23352084', '23352085', '23352087', '23352089', '23352090', '23352099', '23352107', '23352110', '23352117', '23352122', '23352125', '23352126', '23352143', '23352161', '23352163', '23352173', '23352190', '23352200', '23352232', '23352236', '23352239', '23352240', '23352260', '23352265', '23352283', '23352292', '23352296', '23352297', '23352298', '23352299', '23352300', '23352301', '23352303', '23352305', '23352309', '23352314', '23352316', '23352317', '23352319', '23352320', '23352321', '23352322', '23352323', '23352326', '23352327', '23352328', '23352329', '23352331', '23352332', '23352334', '23352335', '23352337', '23352343', '23352345', '23352346', '23352347', '23352348', '23352352', '23352357', '23352359', '23352361', '23352363', '23352366', '23352370', '23352372', '23352373', '23352376', '23352378', '23352381', '23352382', '23352385', '23352390', '23352393', '23352396', '23352397', '23352401', '23352402', '23352405', '23352408', '23352409', '23352410', '23352411', '23352412', '23352413', '23352414', '23352415', '23352416', '23352417', '23352419', '23352421', '23352425', '23352427', '23352428', '23352429', '23352432', '23352433', '23352434', '23352436', '23352437', '23352438', '23352440', '23352442', '23352443', '23352444', '23352445', '23352448', '23352449', '23352450', '23352451', '23352453', '23352456', '23352457', '23352460', '23352461', '23352462', '23352467', '23352468', '23352470', '23352471', '23352473', '23352476', '23352477', '23352480', '23352482', '23352486', '23352487', '23352488', '23352489', '23352491', '23352492', '23352494', '23352495', '23352497', '23352498', '23352499', '23352507', '23352508', '23352517', '23352518', '23352522', '23352523', '23352524', '23352526', '23352530', '23352532', '23352535', '23352536', '23352538', '23352539', '23352540', '23352543', '23352545', '23352546', '23352548', '23352550', '23352551', '23352552', '23352560', '23352561', '23352562', '23352563', '23352568', '23352571', '23352572', '23352573', '23352575', '23352577', '23352578', '23352580', '23352581', '23352583', '23352585', '23352586', '23352588', '23352589', '23352590', '23352591', '23352592', '23352593', '23352594', '23352596', '23352597', '23352600', '23352601', '23352602', '23352603', '23352605', '23352609', '23352610', '23352612', '23352614', '23352615', '23352617', '23352618', '23352619', '23352620', '23352623', '23352624', '23352625', '23352626', '23352628', '23352629', '23352631', '23352634', '23352635', '23352637', '23352640', '23352641', '23352642', '23352644', '23352648', '23352649', '23352650', '23352651', '23352653', '23352654', '23352733', '23352734', '23352736']\n",
        "\n",
        "\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "onlyfiles = sorted(onlyfiles)\n",
        "print(onlyfiles)\n",
        "\n",
        "\n",
        "no_error_count = 0\n",
        "error_count = 0\n",
        "all_docs = 0\n",
        "error_doc_list_has_name = []\n",
        "error_doc_list_trigger_except = []\n",
        "processed_code = set()\n",
        "# clear_file([f\"{OUT_error_not_found_ent_in_msg_FOLDER_path}/txt.txt\"])\n",
        "\n",
        "\n",
        "error_doc_list_has_name_print_BYPASS_ERR = False\n",
        "# error_doc_list_has_name_print_BYPASS_ls ONLY WHEN the except is empty\n",
        "error_doc_list_has_name_print_BYPASS_ls =[]\n",
        "error_doc_list_trigger_except_print =  []\n",
        "\n",
        "error_doc_list_has_name_print =  ['23352073', '23352601']\n",
        "\n",
        "error_doc_list_trigger_except =  []\n",
        "error_doc_list_has_name =  []\n",
        "ignore_docs_list_real_ignore = []\n",
        "for idx, doc_code in enumerate(onlyfiles):\n",
        "    all_docs+=1\n",
        "    if doc_code not in accept_doc_NOT_big:\n",
        "        ignore_docs_list_real_ignore.append(doc_code)\n",
        "        print(f\"**** ignore this doc {doc_code} (TOO-BIG)\")\n",
        "\n",
        "        continue\n",
        "\n",
        "    if doc_code not in train_list:\n",
        "        print(f\"!!!! ignore this doc {doc_code} (NOT-TRAIN CODE)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # if doc_code != '23352601':\n",
        "    #     continue\n",
        "\n",
        "\n",
        "\n",
        "    # error_doc_list_has_name =  ['23352568']\n",
        "\n",
        "    # error_doc_list_has_name = ['23352073']\n",
        "    # if not(doc_code in error_doc_list_has_name_print):\n",
        "    #     continue\n",
        "\n",
        "\n",
        "    ## Exception files ( not yet processed)\n",
        "    # if not(doc_code in error_doc_list_trigger_except_print):\n",
        "    #     print(f\"ignore: {doc_code}\")\n",
        "    #     continue\n",
        "\n",
        "\n",
        "    special_split_sent_not_final_IN_path = special_split_sent_not_final_IN_path_folder+'/'+ doc_code\n",
        "    print(idx, \"processing docs: \", doc_code)\n",
        "    # if idx > 1:\n",
        "    #     break\n",
        "    # try:\n",
        "    if 1:\n",
        "        val = read_file(special_split_sent_not_final_IN_path, processed_OUT_PATH)\n",
        "        if val == 'no-err':\n",
        "            # print(doc_code)\n",
        "            no_error_count+=1\n",
        "            processed_code.add(doc_code)\n",
        "        else:\n",
        "            error_count +=1\n",
        "            print(\"error type \", val)\n",
        "            error_doc_list_has_name.append(doc_code)\n",
        "            write_append_data_to_txt_file(processed_error_files_OUT_PATH, doc_code)\n",
        "    # except:\n",
        "    #     write_append_data_to_txt_file(processed_error_files_OUT_PATH, doc_code)\n",
        "    #     print(\"error: \", doc_code)\n",
        "    #     error_doc_list_trigger_except.append(doc_code)\n",
        "\n",
        "print('* '*50)\n",
        "print('* '*50)\n",
        "len_accept_doc_NOT_big = len(accept_doc_NOT_big)\n",
        "print(\"ignore_docs_list = \", ignore_docs_list)\n",
        "print(\"accept_doc_NOT_big = \", accept_doc_NOT_big)\n",
        "print(\"error_doc_list_trigger_except_print = \", error_doc_list_trigger_except)\n",
        "print(\"error_doc_list_has_name_print = \", error_doc_list_has_name)\n",
        "print(\">>> error_doc_list_has_name_print_BYPASS_ls >>>\") \n",
        "pprint(error_doc_list_has_name_print_BYPASS_ls)\n",
        "print(\">>> LEN error_doc_list_trigger_except: \", len(error_doc_list_trigger_except), \"LEN error_doc_list_has_name: \", len(error_doc_list_has_name))\n",
        "\n",
        "print(f\"valid_files_count / accept_doc_NOT_big {no_error_count}/{len_accept_doc_NOT_big} ~ {round(no_error_count*100/len_accept_doc_NOT_big,2)}%\")\n",
        "print(f\"accept_doc_NOT_big / all_docs: {len_accept_doc_NOT_big}/{all_docs} ~ {round(len_accept_doc_NOT_big*100/all_docs,2)}%\")\n",
        "final_processed_file = len_accept_doc_NOT_big-error_count\n",
        "print(f\"processed_code {len(processed_code)} \", processed_code)\n",
        "print(f\"final_processed_file / VLSP_TRAIN_raw: {final_processed_file}/505 ~ {round(final_processed_file*100/505,2)}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
