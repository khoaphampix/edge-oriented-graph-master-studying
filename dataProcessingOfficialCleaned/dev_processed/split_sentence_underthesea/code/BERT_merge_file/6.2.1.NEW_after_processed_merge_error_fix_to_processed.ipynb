{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "create_ent_sent_json_file = False\n",
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_files(file1_path, file2_path, output_path):\n",
        "    # Read the first file and store the first elements of each line\n",
        "    list_words = []\n",
        "    # with open(file1_path, 'r') as file1:\n",
        "    #     for line in file1:\n",
        "    #         words = line.split()\n",
        "    #         if len(words) > 0:\n",
        "    #             list_words.append(words[0])\n",
        "    \n",
        "    # Process the first file\n",
        "    with open(file1_path, 'r') as file1:\n",
        "        for line in file1:\n",
        "            words = line.split()\n",
        "            list_words.append(words[0])\n",
        "            # new_line = ' '.join(words[:-32])  # Remove the last 32 words\n",
        "            # new_line = line[:-33]  # Remove the last 32 words\n",
        "            new_line = line.replace(\"\\n\",\"\")\n",
        "            write_append_data_to_txt_file(output_path, new_line)\n",
        "            # output_file.write(new_line + '\\n')\n",
        "    \n",
        "    # Process the second file\n",
        "    with open(file2_path, 'r') as file2:\n",
        "        for line in file2:\n",
        "            words = line.split()\n",
        "            if words[0] not in list_words:\n",
        "                # print(words[0])\n",
        "                # output_file.write(line)\n",
        "                # new_line = line[:-33]  # Remove the last 32 words\n",
        "                new_line = line.replace(\"\\n\",\"\")\n",
        "                # output_file.write(new_line)\n",
        "                write_append_data_to_txt_file(output_path, new_line)\n",
        "\n",
        "\n",
        "\n",
        "file1_path = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/code/BERT_merge_file/data/PubMed-VLSP_origin_after_processed.txt\"\n",
        "file2_path = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/code/BERT_merge_file/data/PubMed-VLSP_origin_after_processed_ERROR.txt\"\n",
        "output_path = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/code/BERT_merge_file/data/PubMed-VLSP_origin_after_processed_error_merged.txt\"\n",
        "\n",
        "clear_file([output_path])\n",
        "process_files(file1_path, file2_path, output_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_file(input_file, output_file):\n",
        "    vectors = {}\n",
        "    \n",
        "    # Read the input file\n",
        "    with open(input_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    \n",
        "    # Process each line\n",
        "    for line in lines:\n",
        "        elements = line.strip().split()\n",
        "        key = elements[0]\n",
        "        values = list(map(float, elements[1:]))\n",
        "        \n",
        "        # Standardize the vector\n",
        "        max_value = max(values)\n",
        "        min_value = min(values)\n",
        "        values = [(x - min_value) / (max_value - min_value) * 2 - 1 for x in values]\n",
        "        \n",
        "        vectors[key] = values\n",
        "    \n",
        "    # Write the processed vectors to the output file\n",
        "    with open(output_file, 'w') as file:\n",
        "        for key, values in vectors.items():\n",
        "            file.write(key + ' ' + ' '.join(map(str, values)) + '\\n')\n",
        "\n",
        "# input_file = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/code/BERT_merge_file/data/PubMed-VLSP_origin.txt\"\n",
        "\n",
        "# output_file = \\\n",
        "# \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/code/BERT_merge_file/data/PubMed-VLSP.txt\"\n",
        "# process_file(input_file, output_file)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
