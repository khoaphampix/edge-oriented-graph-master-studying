{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from underthesea import sent_tokenize, word_tokenize\n",
        "\n",
        "PhoBERT_base_fairseq_path = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/edge-oriented-graph-master-WORKING/PhoBERT_base_fairseq\"\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "phoBERT = RobertaModel.from_pretrained(PhoBERT_base_fairseq_path, checkpoint_file='model.pt')\n",
        "phoBERT.eval()  # disable dropout (or leave in train mode to finetune\n",
        "import torch\n",
        "from fairseq.models.roberta import alignment_utils\n",
        "from fairseq.models import roberta \n",
        "from typing import Tuple, List\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from typing import List\n",
        "import torch\n",
        "\n",
        "def align_bpe_to_words(roberta, bpe_tokens: torch.LongTensor, other_tokens: List[str]):\n",
        "    \"\"\"\n",
        "    Helper to align GPT-2 BPE to other tokenization formats (e.g., spaCy).\n",
        "\n",
        "    Args:\n",
        "        roberta (RobertaHubInterface): RoBERTa instance\n",
        "        bpe_tokens (torch.LongTensor): GPT-2 BPE tokens of shape `(T_bpe)`\n",
        "        other_tokens (List[str]): other tokens of shape `(T_words)`\n",
        "\n",
        "    Returns:\n",
        "        List[str]: mapping from *other_tokens* to corresponding *bpe_tokens*.\n",
        "    \"\"\"\n",
        "    assert bpe_tokens.dim() == 1\n",
        "    assert bpe_tokens[0] == 0\n",
        "\n",
        "    def clean(text):\n",
        "        return text.strip()\n",
        "\n",
        "    # remove whitespaces to simplify alignment\n",
        "    bpe_tokens = [roberta.task.source_dictionary.string([x]) for x in bpe_tokens]\n",
        "    bpe_tokens = [\n",
        "        clean(roberta.bpe.decode(x) if x not in {\"<s>\", \"\"} else x) for x in bpe_tokens\n",
        "    ]\n",
        "    other_tokens = [clean(str(o)) for o in other_tokens]\n",
        "\n",
        "    # strip leading <s>\n",
        "    bpe_tokens = bpe_tokens[1:]\n",
        "    assert \"\".join(bpe_tokens) == \"\".join(other_tokens)\n",
        "\n",
        "    # create alignment from every word to a list of BPE tokens\n",
        "    alignment = []\n",
        "    bpe_toks = filter(lambda item: item[1] != \"\", enumerate(bpe_tokens, start=1))\n",
        "    j, bpe_tok = next(bpe_toks)\n",
        "\n",
        "    for other_tok in other_tokens:\n",
        "        # print(\"other_tok \", other_tok)\n",
        "        bpe_indices = []\n",
        "        while True:\n",
        "            if other_tok.startswith(bpe_tok):\n",
        "                bpe_indices.append(j)\n",
        "                other_tok = other_tok[len(bpe_tok) :]\n",
        "                try:\n",
        "                    j, bpe_tok = next(bpe_toks)\n",
        "                except StopIteration:\n",
        "                    j, bpe_tok = None, None\n",
        "            elif bpe_tok.startswith(other_tok):\n",
        "                # other_tok spans multiple BPE tokens\n",
        "                bpe_indices.append(j)\n",
        "                bpe_tok = bpe_tok[len(other_tok) :]\n",
        "                other_tok = \"\"\n",
        "            else:\n",
        "                raise Exception('Cannot align \"{}\" and \"{}\"'.format(other_tok, bpe_tok))\n",
        "            if other_tok == \"\":\n",
        "                break\n",
        "        assert len(bpe_indices) > 0\n",
        "        alignment.append(bpe_indices)\n",
        "    assert len(alignment) == len(other_tokens)\n",
        "\n",
        "    return alignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_aligned_roberta(roberta, sentence: str, \n",
        "                            tokens: List[str], \n",
        "                            return_all_hiddens=False):\n",
        "    ''' Code inspired from: \n",
        "       https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py\n",
        "    \n",
        "    Aligns roberta embeddings for an input tokenization of words for a sentence\n",
        "    \n",
        "    Inputs:\n",
        "    1. roberta: roberta fairseq class\n",
        "    2. sentence: sentence in string\n",
        "    3. tokens: tokens of the sentence in which the alignment is to be done\n",
        "    \n",
        "    Outputs: Aligned roberta features \n",
        "    '''\n",
        "\n",
        "    # tokenize both with GPT-2 BPE and get alignment with given tokens\n",
        "    \n",
        "    \n",
        "    # print(\"* \"*50)\n",
        "    # print(sentence)\n",
        "    # print(tokens)\n",
        "    \n",
        "    \n",
        "    bpe_toks = roberta.encode(sentence)\n",
        "    # alignment = alignment_utils.align_bpe_to_words(roberta, bpe_toks, tokens)\n",
        "    alignment = align_bpe_to_words(roberta, bpe_toks, tokens)\n",
        "    # extract features and align them\n",
        "    features = roberta.extract_features(bpe_toks, return_all_hiddens=return_all_hiddens)\n",
        "    features = features.squeeze(0)   #Batch-size = 1\n",
        "    aligned_feats = alignment_utils.align_features_to_words(roberta, features, alignment)\n",
        "    return aligned_feats[1:-1]  #exclude <s> and </s> tokens\n",
        "\n",
        "# Khởi tạo Byte Pair Encoding cho PhoBERT\n",
        "class BPE():\n",
        "  bpe_codes = PhoBERT_base_fairseq_path+'/bpe.codes'\n",
        "\n",
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def sentence_with_word_tokenize(sentence_org):\n",
        "    sentence = word_tokenize(sentence_org, format=\"text\")\n",
        "    # tokens = sentence.split(\" \")\n",
        "    # print(sentence)\n",
        "    # print(sentence_org)\n",
        "    # print(\"len(sentence) != len(sentence_org)\", len(sentence) != len(sentence_org))\n",
        "    if len(sentence) != len(sentence_org):\n",
        "        sentence_org_replace = sentence_org\n",
        "        lw = sentence.split(\" \")\n",
        "        lw = set(w for w in lw if \"_\" in w)\n",
        "        # print(lw)\n",
        "        for w in lw:\n",
        "            w_ = f\"{w}\"\n",
        "            w = w.replace(\"_\", \" \")\n",
        "            # print(w_, w)\n",
        "            sentence_org_replace = sentence_org_replace.replace(w, w_)\n",
        "        if len(sentence_org_replace) == len(sentence_org):\n",
        "            sentence = sentence_org_replace\n",
        "        else:\n",
        "            sentence = \"\"\n",
        "    # print(sentence)\n",
        "    return sentence\n",
        "\n",
        "def reduce_emb_vec_org(vt1):\n",
        "  \n",
        "  vt1_np = vt1.detach().numpy()\n",
        "  vt1_mean = vt1_np.reshape(-1, 4).mean(axis=1)\n",
        "  return np.hstack((vt1_mean, np.zeros(8))) # 200-192\n",
        "  # return vt1_mean\n",
        "\n",
        "def reduce_emb_vec(vt1):\n",
        "  vt1_np = vt1.detach().numpy()\n",
        "  vt1_mean = vt1_np.reshape(-1, 4).mean(axis=1)\n",
        "  return vt1_mean\n",
        "\n",
        "\n",
        "def embedding_words_in_sents_full_shape(sentence):\n",
        "  tokens = sentence.split(\" \")\n",
        "  # print(\"* \"*40)\n",
        "  # print(sentence)\n",
        "  # print(\"sentence \", sentence)\n",
        "  # print(\"tokens \", tokens)\n",
        "  # print(\"* \"*50)\n",
        "  # *the last sentence char is \"\", fx: \"Hello is this \"\n",
        "  if tokens and tokens[-1] == \"\":\n",
        "    tokens = tokens[:-1]\n",
        "  \n",
        "  w = extract_aligned_roberta(phoBERT, sentence, tokens,False)\n",
        "\n",
        "  emb_vec_reduce = [] \n",
        "  for tk, word in list(zip(tokens, w)):\n",
        "    emb_vec_reduce.append((tk, reduce_emb_vec(word)))\n",
        "  return emb_vec_reduce\n",
        "\n",
        "# def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "#     with open(full_path_to_file,'a') as out:\n",
        "#         out.write(f'{txt}\\n')\n",
        "#         # out.write(f'{txt}')s\n",
        "        \n",
        "# def clear_file(full_path_to_files_list):\n",
        "#     for _file in full_path_to_files_list:\n",
        "#       with open(_file,'w') as out:\n",
        "#         out.write(f'')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "create_ent_sent_json_file = False\n",
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')\n",
        "\n",
        "def split_sentence(sentence, max_words):\n",
        "    words = sentence.split()\n",
        "    sub_sentences = []\n",
        "    current_sub_sentence = []\n",
        "    \n",
        "    for word in words:\n",
        "        current_sub_sentence.append(word)\n",
        "        \n",
        "        if len(current_sub_sentence) == max_words:\n",
        "            sub_sentences.append(' '.join(current_sub_sentence))\n",
        "            current_sub_sentence = []\n",
        "    \n",
        "    # Add any remaining words as the last sub-sentence\n",
        "    if current_sub_sentence:\n",
        "        sub_sentences.append(' '.join(current_sub_sentence))\n",
        "    \n",
        "    return sub_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sent = \"bàn thắng Sang hiệp 2 , HLV Vũ_Hồng Việt yêu_cầu các học_trò tấn_công mạnh_mẽ hơn nữa , trong khi U16 Mông_Cổ có dấu_hiệu xuống sức rõ_rệt Những bàn thắng đến như một tất_yếu Riêng ở hiệp đấu này , Chí_Bảo đã ghi tới 4 bàn thắng , giúp U16 Việt_Nam thắng chung_cuộc 9 Như_vậy qua 2 trận đấu , U16 Việt_Nam toàn_thắng , ghi tới 14 bàn và để lọt_lưới 2 , hiệu_số bàn thắng_bại là 12 Tại bảng I lúc này , thầy_trò Vũ_Hồng Việt có cùng điểm_số như U16_Australia nhưng xếp dưới vì kém hiệu_số bàn thắng_bạ\"\n",
        "# eb = embedding_words_in_sents_full_shape(sent)\n",
        "\n",
        "# print(eb)\n",
        "already_embedded_set = set()\n",
        "embedded_file = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/code/BERT_merge_file/data/PubMed-VLSP_origin_after_processed_ERROR.txt\"\n",
        "clear_file([embedded_file])\n",
        "\n",
        "# error_sent_embedded_file = \\\n",
        "#     \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/code/BERT_merge_file/BERT_create_corpus/view_data/data/error_sent_embedded_file.txt\"\n",
        "# clear_file([error_sent_embedded_file])\n",
        "\n",
        "\n",
        "out_all_sent = \\\n",
        "    \"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/code/BERT_merge_file/BERT_create_corpus/view_data/data/error_sent_embedded_file.txt\"\n",
        "error_count = 0\n",
        "total_sent = 0\n",
        "with open(out_all_sent, 'r') as in_file:\n",
        "    lines = in_file.readlines()\n",
        "    for sent_ls in lines:\n",
        "        total_sent+=1\n",
        "        sent_ls = sent_ls.replace(\"\\n\",\"\").strip()\n",
        "        # sent = \"7 nguyên_nhân khiến Real_Madrid bị Barcelona bỏ_xa tại La_Liga Tờ Marca của Tây Ban_Nha đã chỉ ra những yếu_tố lý_giải cho sự khởi_đầu tệ_hại của Real_Madrid tại La_Liga 20172018\"\n",
        "        \n",
        "        split_sent_word = sent_ls.split(\" \")\n",
        "        if len(split_sent_word) > 200:\n",
        "            sent_ls = split_sentence(sent_ls, 200)\n",
        "        else:\n",
        "            sent_ls = [sent_ls]\n",
        "        for sent in sent_ls:\n",
        "            # try:\n",
        "            if 1:\n",
        "                worked = False\n",
        "                print(\"processing ....sent \", sent)\n",
        "                try:\n",
        "                    emb_vec_reduce =  embedding_words_in_sents_full_shape(sent)\n",
        "                except:\n",
        "                    pass\n",
        "                if not worked:\n",
        "                    try:\n",
        "                        sent = sent.replace(\" Yingluk \", \" \")\n",
        "                        emb_vec_reduce =  embedding_words_in_sents_full_shape(sent)\n",
        "                        worked = True\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                if not worked:\n",
        "                    try:\n",
        "                        sent = sent.replace(\" - \",\" \")              \n",
        "                        emb_vec_reduce =  embedding_words_in_sents_full_shape(sent)\n",
        "                        worked = True\n",
        "                    except:\n",
        "                        pass\n",
        "                \n",
        "                if not worked:\n",
        "                    try:\n",
        "                        sent = sent.replace(\"_\",\" \")              \n",
        "                        emb_vec_reduce =  embedding_words_in_sents_full_shape(sent)\n",
        "                        worked = True\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                # print(emb_vec_reduce)\n",
        "                for tk, em_w in emb_vec_reduce:\n",
        "                    if tk in already_embedded_set:\n",
        "                        continue\n",
        "                    already_embedded_set.add(tk)\n",
        "                    em_w_ls = em_w.tolist()\n",
        "                    em_w_ls = \" \".join(map(str, em_w_ls))      \n",
        "                    write_append_data_to_txt_file(embedded_file, f\"{tk} {em_w_ls}\")\n",
        "            # except:\n",
        "            #     error_count +=1\n",
        "                # write_append_data_to_txt_file(error_sent_embedded_file, sent)\n",
        "                print(sent, \"\\n\")\n",
        "            # break\n",
        "            if error_count == 1:\n",
        "                break\n",
        "\n",
        "print(\"already_embedded_set word: \", len(already_embedded_set))\n",
        "print(\"total_sent: \", total_sent)\n",
        "print(\"error_count sent: \", error_count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# s = \"Phạm tội thuộc một trong các trường hợp sau đây , thì bị phạt tù từ bảy năm đến mười lăm năm a Có tổ chức b Phạm tội nhiều lần c Lợi dụng chức vụ , quyền hạn d Lợi dụng danh nghĩa cơ quan , tổ chức đ Vận chuyển , mua bán qua biên giới e Sử dụng trẻ em vào việc phạm tội hoặc bán ma tuý cho trẻ em g Nhựa thuốc phiện , nhựa cần sa hoặc cao côca có trọng lượng từ năm trăm gam đến dưới một kilôgam h Hêrôin hoặc côcain có trọng lượng từ năm gam đến dưới ba mươi gam i Lá , hoa , quả cây cần sa hoặc lá cây côca có trọng lượng từ mười kilôgam đến dưới hai mươi lăm kilôgam k Quả thuốc phiện khô có trọng lượng từ năm mươi kilôgam đến dưới hai trăm kilôgam l Quả thuốc phiện tươi có trọng lượng từ mười kilôgam đến dưới năm mươi kilôgam m Các chất ma tuý khác ở thể rắn có trọng lượng từ hai mươi gam đến dưới một trăm gam n Các chất ma tuý khác ở thể lỏng từ một trăm mililít đến dưới hai trăm năm mươi mililít o Có từ hai chất ma tuý trở lên mà tổng số lượng của các chất đó tương đương với số lượng chất ma tuý quy định tại một trong các điểm từ điểm g đến điểm n khoản 2 Điều này p Tái phạm nguy hiểm\"\n",
        "\n",
        "# sent_ls = split_sentence(s, 255)\n",
        "# print(sent_ls)\n",
        "sent = \\\n",
        "    \"Các nhà điều tra bắt đầu thẩm vấn 3 nhân viên cảnh sát trong đó có viên sĩ quan cảnh sát nói trên vào tối 21 9 về việc bà Yingluk trốn thoát\"\n",
        "sent = sent.replace(\" Yingluk \", \" \")\n",
        "emb_vec_reduce =  embedding_words_in_sents_full_shape(sent)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
