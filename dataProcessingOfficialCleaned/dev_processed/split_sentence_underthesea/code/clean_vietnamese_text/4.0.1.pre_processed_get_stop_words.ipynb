{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import json\n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "output_file_sent = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/code/clean_vietnamese_text/prevent_stop_word_remove_entity/view_data/ent_split_space_doc.json\"\n",
        "ent_space_doct_dict = {}\n",
        "with open(output_file_sent, \"r\", encoding=\"utf-8\") as f:\n",
        "    ent_space_doct_dict = json.load(f)\n",
        "print(ent_space_doct_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from stop_words import get_stop_words\n",
        "stop_words = get_stop_words('vietnamese')\n",
        "\n",
        "def remove_stop_words(text, doc_code):\n",
        "    list_single_sent_workd = ent_space_doct_dict[doc_code]\n",
        "    print(list_single_sent_workd)\n",
        "    text_old = ' '.join([word for word in text.split() if (word.lower() not in stop_words) ])\n",
        "    print(text_old)\n",
        "    text = ' '.join([word for word in text.split() if (word.lower() not in stop_words or (word in list_single_sent_workd) ) ])\n",
        "    \n",
        "                \n",
        "    return text\n",
        "t = \\\n",
        "\"\"\"23352825|a|Uber bị tước giấy_phép ở London Ứng_dụng taxi chiếm 1/3 số xe thuê tư_nhân với hơn 40.000 tài_xế tại thủ_đô Anh , sẽ bị tước giấy_phép hoạt_động từ cuối tháng 9 này .|Ngày 22-9, Sở Giao_thông London (TfL ) thông_báo không gia_hạn giấy_phép Uber khi hết hạn vào ngày 30-9 vì hoạt_động của Uber gây nguy_hiểm cho an_toàn công_cộng .|Uber có 21 ngày để khiếu_nại và có_thể tiếp_tục hoạt_động đến khi quá_trình kháng_cáo kết_thúc .|\"Cách tiếp_cận và hành_xử của Uber thể_hiện sự thiếu trách_nhiệm của công_ty với một_số vấn_đề liên_quan an_toàn và an_ninh công_cộng tiềm_tàng\", TfL cho biết .|Tổng_Giám_đốc Uber tại London Tom_Elvidge cho biết sẽ lập_tức đưa vụ_việc lên tòa_án .|Các tài_xế taxi ở London cáo_buộc Uber vi_phạm các quy_tắc an_toàn và đe_dọa sinh_kế của họ .|Uber cũng bị nhiều phản_đối từ các công_đoàn và các nhà lập_pháp, cùng đối_mặt các cuộc_chiến_pháp_lý về quyền của người lao_động .|Uber đã phải rời một_số quốc_gia châu_Âu , gồm Đan_Mạch và Hungary , và đang đối_mặt các cuộc_chiến_pháp_lý tại nhiều nước khác cùng nhiều bang ở Mỹ .|Thị_trưởng London Sadiq_Khan tuyên_bố ủng_hộ quyết_định của TfL không gia_hạn giấy_phép của Uber .|\"Tất_cả công_ty ở London đều phải hoạt_động theo các quy_tắc và tuân_thủ các tiêu_chuẩn cao của chúng_tôi, đặc_biệt về mặt an_toàn cho khách_hàng .|Sẽ sai nếu TfL tiếp_tục cấp phép cho Uber khi có nguy_cơ an_toàn và an_ninh cho người dân London \", Reuters dẫn lời Thị_trưởng Khan .|BẢO NGHI .\"\"\"\n",
        "print(t)\n",
        "test = remove_stop_words(t, \"23352825\")\n",
        "print(len(t), len(test))\n",
        "print(test)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "# clear_file([shortEnt_in_longEntity_OUT_PATH])\n",
        "\n",
        "\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "onlyfiles = sorted(onlyfiles)\n",
        "print(onlyfiles)\n",
        "\n",
        "count_abnomal = 0\n",
        "for idx, doc_code in enumerate(onlyfiles):\n",
        "    # print(doc_code)\n",
        "    # if doc_code != \"23352508\":\n",
        "    #     continue\n",
        "    # print(doc_code)\n",
        "    \n",
        "    # if idx > 1:\n",
        "    #     break\n",
        "    # print(doc_code)\n",
        "    special_split_sent_not_final_IN_path = special_split_sent_not_final_IN_path_folder+'/'+ doc_code\n",
        "    with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "        doc_list = []\n",
        "        for org_docs_line in _org_docs_file:\n",
        "            org_docs_line = org_docs_line[:-1]\n",
        "            word_after_code =  org_docs_line[8:11]\n",
        "            if  word_after_code == '|a|':\n",
        "                # line_ls_tab_split = org_docs_line.split('\\t')\n",
        "                \n",
        "                print(len(org_docs_line) ,org_docs_line[:120] )\n",
        "                org_docs_line = remove_stop_words(org_docs_line, doc_code)\n",
        "                print(len(org_docs_line) ,org_docs_line[:120] )\n",
        "                print('\\n')\n",
        "                        \n",
        "            doc_list.append(org_docs_line)\n",
        "\n",
        "        file_name = special_split_sent_not_final_IN_path_folder+\"/\"+doc_code\n",
        "        clear_file([file_name])        \n",
        "        for line_to_write in doc_list:\n",
        "            \"\"\"\n",
        "                replace abnormal words\n",
        "            \"\"\"\n",
        "            write_append_data_to_txt_file(file_name, line_to_write)\n",
        "        # pprint(doc_list)\n",
        "        # print(doc_list[1])\n",
        "        # print(doc_list[2])\n",
        "print(f\"count_abnomal:  {count_abnomal}\")\n",
        "        \n",
        "       \n",
        "# # Serializing json\n",
        "# json_object = json.dumps(shortEnt_in_longEntity_json, indent=4)\n",
        "# with open(shortEnt_in_longEntity_OUT_PATH, \"w\") as outfile:\n",
        "#     outfile.write(json_object)\n",
        "\n",
        "                                            \n",
        "                            \n",
        "                        # write_append_data_to_txt_file(processed_error_files_OUT_PATH, doc_code)\n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### START TO CREATE PROCESSED FILE\n",
        "# sample:\n",
        "625456\tObsolete ... preparations .|One case ... years .|\n",
        "\n",
        "1:CID:2\tR2L\tNON-CROSS\t27-31\t10-11\t\n",
        "    D002119\tcalcium carbon - ate\tChemical\t27\t31\t1\t\n",
        "    D006934\thypercalcaemia\tDisease\t10\t11\t1\t\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
