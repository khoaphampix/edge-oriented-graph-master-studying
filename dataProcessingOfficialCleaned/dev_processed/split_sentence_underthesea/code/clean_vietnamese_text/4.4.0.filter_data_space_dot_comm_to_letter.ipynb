{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import json\n",
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "\n",
        "def write_append_data_to_txt_file(full_path_to_file, txt):\n",
        "    with open(full_path_to_file,'a') as out:\n",
        "        out.write(f'{txt}\\n')\n",
        "        # out.write(f'{txt}')s\n",
        "        \n",
        "def clear_file(full_path_to_files_list):\n",
        "    for _file in full_path_to_files_list:\n",
        "      with open(_file,'w') as out:\n",
        "        out.write(f'')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "['’', '¾', '²', '.', '>', '+', '/', '|', '\\n', ',', '‘', \n",
        "\n",
        "'&', \"'\", '*', '-', '\"', ']', '?', ':', '“', '\\t', ';', '–', \n",
        "\n",
        "'½', '_', '\\ufeff', '…', '!', '(', ' ', '@', '”', '[', '=', '%', ')']\n",
        "\n",
        "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
        "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
        "\n",
        "['.', '>', '+', '/', ',', '&', \"'\", '*', '-', '\"', ']', '?', ':', ';', '!', '(', ')' '@', \"[\", '=', '%', ']'\n",
        ".>+/&,*-?:;!()@[,=%]\"'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def spacing_char_dot_etc__and_word(sent):\n",
        "    # Define the regular expression pattern for splitting\n",
        "    # pattern = r\"\"\"([.,-])\"\"\" # origin\n",
        "    pattern = r\"\"\"([.>+/&,*\\-?:;!()@\\[\\]=%\\\"'])\"\"\"\n",
        "    # Split the input string based on the pattern and add spaces around the punctuation marks\n",
        "    output_str = re.sub(pattern, r\" \\1 \", sent).replace(\"  \",\" \")\n",
        "    return output_str\n",
        "\n",
        "input_str = \"Khiếp cảnh ? tượng siêu bão Harvey tàn phá bang Texas Mỹ Hãng tin Reuters đăng tải loạt ảnh phần nào cho thấy sức tàn phá kinh hoàng của siêu bão Harvey khi nó đổ bộ vào bang Texas , Mỹ . Siêu bão Harvey mạnh cấp 4 đã đổ bộ vào bang Texas đêm 25/8 (giờ địa phương), với sức gió lên tới 200 km/h. Ảnh: Cảnh tượng tan hoang ở Rockport sau khi bão quét qua. (Nguồn: Reuters ). Theo Wall Street Journal , ít nhất một người thiệt mạng, nhiều công trình bị hư hại và hàng nghìn hộ gia đình bị mất điện sau khi siêu bão thập kỷ Harvey càn quét bang Texas . Ảnh: Ảnh: Reuters . Một ngôi nhà bị phá hủy trong bão ở Rockport . Ảnh: Reuters . Người phụ nữ tìm những vật dụng còn dùng được trong ngôi nhà tan hoang ở Fulton . Ảnh: Reuters . Con đường ngập lụt ở Rockport . Ảnh: Reuters . Được biết, do ảnh hưởng của bão Harvey, hàng nghìn cư dân đã phải sơ tán đến nơi an toàn. Ảnh: Một chiếc máy bay bị lật úp tại sân bay gần Fulton . Ảnh: Reuters . Người đàn ông đang dọn dẹp con đường ở Rockport sau bão. Ảnh: Reuters . Những con thuyền bị gió mạnh đẩy vào bờ ở Port Lavaca . Ảnh: Reuters . Nhiều con bò chết trong cơn bão nằm trên đường cao tốc 35 gần Fulton . Ảnh: Reuters . Một ngôi nhà bị nước lũ bủa vây gần Port Lavca . Ảnh: Reuters . Quang cảnh ở Corpus Christie , Texas , sau bão. Các tình nguyện viên của Hội Chữ Thập Đỏ chuẩn bị những chiếc giường tại trung tâm cho người sơ tán ở Corpus Christi . Một cây to bật gốc đè bẹp chiếc ô tô bên cạnh ở Rockport . Ông Carlos Lopez dọn dẹp đống đổ nát bên ngoài cửa hàng ở Rockport . Phần đuôi của một chiếc máy bay bị bẻ cong tại sân bay gần Fulton . Biển nước mênh mông tại thành phố Port Lavaca , bang Texas . Ảnh: Reuters . Một cây xanh đổ ngang đường ở Corpus Christi . Được biết, Harvey là cơn bão mạnh nhất đổ bộ vào Mỹ trong 12 năm qua và là cơn bão mạnh nhất đổ bộ vào bang Texas trong hơn 50 năm qua. Ảnh: Reuters . Thiên An (Theo Reuters )\"\n",
        "output_str = spacing_char_dot_etc__and_word(input_str)\n",
        "# # Print the output string\n",
        "print(output_str[:20])\n",
        "print(len(output_str[:20]))\n",
        "print(output_str)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir, remove\n",
        "from os.path import isfile, join, isdir\n",
        "from pprint import pprint\n",
        "\n",
        "special_split_sent_not_final_IN_path_folder = \\\n",
        "\"/Users/n2t2k/Documents/Studying/Master/Thesis/InProgress/Coding/ORIGIN_RUN_ALL_edge-oriented-graph-master-studying/dataProcessingOfficialCleaned/dev_processed/split_sentence_underthesea/docs/common_info_embedd_files/split_passage_for_final_processed\"\n",
        "\n",
        "# clear_file([shortEnt_in_longEntity_OUT_PATH])\n",
        "\n",
        "\n",
        "onlyfiles = [f for f in listdir(special_split_sent_not_final_IN_path_folder) if isfile(join(special_split_sent_not_final_IN_path_folder, f))]\n",
        "onlyfiles = sorted(onlyfiles)\n",
        "print(onlyfiles)\n",
        "\n",
        "count_abnomal = 0\n",
        "for idx, doc_code in enumerate(onlyfiles):\n",
        "\n",
        "    \n",
        "    # if idx > 1:\n",
        "    #     break\n",
        "    # print(doc_code)\n",
        "    special_split_sent_not_final_IN_path = special_split_sent_not_final_IN_path_folder+'/'+ doc_code\n",
        "    with open(special_split_sent_not_final_IN_path,'r') as _org_docs_file:\n",
        "        doc_list = []\n",
        "        for org_docs_line in _org_docs_file:\n",
        "            org_docs_line = org_docs_line[:-1]\n",
        "            word_after_code =  org_docs_line[8:11]\n",
        "            if  word_after_code == '|a|':\n",
        "                not_sent = org_docs_line[:11]\n",
        "                list_sent = org_docs_line[11:]\n",
        "                # print(not_sent)\n",
        "                # print(list_sent)\n",
        "                list_sent = list_sent.split(\".|\")\n",
        "                list_sent = [spacing_char_dot_etc__and_word(s) for s in list_sent] \n",
        "                list_sent = \".|\".join(list_sent)\n",
        "                # print(list_sent)\n",
        "                org_docs_line = not_sent + list_sent\n",
        "                        \n",
        "            doc_list.append(org_docs_line)\n",
        "\n",
        "        file_name = special_split_sent_not_final_IN_path_folder+\"/\"+doc_code\n",
        "        clear_file([file_name])        \n",
        "        for line_to_write in doc_list:\n",
        "            write_append_data_to_txt_file(file_name, line_to_write)\n",
        "print(f\"count_abnomal:  {count_abnomal}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### START TO CREATE PROCESSED FILE\n",
        "# sample:\n",
        "625456\tObsolete ... preparations .|One case ... years .|\n",
        "\n",
        "1:CID:2\tR2L\tNON-CROSS\t27-31\t10-11\t\n",
        "    D002119\tcalcium carbon - ate\tChemical\t27\t31\t1\t\n",
        "    D006934\thypercalcaemia\tDisease\t10\t11\t1\t\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
